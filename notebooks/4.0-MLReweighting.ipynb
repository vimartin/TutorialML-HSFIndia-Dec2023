{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from matplotlib import pyplot as plt\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the variables with similar, but different, models for data and MC\n",
    "Instead of reading data from a simulation let generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_var1 = tfd.mixture.Mixture(tfd.Categorical(probs=[0.4, 0.6]),\n",
    "            components=[\n",
    "              tfd.Normal(loc=0., scale=0.1),\n",
    "              tfd.Normal(loc=0., scale=0.8),\n",
    "            ])\n",
    "data_model_var2 = tfd.Exponential(0.2)\n",
    "\n",
    "mc_model_var1 = tfd.mixture.Mixture(tfd.Categorical(probs=[0.3, 0.7]),\n",
    "            components=[\n",
    "              tfd.Normal(loc=0.1, scale=0.1),\n",
    "              tfd.Normal(loc=0., scale=1),\n",
    "            ])\n",
    "mc_model_var2 = tfd.Exponential(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the random data from the model\n",
    "The generated data has 3 columns. The first two are two observed variables, the third is 0 for data, 1 for MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSAMPLE = 100000\n",
    "data_var1 = data_model_var1.sample(NSAMPLE).numpy()\n",
    "data_var2 = data_model_var2.sample(NSAMPLE).numpy()\n",
    "# add a rotation to create correlations\n",
    "data_var1, data_var2 = 0.7 * data_var1 + 0.3 * data_var2, 0.3 * data_var1 + 0.7 * data_var2\n",
    "\n",
    "mc_var1 = mc_model_var1.sample(NSAMPLE).numpy()\n",
    "mc_var2 = mc_model_var2.sample(NSAMPLE).numpy()\n",
    "# add a small rotation to create correlations\n",
    "mc_var1, mc_var2 = 0.9 * mc_var1 + 0.1 * mc_var2, 0.1 * mc_var1 + 0.9 * mc_var2\n",
    "\n",
    "# merge the variables in a single vector, put a flag as last element (0/1 for data/mc)\n",
    "data = np.vstack([data_var1, data_var2, np.zeros(NSAMPLE)]).T\n",
    "mc = np.vstack([mc_var1, mc_var2, np.ones(NSAMPLE)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample mixing and shuffling data and mc\n",
    "sample = np.vstack([data, mc])\n",
    "np.random.shuffle(sample)\n",
    "\n",
    "# divide in train and test 50/50\n",
    "training_sample = sample[int(NSAMPLE/2):]\n",
    "testing_sample = sample[:int(NSAMPLE/2)]\n",
    "\n",
    "X_train = training_sample[:, :-1]\n",
    "Y_train = training_sample[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the classifier, optimizing the cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential([\n",
    "    tf.keras.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "classifier.compile(loss='binary_crossentropy', metrics='accuracy')\n",
    "classifier.fit(X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the variables before the correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sample_mc = testing_sample[testing_sample[:, -1] == 1]\n",
    "testing_sample_data = testing_sample[testing_sample[:, -1] == 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "bins = np.linspace(-2, 4, 100)\n",
    "axs[0].hist(testing_sample_mc[:, 0], bins=bins, histtype='step', label='MC', lw=2)\n",
    "axs[0].hist(testing_sample_data[:, 0], bins=bins, histtype='step', label='data', lw=2)\n",
    "axs[0].legend(loc=0, fontsize=14)\n",
    "axs[0].set_title('var1', fontsize=14)\n",
    "\n",
    "bins = np.linspace(-1, 30, 100)\n",
    "axs[1].hist(testing_sample_mc[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_data[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].set_title('var2', fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the weight on the test sample (the value is close to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight is just the ratio between\n",
    "\n",
    "$$w = \\frac{P[data|x]}{P[MC|x]}$$\n",
    "\n",
    "Usually this is estimated with a ND-histogram, or just ignoring correlation using a set of 1D histograms.\n",
    "\n",
    "A perfect classifier will return $y=P[data|x]$. Of course $P[MC|x] = 1 - P[data|x]$. So:\n",
    "\n",
    "$$w = \\frac{y}{1 - y}$$\n",
    "\n",
    "Note that if we have data very similar to MC we will get a very poor classifier, which will output 0.5, and so the weight will be 1.\n",
    "\n",
    "An additional thing to do (not done here), is to calibrate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = classifier.predict(testing_sample_mc[:, :-1])\n",
    "weights = weights / (1 - weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(weights, bins=np.linspace(0, 20, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the weights and plot the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "bins = np.linspace(-2, 4, 100)\n",
    "axs[0].hist(testing_sample_mc[:, 0], weights=1/weights, bins=bins, histtype='step', label='MC reweighted', lw=2)\n",
    "axs[0].hist(testing_sample_data[:, 0], bins=bins, histtype='step', label='data', lw=2)\n",
    "axs[0].hist(testing_sample_mc[:, 0], bins=bins, histtype='step', label='MC', ls='--')\n",
    "axs[0].legend(loc=0, fontsize=14)\n",
    "axs[0].set_title('var1', fontsize=14)\n",
    "\n",
    "bins = np.linspace(-1, 30, 100)\n",
    "axs[1].hist(testing_sample_mc[:, 1], weights=1/weights, bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_data[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_mc[:, 1], bins=bins, histtype='step', label='MC', ls='--')\n",
    "axs[1].set_title('var2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the correlation\n",
    "The reweighed MC reproduce the data correlation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "bins = [np.linspace(-2, 2, 50), np.linspace(0., 10, 50)]\n",
    "axs[0].hist2d(testing_sample_data[:, 0], testing_sample_data[:, 1], bins=bins)\n",
    "axs[1].hist2d(testing_sample_mc[:, 0], testing_sample_mc[:, 1], bins=bins)\n",
    "axs[2].hist2d(testing_sample_mc[:, 0], testing_sample_mc[:, 1], bins=bins, weights=1/np.squeeze(weights))\n",
    "axs[0].set_title('data', fontsize=14)\n",
    "axs[1].set_title('MC', fontsize=14)\n",
    "axs[2].set_title('MC reweighted', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare effective statistics with size of the sample\n",
    "We loose a lot of effective statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1/np.squeeze(weights)\n",
    "np.sum(w) ** 2 / np.sum(w ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
