{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations\n",
    "The data is available in the CERN [OpenData](http://opendata.cern.ch/record/15012)\n",
    "The code is based on the FastCaloGAN code which is available on [Zenodo](https://zenodo.org/record/5589623) with the latest development available for ATLAS member on the [FCS git repository](https://gitlab.cern.ch/atlas-simulation-fastcalosim/fastcalogan)\n",
    "\n",
    "The data and code are also the case for the [#calochallenge](https://github.com/CaloChallenge/homepage)\n",
    "\n",
    "This example runs on only two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import DataLoader \n",
    "import importlib\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from functools import partial\n",
    "tf.keras.backend.set_floatx('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = {\n",
    "    256: \"http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E256_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=bSSGDUvNFuCxHHz3YlCqga0Jq0g%3D&Expires=1829145580\",\n",
    "    512: \"http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E512_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=Dr3A32ycujBSm4bQ14l%2BHvEf1Ig%3D&Expires=1829145645\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN architecture\n",
    "The generator takes as input noise and in this case two values to create a conditional generator. It will output 368 values, using a stack of dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Noise (InputLayer)             [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 52)           0           ['Noise[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           2650        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50)          200         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 50)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          5100        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 200)          20200       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 200)         800         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 200)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 368)          73968       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 368)         1472        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 368)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 104,790\n",
      "Trainable params: 103,354\n",
      "Non-trainable params: 1,436\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "noise = layers.Input(shape=(50), name=\"Noise\")\n",
    "condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "con = layers.concatenate([noise,condition])\n",
    "G = layers.Dense(50, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(con)  \n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(100, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(200, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "\n",
    "generator = Model(inputs=[noise, condition], outputs=G)\n",
    "generator.build(370)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Discriminator network\n",
    "The discriminator takes as input the values generated by the generator and check if they discriminate if they are real or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image (InputLayer)             [(None, 368)]        0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 370)          0           ['Image[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 368)          136528      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 368)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 368)          135792      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 368)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 368)          135792      ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 368)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            369         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,481\n",
      "Trainable params: 408,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "\n",
    "image = layers.Input(shape=(368), name=\"Image\")\n",
    "d_condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "d_con = layers.concatenate([image, d_condition])\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(d_con)  \n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(1, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "\n",
    "discriminator = Model(inputs=[image, d_condition], outputs=D)\n",
    "discriminator.build(370)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, loss and gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_penalty(f, x_real, x_fake, cond_label, batchsize, D):\n",
    "  alpha = tf.random.uniform([batchsize, 1], minval=0., maxval=1.)\n",
    "\n",
    "  inter = alpha * x_real + (1-alpha) * x_fake\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(inter)\n",
    "    pred = D(inputs=[inter, cond_label])\n",
    "  grad = t.gradient(pred, [inter])[0]\n",
    "  \n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "  gp = 0.00001 * tf.reduce_mean((slopes - 1.)**2) #Lambda\n",
    "  return gp\n",
    "\n",
    "@tf.function\n",
    "def D_loss(x_real, cond_label, batchsize, G, D): \n",
    "  z = tf.random.normal([batchsize, 50], mean=0.5, stddev=0.5, dtype=tf.dtypes.float32) #batch and latent dim\n",
    "  x_fake = G(inputs=[z, cond_label])\n",
    "  D_fake = D(inputs=[x_fake, cond_label])\n",
    "  D_real = D(inputs=[x_real, cond_label])\n",
    "  D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + gradient_penalty(f=partial(D, training=True), x_real=x_real, x_fake=x_fake, cond_label=cond_label, batchsize=batchsize, D=D)\n",
    "  return D_loss, D_fake\n",
    "\n",
    "@tf.function\n",
    "def G_loss(D_fake):\n",
    "  G_loss = -tf.reduce_mean(D_fake)\n",
    "  return G_loss\n",
    "\n",
    "def getTrainData_ultimate( n_iteration, batchsize, dgratio, X ,Labels):\n",
    "  true_batchsize = tf.cast(tf.math.multiply(batchsize, dgratio), tf.int64)\n",
    "  n_samples = tf.cast(tf.gather(tf.shape(X), 0), tf.int64)\n",
    "  n_batch = tf.cast(tf.math.floordiv(n_samples, true_batchsize), tf.int64)\n",
    "  n_shuffles = tf.cast(tf.math.ceil(tf.divide(n_iteration, n_batch)), tf.int64)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((X, Labels))\n",
    "  ds = ds.shuffle(buffer_size = n_samples).repeat(n_shuffles).batch(true_batchsize, drop_remainder=True).prefetch(2)\n",
    "  return iter(ds)\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X_trains, cond_labels, batchsize, dgratio, G, D, generator_optimizer, discriminator_optimizer): \n",
    "  for i in tf.range(dgratio):\n",
    "    print(\"d train: \" + str(i))\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, i), tf.gather(cond_labels, i), batchsize, G, D)\n",
    "      gradients_of_discriminator = disc_tape.gradient(D_loss_curr, D.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, D.trainable_variables))    \n",
    "      \n",
    "  print(\"g train\")\n",
    "  last_index = tf.subtract(dgratio, 1)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    # Need to recompute D_fake, otherwise gen_tape doesn't know the history\n",
    "    (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, last_index), tf.gather(cond_labels, last_index), batchsize, G, D)\n",
    "    G_loss_curr = G_loss(D_fake)\n",
    "    gradients_of_generator = gen_tape.gradient(G_loss_curr, G.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, G.trainable_variables))\n",
    "    return D_loss_curr, G_loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgratio = 5\n",
    "batchsize = 128\n",
    "G_lr = D_lr = 0.0001\n",
    "G_beta1 = D_beta1 = 0.55\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate=G_lr, beta_1=G_beta1)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=D_lr, beta_1=D_beta1)\n",
    "\n",
    "# Prepare for check pointing\n",
    "saver = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                            discriminator_optimizer=discriminator_optimizer,\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* DATA READER ***************\n",
      "Loading data\n",
      "Opening file http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E256_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=bSSGDUvNFuCxHHz3YlCqga0Jq0g%3D&Expires=1829145580\n",
      "Loaded momentum 256 GeV with 10000 events and 368 columns\n",
      "Opening file http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E512_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=Dr3A32ycujBSm4bQ14l%2BHvEf1Ig%3D&Expires=1829145645\n",
      "Loaded momentum 512 GeV with 10000 events and 368 columns\n",
      "Data was normalised by 256.000000\n",
      "Data was normalised by 512.000000\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader.DataLoader(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20000 events\n",
      "Model and loss values will be saved every 2 iterations.\n",
      "Iter: 2; D loss: 1.525; G_loss:  -2.586e+03\n",
      "Iter: 4; D loss: -6.39; G_loss:  -2.652e+03\n",
      "Iter: 6; D loss: 8.888; G_loss:  -2.579e+03\n",
      "Iter: 8; D loss: 3.095; G_loss:  -2.734e+03\n",
      "Iter: 10; D loss: 8.349; G_loss:  -2.683e+03\n",
      "Iter: 12; D loss: -10.21; G_loss:  -2.819e+03\n",
      "Iter: 14; D loss: -53.84; G_loss:  -2.783e+03\n",
      "Iter: 16; D loss: -57.77; G_loss:  -2.97e+03\n",
      "Iter: 18; D loss: -80.9; G_loss:  -3.018e+03\n",
      "Iter: 20; D loss: -85.71; G_loss:  -3.084e+03\n",
      "Iter: 22; D loss: -88.89; G_loss:  -3.283e+03\n",
      "Iter: 24; D loss: -93.5; G_loss:  -3.225e+03\n",
      "Iter: 26; D loss: -116.9; G_loss:  -3.314e+03\n",
      "Iter: 28; D loss: -91.08; G_loss:  -3.44e+03\n",
      "Iter: 30; D loss: -95.25; G_loss:  -3.548e+03\n",
      "Iter: 32; D loss: -102.9; G_loss:  -3.506e+03\n",
      "Iter: 34; D loss: -67.22; G_loss:  -3.445e+03\n",
      "Iter: 36; D loss: -27.72; G_loss:  -3.41e+03\n",
      "Iter: 38; D loss: -4.953; G_loss:  -3.477e+03\n",
      "Iter: 40; D loss: 9.39; G_loss:  -3.414e+03\n",
      "Iter: 42; D loss: -0.3814; G_loss:  -3.126e+03\n",
      "Iter: 44; D loss: 11.52; G_loss:  -3.08e+03\n",
      "Iter: 46; D loss: 15.27; G_loss:  -2.894e+03\n",
      "Iter: 48; D loss: -3.333; G_loss:  -2.858e+03\n",
      "Iter: 50; D loss: -26.77; G_loss:  -2.787e+03\n",
      "Iter: 52; D loss: -57.87; G_loss:  -2.648e+03\n",
      "Iter: 54; D loss: -102.1; G_loss:  -2.589e+03\n",
      "Iter: 56; D loss: -130.6; G_loss:  -2.492e+03\n",
      "Iter: 58; D loss: -156.5; G_loss:  -2.444e+03\n",
      "Iter: 60; D loss: -163.7; G_loss:  -2.123e+03\n",
      "Iter: 62; D loss: -144.9; G_loss:  -2.11e+03\n",
      "Iter: 64; D loss: -144.4; G_loss:  -2.097e+03\n",
      "Iter: 66; D loss: -93.62; G_loss:  -2.168e+03\n",
      "Iter: 68; D loss: -40.49; G_loss:  -2.226e+03\n",
      "Iter: 70; D loss: -23.12; G_loss:  -2.329e+03\n",
      "Iter: 72; D loss: 22.79; G_loss:  -2.398e+03\n",
      "Iter: 74; D loss: 46.31; G_loss:  -2.444e+03\n",
      "Iter: 76; D loss: 38.79; G_loss:  -2.39e+03\n",
      "Iter: 78; D loss: 33.48; G_loss:  -2.516e+03\n",
      "Iter: 80; D loss: 37.51; G_loss:  -2.637e+03\n",
      "Iter: 82; D loss: -5.757; G_loss:  -2.665e+03\n",
      "Iter: 84; D loss: -20.75; G_loss:  -2.699e+03\n",
      "Iter: 86; D loss: -29.17; G_loss:  -2.718e+03\n",
      "Iter: 88; D loss: -40.71; G_loss:  -2.885e+03\n",
      "Iter: 90; D loss: -63.23; G_loss:  -2.829e+03\n",
      "Iter: 92; D loss: -84.04; G_loss:  -2.97e+03\n",
      "Iter: 94; D loss: -107.6; G_loss:  -3.148e+03\n",
      "Iter: 96; D loss: -125.4; G_loss:  -3.29e+03\n",
      "Iter: 98; D loss: -130.4; G_loss:  -3.228e+03\n",
      "Iter: 100; D loss: -109.8; G_loss:  -3.612e+03\n",
      "Iter: 102; D loss: -137.0; G_loss:  -3.526e+03\n",
      "Iter: 104; D loss: -119.4; G_loss:  -3.523e+03\n",
      "Iter: 106; D loss: -88.21; G_loss:  -3.68e+03\n",
      "Iter: 108; D loss: -84.52; G_loss:  -3.764e+03\n",
      "Iter: 110; D loss: -23.92; G_loss:  -3.816e+03\n",
      "Iter: 112; D loss: -33.28; G_loss:  -3.61e+03\n",
      "Iter: 114; D loss: -5.019; G_loss:  -3.558e+03\n",
      "Iter: 116; D loss: 9.143; G_loss:  -3.572e+03\n",
      "Iter: 118; D loss: 18.18; G_loss:  -3.414e+03\n",
      "Iter: 120; D loss: 11.96; G_loss:  -3.384e+03\n",
      "Iter: 122; D loss: 24.08; G_loss:  -3.174e+03\n",
      "Iter: 124; D loss: 13.14; G_loss:  -3.005e+03\n",
      "Iter: 126; D loss: 1.027; G_loss:  -2.836e+03\n",
      "Iter: 128; D loss: -13.83; G_loss:  -2.642e+03\n",
      "Iter: 130; D loss: -50.51; G_loss:  -2.497e+03\n",
      "Iter: 132; D loss: -80.93; G_loss:  -2.446e+03\n",
      "Iter: 134; D loss: -100.3; G_loss:  -2.248e+03\n",
      "Iter: 136; D loss: -134.0; G_loss:  -2.179e+03\n",
      "Iter: 138; D loss: -141.3; G_loss:  -2.012e+03\n",
      "Iter: 140; D loss: -177.9; G_loss:  -2.006e+03\n",
      "Iter: 142; D loss: -139.8; G_loss:  -1.852e+03\n",
      "Iter: 144; D loss: -136.5; G_loss:  -1.846e+03\n",
      "Iter: 146; D loss: -115.9; G_loss:  -1.858e+03\n",
      "Iter: 148; D loss: -57.95; G_loss:  -1.761e+03\n",
      "Iter: 150; D loss: -35.24; G_loss:  -1.931e+03\n",
      "Iter: 152; D loss: -6.628; G_loss:  -2.093e+03\n",
      "Iter: 154; D loss: 20.1; G_loss:  -2.161e+03\n",
      "Iter: 156; D loss: 1.243; G_loss:  -2.249e+03\n",
      "Iter: 158; D loss: 34.1; G_loss:  -2.228e+03\n",
      "Iter: 160; D loss: 30.3; G_loss:  -2.302e+03\n",
      "Iter: 162; D loss: 27.38; G_loss:  -2.455e+03\n",
      "Iter: 164; D loss: 16.47; G_loss:  -2.41e+03\n",
      "Iter: 166; D loss: -10.85; G_loss:  -2.466e+03\n",
      "Iter: 168; D loss: -18.85; G_loss:  -2.591e+03\n",
      "Iter: 170; D loss: -36.81; G_loss:  -2.693e+03\n",
      "Iter: 172; D loss: -44.38; G_loss:  -2.772e+03\n",
      "Iter: 174; D loss: -52.68; G_loss:  -2.896e+03\n",
      "Iter: 176; D loss: -82.43; G_loss:  -3.069e+03\n",
      "Iter: 178; D loss: -87.02; G_loss:  -3.008e+03\n",
      "Iter: 180; D loss: -89.49; G_loss:  -3.239e+03\n",
      "Iter: 182; D loss: -98.82; G_loss:  -3.263e+03\n",
      "Iter: 184; D loss: -93.3; G_loss:  -3.37e+03\n",
      "Iter: 186; D loss: -119.4; G_loss:  -3.375e+03\n",
      "Iter: 188; D loss: -95.19; G_loss:  -3.567e+03\n",
      "Iter: 190; D loss: -74.05; G_loss:  -3.645e+03\n",
      "Iter: 192; D loss: -66.38; G_loss:  -3.627e+03\n",
      "Iter: 194; D loss: -44.87; G_loss:  -3.637e+03\n",
      "Iter: 196; D loss: -62.27; G_loss:  -3.593e+03\n",
      "Iter: 198; D loss: -48.99; G_loss:  -3.474e+03\n",
      "Iter: 200; D loss: 0.7069; G_loss:  -3.565e+03\n",
      "Iter: 202; D loss: 12.85; G_loss:  -3.521e+03\n",
      "Iter: 204; D loss: -13.83; G_loss:  -3.427e+03\n",
      "Iter: 206; D loss: -6.402; G_loss:  -3.403e+03\n",
      "Iter: 208; D loss: 3.326; G_loss:  -3.195e+03\n",
      "Iter: 210; D loss: -1.513; G_loss:  -3.06e+03\n",
      "Iter: 212; D loss: -16.44; G_loss:  -2.874e+03\n",
      "Iter: 214; D loss: -23.37; G_loss:  -2.893e+03\n",
      "Iter: 216; D loss: -43.21; G_loss:  -2.751e+03\n",
      "Iter: 218; D loss: -50.69; G_loss:  -2.596e+03\n",
      "Iter: 220; D loss: -64.54; G_loss:  -2.265e+03\n",
      "Iter: 222; D loss: -90.06; G_loss:  -2.142e+03\n",
      "Iter: 224; D loss: -136.8; G_loss:  -2.003e+03\n",
      "Iter: 226; D loss: -141.0; G_loss:  -2.023e+03\n",
      "Iter: 228; D loss: -96.0; G_loss:  -1.878e+03\n",
      "Iter: 230; D loss: -109.1; G_loss:  -1.997e+03\n",
      "Iter: 232; D loss: -94.05; G_loss:  -1.921e+03\n",
      "Iter: 234; D loss: -98.91; G_loss:  -1.977e+03\n",
      "Iter: 236; D loss: -90.04; G_loss:  -1.953e+03\n",
      "Iter: 238; D loss: -69.13; G_loss:  -1.969e+03\n",
      "Iter: 240; D loss: -17.28; G_loss:  -2.04e+03\n",
      "Iter: 242; D loss: -8.522; G_loss:  -2.011e+03\n",
      "Iter: 244; D loss: 0.4228; G_loss:  -2.068e+03\n",
      "Iter: 246; D loss: 8.482; G_loss:  -2.043e+03\n",
      "Iter: 248; D loss: 1.596; G_loss:  -1.985e+03\n",
      "Iter: 250; D loss: 12.65; G_loss:  -2.168e+03\n",
      "Iter: 252; D loss: 9.18; G_loss:  -2.142e+03\n",
      "Iter: 254; D loss: -16.02; G_loss:  -2.254e+03\n",
      "Iter: 256; D loss: -23.73; G_loss:  -2.417e+03\n",
      "Iter: 258; D loss: -24.5; G_loss:  -2.46e+03\n",
      "Iter: 260; D loss: -21.42; G_loss:  -2.613e+03\n",
      "Iter: 262; D loss: -30.7; G_loss:  -2.665e+03\n",
      "Iter: 264; D loss: -50.19; G_loss:  -2.634e+03\n",
      "Iter: 266; D loss: -67.08; G_loss:  -2.666e+03\n",
      "Iter: 268; D loss: -71.56; G_loss:  -2.92e+03\n",
      "Iter: 270; D loss: -79.28; G_loss:  -2.948e+03\n",
      "Iter: 272; D loss: -81.89; G_loss:  -3.13e+03\n",
      "Iter: 274; D loss: -88.9; G_loss:  -2.972e+03\n",
      "Iter: 276; D loss: -97.69; G_loss:  -3.185e+03\n",
      "Iter: 278; D loss: -84.81; G_loss:  -3.223e+03\n",
      "Iter: 280; D loss: -58.7; G_loss:  -3.228e+03\n",
      "Iter: 282; D loss: -61.66; G_loss:  -3.402e+03\n",
      "Iter: 284; D loss: -53.72; G_loss:  -3.081e+03\n",
      "Iter: 286; D loss: -41.18; G_loss:  -3.215e+03\n",
      "Iter: 288; D loss: -53.68; G_loss:  -3.441e+03\n",
      "Iter: 290; D loss: -19.39; G_loss:  -3.238e+03\n",
      "Iter: 292; D loss: -27.38; G_loss:  -3.207e+03\n",
      "Iter: 294; D loss: -23.08; G_loss:  -3.163e+03\n",
      "Iter: 296; D loss: -21.15; G_loss:  -3.132e+03\n",
      "Iter: 298; D loss: 2.503; G_loss:  -3.105e+03\n",
      "Iter: 300; D loss: -4.024; G_loss:  -2.906e+03\n",
      "Iter: 302; D loss: -14.61; G_loss:  -2.972e+03\n",
      "Iter: 304; D loss: -16.86; G_loss:  -2.779e+03\n",
      "Iter: 306; D loss: -22.15; G_loss:  -2.753e+03\n",
      "Iter: 308; D loss: -30.81; G_loss:  -2.6e+03\n",
      "Iter: 310; D loss: -55.04; G_loss:  -2.503e+03\n",
      "Iter: 312; D loss: -56.16; G_loss:  -2.148e+03\n",
      "Iter: 314; D loss: -63.93; G_loss:  -2.054e+03\n",
      "Iter: 316; D loss: -78.85; G_loss:  -2.073e+03\n",
      "Iter: 318; D loss: -87.04; G_loss:  -1.931e+03\n",
      "Iter: 320; D loss: -90.86; G_loss:  -1.866e+03\n",
      "Iter: 322; D loss: -77.51; G_loss:  -1.837e+03\n",
      "Iter: 324; D loss: -83.34; G_loss:  -1.848e+03\n",
      "Iter: 326; D loss: -84.21; G_loss:  -1.86e+03\n",
      "Iter: 328; D loss: -55.43; G_loss:  -1.869e+03\n",
      "Iter: 330; D loss: -41.2; G_loss:  -1.982e+03\n",
      "Iter: 332; D loss: -13.66; G_loss:  -1.957e+03\n",
      "Iter: 334; D loss: -27.16; G_loss:  -1.981e+03\n",
      "Iter: 336; D loss: -11.91; G_loss:  -2.06e+03\n",
      "Iter: 338; D loss: -3.302; G_loss:  -2.042e+03\n",
      "Iter: 340; D loss: 8.141; G_loss:  -2.128e+03\n",
      "Iter: 342; D loss: -7.358; G_loss:  -2.044e+03\n",
      "Iter: 344; D loss: 4.262; G_loss:  -2.18e+03\n",
      "Iter: 346; D loss: 12.8; G_loss:  -2.244e+03\n",
      "Iter: 348; D loss: -12.04; G_loss:  -2.208e+03\n",
      "Iter: 350; D loss: 2.886; G_loss:  -2.37e+03\n",
      "Iter: 352; D loss: -21.51; G_loss:  -2.514e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 354; D loss: -25.25; G_loss:  -2.562e+03\n",
      "Iter: 356; D loss: -48.25; G_loss:  -2.505e+03\n",
      "Iter: 358; D loss: -50.08; G_loss:  -2.7e+03\n",
      "Iter: 360; D loss: -62.94; G_loss:  -2.737e+03\n",
      "Iter: 362; D loss: -72.48; G_loss:  -2.853e+03\n",
      "Iter: 364; D loss: -72.53; G_loss:  -2.824e+03\n",
      "Iter: 366; D loss: -52.37; G_loss:  -2.832e+03\n",
      "Iter: 368; D loss: -61.92; G_loss:  -3.11e+03\n",
      "Iter: 370; D loss: -71.21; G_loss:  -3.011e+03\n",
      "Iter: 372; D loss: -64.32; G_loss:  -3.214e+03\n",
      "Iter: 374; D loss: -91.92; G_loss:  -3.225e+03\n",
      "Iter: 376; D loss: -77.21; G_loss:  -3.259e+03\n",
      "Iter: 378; D loss: -40.06; G_loss:  -3.219e+03\n",
      "Iter: 380; D loss: -64.72; G_loss:  -3.275e+03\n",
      "Iter: 382; D loss: -30.6; G_loss:  -3.142e+03\n",
      "Iter: 384; D loss: -49.27; G_loss:  -3.127e+03\n",
      "Iter: 386; D loss: -25.63; G_loss:  -2.987e+03\n",
      "Iter: 388; D loss: 8.431; G_loss:  -3.022e+03\n",
      "Iter: 390; D loss: -5.184; G_loss:  -2.868e+03\n",
      "Iter: 392; D loss: 9.723; G_loss:  -2.775e+03\n",
      "Iter: 394; D loss: -5.686; G_loss:  -2.671e+03\n",
      "Iter: 396; D loss: -14.01; G_loss:  -2.59e+03\n",
      "Iter: 398; D loss: -29.89; G_loss:  -2.537e+03\n",
      "Iter: 400; D loss: -21.14; G_loss:  -2.289e+03\n",
      "Iter: 402; D loss: -54.47; G_loss:  -2.274e+03\n",
      "Iter: 404; D loss: -51.0; G_loss:  -2.123e+03\n",
      "Iter: 406; D loss: -66.92; G_loss:  -2.02e+03\n",
      "Iter: 408; D loss: -79.92; G_loss:  -1.979e+03\n",
      "Iter: 410; D loss: -66.57; G_loss:  -1.939e+03\n",
      "Iter: 412; D loss: -85.12; G_loss:  -1.875e+03\n",
      "Iter: 414; D loss: -78.46; G_loss:  -1.773e+03\n",
      "Iter: 416; D loss: -85.76; G_loss:  -1.839e+03\n",
      "Iter: 418; D loss: -79.38; G_loss:  -1.731e+03\n",
      "Iter: 420; D loss: -54.41; G_loss:  -1.802e+03\n",
      "Iter: 422; D loss: -40.23; G_loss:  -1.796e+03\n",
      "Iter: 424; D loss: -20.56; G_loss:  -1.781e+03\n",
      "Iter: 426; D loss: -12.56; G_loss:  -1.806e+03\n",
      "Iter: 428; D loss: -6.918; G_loss:  -1.832e+03\n",
      "Iter: 430; D loss: -11.97; G_loss:  -1.886e+03\n",
      "Iter: 432; D loss: -7.303; G_loss:  -1.934e+03\n",
      "Iter: 434; D loss: -8.041; G_loss:  -2.057e+03\n",
      "Iter: 436; D loss: -4.924; G_loss:  -2.129e+03\n",
      "Iter: 438; D loss: -7.363; G_loss:  -2.198e+03\n",
      "Iter: 440; D loss: -16.39; G_loss:  -2.301e+03\n",
      "Iter: 442; D loss: -26.35; G_loss:  -2.295e+03\n",
      "Iter: 444; D loss: -27.38; G_loss:  -2.465e+03\n",
      "Iter: 446; D loss: -39.55; G_loss:  -2.385e+03\n",
      "Iter: 448; D loss: -38.63; G_loss:  -2.508e+03\n",
      "Iter: 450; D loss: -41.85; G_loss:  -2.559e+03\n",
      "Iter: 452; D loss: -65.3; G_loss:  -2.725e+03\n",
      "Iter: 454; D loss: -61.16; G_loss:  -2.824e+03\n",
      "Iter: 456; D loss: -61.18; G_loss:  -2.847e+03\n",
      "Iter: 458; D loss: -61.1; G_loss:  -2.943e+03\n",
      "Iter: 460; D loss: -47.68; G_loss:  -2.995e+03\n",
      "Iter: 462; D loss: -52.53; G_loss:  -3.044e+03\n",
      "Iter: 464; D loss: -39.65; G_loss:  -3.159e+03\n",
      "Iter: 466; D loss: -57.66; G_loss:  -3.092e+03\n",
      "Iter: 468; D loss: -40.11; G_loss:  -3.168e+03\n",
      "Iter: 470; D loss: -33.84; G_loss:  -3.074e+03\n",
      "Iter: 472; D loss: -25.0; G_loss:  -3.103e+03\n",
      "Iter: 474; D loss: -3.205; G_loss:  -3.072e+03\n",
      "Iter: 476; D loss: -11.86; G_loss:  -3.058e+03\n",
      "Iter: 478; D loss: -2.269; G_loss:  -2.87e+03\n",
      "Iter: 480; D loss: -27.53; G_loss:  -2.806e+03\n",
      "Iter: 482; D loss: -9.617; G_loss:  -2.588e+03\n",
      "Iter: 484; D loss: 2.623; G_loss:  -2.584e+03\n",
      "Iter: 486; D loss: -3.833; G_loss:  -2.449e+03\n",
      "Iter: 488; D loss: -14.47; G_loss:  -2.358e+03\n",
      "Iter: 490; D loss: -30.79; G_loss:  -2.223e+03\n",
      "Iter: 492; D loss: -45.49; G_loss:  -2.135e+03\n",
      "Iter: 494; D loss: -51.51; G_loss:  -1.985e+03\n",
      "Iter: 496; D loss: -63.42; G_loss:  -1.908e+03\n",
      "Iter: 498; D loss: -79.5; G_loss:  -1.837e+03\n"
     ]
    }
   ],
   "source": [
    "start_iteration = 0 \n",
    "max_iterations = 500\n",
    "\n",
    "for iteration in range(start_iteration,max_iterations): \n",
    "  change_data = (iteration == start_iteration)\n",
    "  \n",
    "  if (change_data == True):\n",
    "    X, Labels = dl.getAllTrainData(8, 9)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    Labels = tf.convert_to_tensor(Labels, dtype=tf.float32)\n",
    " \n",
    "    remained_iteration = tf.constant(max_iterations - iteration, dtype=tf.int64)\n",
    "    ds_iter = getTrainData_ultimate(remained_iteration, batchsize, dgratio, X ,Labels)\n",
    "    print (\"Using \"+ str(X.shape[0])+ \" events\")\n",
    "\n",
    "  X, Labels = ds_iter.get_next()\n",
    "\n",
    "  X_feature_size = tf.gather(tf.shape(X), 1)\n",
    "  Labels_feature_size = tf.gather(tf.shape(Labels), 1)\n",
    "  X_batch_shape = tf.stack((dgratio, batchsize, X_feature_size), axis=0)\n",
    "  Labels_batch_shape = tf.stack((dgratio, batchsize, Labels_feature_size), axis=0)\n",
    "\n",
    "  X_trains    = tf.reshape(X, X_batch_shape)\n",
    "  cond_labels = tf.reshape(Labels, Labels_batch_shape)  \n",
    "\n",
    "  #print(X_trains) \n",
    "  #print(cond_labels) \n",
    "  #print(batchsize) \n",
    "  #print(dgratio) \n",
    "  #generator.summary() \n",
    "  #discriminator.summary() \n",
    "\n",
    "  D_loss_curr, G_loss_curr = train_loop(X_trains, cond_labels, batchsize, dgratio, generator, discriminator,  generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "  if iteration == 0: \n",
    "    print(\"Model and loss values will be saved every 2 iterations.\" )\n",
    "  \n",
    "  if iteration % 2 == 0 and iteration > 0:\n",
    "\n",
    "    try:\n",
    "      saver.save(file_prefix = checkpoint_dir+ '/model')\n",
    "    except:\n",
    "      print(\"Something went wrong in saving iteration %s, moving to next one\" % (iteration))\n",
    "      print(\"exception message \", sys.exc_info()[0])     \n",
    "    \n",
    "    print('Iter: {}; D loss: {:.4}; G_loss:  {:.4}'.format(iteration, D_loss_curr, G_loss_curr))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse \n",
    "import os,sys,ctypes\n",
    "import ROOT \n",
    "import shutil\n",
    "import ctypes\n",
    "import glob\n",
    "\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "save_plots = True\n",
    "lparams = {'xoffset' : 0.1, 'yoffset' : 0.27, 'width'   : 0.8, 'height'  : 0.35}\n",
    "canvases = []   \n",
    "\n",
    "histos_vox = []\n",
    "input_files_vox = []\n",
    "\n",
    "minFactor = 3\n",
    "maxFactor = 3  \n",
    "particleName=\"#gamma\"\n",
    "particle=\"photons\"\n",
    "\n",
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)\n",
    "\n",
    "maxVoxel = 0\n",
    "midEnergy = 0\n",
    "step = 50\n",
    "\n",
    "wgan = conditional_wgangp.WGANGP()\n",
    "dl = DataLoader.DataLoader()\n",
    "ekins = dl.ekins\n",
    "\n",
    "firstPosition = 0\n",
    "\n",
    "print(\"Opening vox files\")\n",
    "for index, energy in enumerate(ekins):    \n",
    "  #print(\" Energy \", energy)\n",
    "  input_file_vox = ('rootFiles/pid22_E%s_eta_20_25.root' % (energy))\n",
    "  print(\" Opening file: \" + input_file_vox)\n",
    "  infile_vox = ROOT.TFile.Open(input_file_vox, 'read') \n",
    "  input_files_vox.append(infile_vox)\n",
    "  tree = infile_vox.Get('rootTree') \n",
    "  \n",
    "  h = ROOT.TH1F(\"h\",\"\",100,0,energy*2) \n",
    "  tree.Draw(\"etot>>h\",\"\",\"off\")\n",
    "  xmax=h.GetBinCenter(h.GetMaximumBin());\n",
    "  minX = max(0, xmax-minFactor*h.GetRMS()) #max(0, xmax-minFactors[item]*h.GetRMS())\n",
    "  maxX = xmax+maxFactor*h.GetRMS()\n",
    "  print(\"min \"+ str(minX) + \" max \" + str(maxX))\n",
    "      \n",
    "  h_vox = ROOT.TH1F(\"h_vox\",\"\",30,minX/1000,maxX/1000) \n",
    "  tree.Draw(\"etot/1000>>h_vox\",\"\",\"off\")\n",
    "  h_vox.Scale(1/h_vox.GetEntries())\n",
    "  histos_vox.append(h_vox)\n",
    "\n",
    "print (\"Running from %i to %i in step of %i\" %(0, 1000, step))\n",
    "for iteration in range(0, 1000, step):\n",
    "  try:\n",
    "    histos_gan =[]\n",
    "    canvas = ROOT.TCanvas('canvas_h', 'Total Energy comparison plots', 900, 900)\n",
    "    canvas.Divide(4,4)\n",
    "    legendPadIndex = 16\n",
    "\n",
    "    canvases.append(canvas)\n",
    "\n",
    "    chi2_tot = 0.\n",
    "    ndf_tot = 0\n",
    "    input_files_gan = []\n",
    "\n",
    "    for index, energy in enumerate(ekins):     \n",
    "      ekin_sample = ekins[index]\n",
    "      energyArray = np.array([energy] * nevents)\n",
    "      etaArray = np.zeros(nevents) \n",
    "      labels = np.vstack((energyArray, etaArray)).T   \n",
    "\n",
    "      data = wgan.load(iteration, labels, n_events, 'checkpoints')\n",
    "      data = data * ekin_sample       #needed for conditional\n",
    "        \n",
    "      h_vox = histos_vox[index]\n",
    "      h_gan = ROOT.TH1F(\"h_gan\",\"\",30,h_vox.GetXaxis().GetXmin(),h_vox.GetXaxis().GetXmax())\n",
    "\n",
    "      E_tot = data.numpy().sum(axis=1)\n",
    "      for e in E_tot:\n",
    "        h_gan.Fill(e/1000)\n",
    "      \n",
    "      h_gan.Scale(1/h_gan.GetEntries())\n",
    "      h_gan.SetLineColor(ROOT.kRed)\n",
    "      h_gan.SetLineStyle(7)\n",
    "      m = [h_vox.GetBinContent(h_vox.GetMaximumBin()),h_gan.GetBinContent(h_gan.GetMaximumBin())]\n",
    "      h_vox.GetYaxis().SetRangeUser(0,max(m) *1.25)\n",
    "      histos_gan.append(h_gan)\n",
    "      h_vox.GetYaxis().SetTitle(\"Entries\")\n",
    "\n",
    "      xAxisTitle = \"Energy [GeV]\"\n",
    "      h_vox.GetXaxis().SetTitle(xAxisTitle)  \n",
    "      h_vox.GetXaxis().SetNdivisions(506)\n",
    "      chi2 = ctypes.c_double(0.)\n",
    "      ndf = ctypes.c_int(0)\n",
    "      igood = ctypes.c_int(0)\n",
    "      histos_vox[index].Chi2TestX(h_gan, chi2, ndf, igood, \"WW\")\n",
    "      ndf = ndf.value\n",
    "      chi2=chi2.value\n",
    "      chi2_tot += chi2\n",
    "      ndf_tot += ndf\n",
    "\n",
    "      if (ndf != 0):\n",
    "        print(\"Iteration %s Energy %s : chi2/ndf = %.1f / %i = %.1f\\n\" % (iteration, energy, chi2, ndf, chi2/ndf))\n",
    "\n",
    "      # Plotting\n",
    "\n",
    "      canvas.cd(index+1)\n",
    "      histos_vox[index].Draw(\"HIST\")\n",
    "      histos_gan[index].Draw(\"HIST same\")\n",
    "\n",
    "      # Legend box                                                                                                                                                                            \n",
    "      if (energy > 1024):\n",
    "          energy_legend =  str(round(energy/1000,1)) + \" GeV\"\n",
    "      else:\n",
    "          energy_legend =  str(energy) + \" MeV\"\n",
    "      t = ROOT.TLatex()\n",
    "      t.SetNDC()\n",
    "      t.SetTextFont(42)\n",
    "      t.SetTextSize(0.1)\n",
    "      t.DrawLatex(0.2, 0.83, energy_legend)\n",
    "   \n",
    "    # Total Energy chi2\n",
    "    chi2_o_ndf = chi2_tot / ndf_tot\n",
    "    print(\"Iteration %s Total Energy : chi2/ndf = %.1f / %i = %.3f\\n\" % (iteration, chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "    chi2File = \"%s/chi2.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "    if chi2_o_ndf > 0:\n",
    "      f = open(chi2File, 'a')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close()\n",
    "    else:\n",
    "      print(\"Something went wrong, chi2 will not be written. Chi2/ndf is %f \" % (chi2_o_ndf))\n",
    "      print(E_tot)\n",
    "      continue\n",
    "\n",
    "    # Legend box particle\n",
    "    leg = MakeLegend( lparams )\n",
    "    leg.SetTextFont( 42 )\n",
    "    leg.SetTextSize(0.1)\n",
    "    canvas.cd(legendPadIndex)\n",
    "    leg.AddEntry(h_vox,\"Geant4\",\"l\") #Geant4\n",
    "    leg.Draw()\n",
    "    leg.AddEntry(h_gan,\"GAN\",\"l\")  #WGAN-GP\n",
    "    leg.Draw('same')\n",
    "    legend = (particleName + \", \" + str('{:.2f}'.format(int(20)/100,2)) + \"<|#eta|<\" + str('{:.2f}'.format((int(20)+5)/100,2)))\n",
    "    ROOT.ATLAS_LABEL_BIG( 0.1, 0.9, ROOT.kBlack, legend )\n",
    "\n",
    "    # Legend box Epoc&chi2 \n",
    "\n",
    "    t = ROOT.TLatex()\n",
    "    t.SetNDC()\n",
    "    t.SetTextFont(42)\n",
    "    t.SetTextSize(0.1)\n",
    "    t.DrawLatex(0.1, 0.18, \"Iter: %s\" % (iteration))\n",
    "    t.DrawLatex(0.1, 0.07, \"#scale[0.8]{#chi^{2}/NDF = %.0f/%i = %.1f}\" % (chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "\n",
    "\n",
    "    #Copy best epoch files, including plots\n",
    "    epochs, chi2_o_ndf_list = np.loadtxt(chi2File, delimiter=' ', unpack=True)\n",
    "    \n",
    "    checkpointName =  \"Plot_comparison_tot_energy\"\n",
    "      \n",
    "    if round(chi2_o_ndf,3) <= np.amin(chi2_o_ndf_list) and chi2_o_ndf > 0:\n",
    "      print (\"Better chi2, creating plots\")\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "     \n",
    "      print(\"Epoch with lowest chi2/ndf is %s with a value of %.3f\" % (epoch, chi2_o_ndf))\n",
    "      #Now save best epoch number to file\n",
    "      chi2File = \"%s/chi2/epoch_best_chi2_%s_%s_%s.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "      f = open(chi2File, 'w')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close() \n",
    "\n",
    "    if (save_plots) :\n",
    "      checkpointName =  \"Plot_comparison_tot_energy_%i\" % (iteration)\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "\n",
    "  except:\n",
    "    print(\"Something went wrong in iteration %s, moving to next one\" % (iteration))\n",
    "    print(\"exception message \", sys.exc_info()[0])     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training and evaluation with 1M iterations\n",
    "The GAN learn but with significant fluctuations [plot](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SIMU-2018-04/fig_09.png)\n",
    "Animated gif with all energies [gif](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-SOFT-PUB-2020-006/fig_37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest results\n",
    "![pions](imgs/latest_pions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba67b957603c6589b793cbc779fadd4d74491f4ed475d4948a7778f403f5ead2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
