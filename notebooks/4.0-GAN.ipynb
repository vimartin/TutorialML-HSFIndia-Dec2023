{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import DataLoader \n",
    "import importlib\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from functools import partial\n",
    "tf.keras.backend.set_floatx('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "noise = layers.Input(shape=(50), name=\"Noise\")\n",
    "condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "con = layers.concatenate([noise,condition])\n",
    "G = layers.Dense(50, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(con)  \n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(100, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(200, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "\n",
    "generator = Model(inputs=[noise, condition], outputs=G)\n",
    "generator.build(370)\n",
    "generator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "\n",
    "image = layers.Input(shape=(368), name=\"Image\")\n",
    "d_condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "d_con = layers.concatenate([image,d_condition])\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(d_con)  \n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(1, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "\n",
    "discriminator = Model(inputs=[image, d_condition], outputs=D)\n",
    "discriminator.build(370)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, loss anf gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def gradient_penalty(f, x_real, x_fake, cond_label, batchsize, D):\n",
    "  alpha = tf.random.uniform([batchsize, 1], minval=0., maxval=1.)\n",
    "\n",
    "  inter = alpha * x_real + (1-alpha) * x_fake\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(inter)\n",
    "    pred = D(inputs=[inter, cond_label])\n",
    "  grad = t.gradient(pred, [inter])[0]\n",
    "  \n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "  gp = 0.00001 * tf.reduce_mean((slopes - 1.)**2) #Lambda\n",
    "  return gp\n",
    "\n",
    "@tf.function\n",
    "def D_loss(x_real, cond_label, batchsize, G, D): \n",
    "  z = tf.random.normal([batchsize, 50], mean=0.5, stddev=0.5, dtype=tf.dtypes.float32) #batch and latent dim\n",
    "  x_fake = G(inputs=[z, cond_label])\n",
    "  D_fake = D(inputs=[x_fake, cond_label])\n",
    "  D_real = D(inputs=[x_real, cond_label])\n",
    "  D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + gradient_penalty(f = partial(D, training=True), x_real = x_real, x_fake = x_fake, cond_label=cond_label, batchsize=batchsize, D=D)\n",
    "  return D_loss, D_fake\n",
    "\n",
    "@tf.function\n",
    "def G_loss(D_fake):\n",
    "  G_loss = -tf.reduce_mean(D_fake)\n",
    "  return G_loss\n",
    "\n",
    "def getTrainData_ultimate( n_iteration, batchsize, dgratio, X ,Labels):\n",
    "  true_batchsize = tf.cast(tf.math.multiply(batchsize, dgratio), tf.int64)\n",
    "  n_samples = tf.cast(tf.gather(tf.shape(X), 0), tf.int64)\n",
    "  n_batch = tf.cast(tf.math.floordiv(n_samples, true_batchsize), tf.int64)\n",
    "  n_shuffles = tf.cast(tf.math.ceil(tf.divide(n_iteration, n_batch)), tf.int64)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((X, Labels))\n",
    "  ds = ds.shuffle(buffer_size = n_samples).repeat(n_shuffles).batch(true_batchsize, drop_remainder=True).prefetch(2)\n",
    "  return iter(ds)\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X_trains, cond_labels, batchsize, dgratio, G, D, generator_optimizer, discriminator_optimizer): \n",
    "  for i in tf.range(dgratio):\n",
    "    print(\"d train: \" + str(i))\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, i), tf.gather(cond_labels, i), batchsize, G, D)\n",
    "      gradients_of_discriminator = disc_tape.gradient(D_loss_curr, D.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, D.trainable_variables))    \n",
    "      \n",
    "  print(\"g train\")\n",
    "  last_index = tf.subtract(dgratio, 1)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    # Need to recompute D_fake, otherwise gen_tape doesn't know the history\n",
    "    (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, last_index), tf.gather(cond_labels, last_index), batchsize, G, D)\n",
    "    G_loss_curr = G_loss(D_fake)\n",
    "    gradients_of_generator = gen_tape.gradient(G_loss_curr, G.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, G.trainable_variables))\n",
    "    return D_loss_curr, G_loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgratio = 5\n",
    "batchsize = 128\n",
    "G_lr = D_lr = 0.0001\n",
    "G_beta1 = D_beta1 = 0.55\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate=G_lr, beta_1=G_beta1)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=D_lr, beta_1=D_beta1)\n",
    "\n",
    "# Prepare for check pointing\n",
    "saver = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                            discriminator_optimizer=discriminator_optimizer,\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "  os.makedirs(checkpoint_dir)\n",
    "\n",
    "print ('training started')\n",
    "dl = DataLoader.DataLoader()\n",
    "\n",
    "start_iteration = 0 \n",
    "max_iterations = 1000\n",
    "\n",
    "for iteration in range(start_iteration,max_iterations): \n",
    "  change_data = (iteration == start_iteration)\n",
    "  \n",
    "  if (change_data == True):\n",
    "    X, Labels = dl.getAllTrainData(8, 9)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    Labels = tf.convert_to_tensor(Labels, dtype=tf.float32)\n",
    " \n",
    "    remained_iteration = tf.constant(max_iterations - iteration, dtype=tf.int64)\n",
    "    ds_iter = getTrainData_ultimate(remained_iteration, batchsize, dgratio, X ,Labels)\n",
    "    print (\"Using \"+ str(X.shape[0])+ \" events\")\n",
    "\n",
    "  X, Labels = ds_iter.get_next()\n",
    "\n",
    "  X_feature_size = tf.gather(tf.shape(X), 1)\n",
    "  Labels_feature_size = tf.gather(tf.shape(Labels), 1)\n",
    "  X_batch_shape = tf.stack((dgratio, batchsize, X_feature_size), axis=0)\n",
    "  Labels_batch_shape = tf.stack((dgratio, batchsize, Labels_feature_size), axis=0)\n",
    "\n",
    "  X_trains    = tf.reshape(X, X_batch_shape)\n",
    "  cond_labels = tf.reshape(Labels, Labels_batch_shape)  \n",
    "\n",
    "  #print(X_trains) \n",
    "  #print(cond_labels) \n",
    "  #print(batchsize) \n",
    "  #print(dgratio) \n",
    "  #generator.summary() \n",
    "  #discriminator.summary() \n",
    "\n",
    "  D_loss_curr, G_loss_curr = train_loop(X_trains, cond_labels, batchsize, dgratio, generator, discriminator,  generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "  if iteration == 0: \n",
    "    print(\"Model and loss values will be saved every 2 iterations.\" )\n",
    "  \n",
    "  if iteration % 2 == 0 and iteration > 0:\n",
    "\n",
    "    try:\n",
    "      saver.save(file_prefix = checkpoint_dir+ '/model')\n",
    "    except:\n",
    "      print(\"Something went wrong in saving iteration %s, moving to next one\" % (iteration))\n",
    "      print(\"exception message \", sys.exc_info()[0])     \n",
    "    \n",
    "    print('Iter: {}; D loss: {:.4}; G_loss:  {:.4}'.format(iteration, D_loss_curr, G_loss_curr))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Noise (InputLayer)             [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 52)           0           ['Noise[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 50)           2650        ['concatenate_30[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 50)          200         ['dense_120[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 50)           0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 100)          5100        ['activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 100)         400         ['dense_121[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 100)          0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_122 (Dense)              (None, 200)          20200       ['activation_106[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 200)         800         ['dense_122[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 200)          0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " dense_123 (Dense)              (None, 368)          73968       ['activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 368)         1472        ['dense_123[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 368)          0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 104,790\n",
      "Trainable params: 103,354\n",
      "Non-trainable params: 1,436\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image (InputLayer)             [(None, 368)]        0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 370)          0           ['Image[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_124 (Dense)              (None, 368)          136528      ['concatenate_31[0][0]']         \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 368)          0           ['dense_124[0][0]']              \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 368)          135792      ['activation_109[0][0]']         \n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 368)          0           ['dense_125[0][0]']              \n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 368)          135792      ['activation_110[0][0]']         \n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 368)          0           ['dense_126[0][0]']              \n",
      "                                                                                                  \n",
      " dense_127 (Dense)              (None, 1)            369         ['activation_111[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,481\n",
      "Trainable params: 408,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "training started\n",
      "************* DATA READER ***************\n",
      "Loading data\n",
      "[256, 512]\n",
      "----Loading files----\n",
      "Opening file gan_inputs/pid22_E256_eta_20_25_voxalisation.csv\n",
      "Loaded momentum 256\n",
      "from file gan_inputs/pid22_E256_eta_20_25_voxalisation.csv\n",
      "with 10000 events\n",
      "Vector of data of size 368\n",
      "Opening file gan_inputs/pid22_E512_eta_20_25_voxalisation.csv\n",
      "Loaded momentum 512\n",
      "from file gan_inputs/pid22_E512_eta_20_25_voxalisation.csv\n",
      "with 10000 events\n",
      "Vector of data of size 368\n",
      "Data was normalised by 256.000000\n",
      "Data was normalised by 512.000000\n",
      "Using 20000 events\n",
      "Model and loss values will be saved every 2 iterations.\n",
      "Something went wrong in saving iteration 2, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 2; D loss: -2.403; G_loss:  2.52\n",
      "Something went wrong in saving iteration 4, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 4; D loss: -4.854; G_loss:  5.057\n",
      "Something went wrong in saving iteration 6, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 6; D loss: -9.321; G_loss:  9.678\n",
      "Something went wrong in saving iteration 8, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 8; D loss: -15.83; G_loss:  16.42\n",
      "Something went wrong in saving iteration 10, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 10; D loss: -27.45; G_loss:  28.41\n",
      "Something went wrong in saving iteration 12, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 12; D loss: -41.77; G_loss:  43.37\n",
      "Something went wrong in saving iteration 14, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 14; D loss: -65.67; G_loss:  68.0\n",
      "Something went wrong in saving iteration 16, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 16; D loss: -93.99; G_loss:  97.63\n",
      "Something went wrong in saving iteration 18, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 18; D loss: -121.6; G_loss:  126.7\n",
      "Something went wrong in saving iteration 20, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 20; D loss: -142.8; G_loss:  149.9\n",
      "Something went wrong in saving iteration 22, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 22; D loss: -179.8; G_loss:  189.6\n",
      "Something went wrong in saving iteration 24, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 24; D loss: -202.3; G_loss:  215.2\n",
      "Something went wrong in saving iteration 26, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 26; D loss: -233.8; G_loss:  250.5\n",
      "Something went wrong in saving iteration 28, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 28; D loss: -246.0; G_loss:  266.8\n",
      "Something went wrong in saving iteration 30, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 30; D loss: -245.2; G_loss:  268.5\n",
      "Something went wrong in saving iteration 32, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 32; D loss: -236.8; G_loss:  263.6\n",
      "Something went wrong in saving iteration 34, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 34; D loss: -258.8; G_loss:  288.9\n",
      "Something went wrong in saving iteration 36, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 36; D loss: -257.1; G_loss:  290.9\n",
      "Something went wrong in saving iteration 38, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 38; D loss: -250.6; G_loss:  284.1\n",
      "Something went wrong in saving iteration 40, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 40; D loss: -272.6; G_loss:  307.3\n",
      "Something went wrong in saving iteration 42, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 42; D loss: -284.8; G_loss:  322.9\n",
      "Something went wrong in saving iteration 44, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 44; D loss: -261.3; G_loss:  298.0\n",
      "Something went wrong in saving iteration 46, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 46; D loss: -293.1; G_loss:  328.3\n",
      "Something went wrong in saving iteration 48, moving to next one\n",
      "exception message  <class 'NameError'>\n",
      "Iter: 48; D loss: -277.0; G_loss:  316.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e27949df10b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconditional_wgangp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWGANGP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\miche\\Documents\\Post-doc Roma\\ATLAS Italia\\TutorialML-AtlasItalia2022\\notebooks\\gan_code\\conditional_wgangp.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mX_trains\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_batch_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mcond_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabels_batch_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mD_loss_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trains\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "\n",
    "gan = conditional_wgangp.WGANGP()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba67b957603c6589b793cbc779fadd4d74491f4ed475d4948a7778f403f5ead2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
