{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import DataLoader \n",
    "import importlib\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from functools import partial\n",
    "tf.keras.backend.set_floatx('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "noise = layers.Input(shape=(50), name=\"Noise\")\n",
    "condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "con = layers.concatenate([noise,condition])\n",
    "G = layers.Dense(50, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(con)  \n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(100, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(200, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "\n",
    "generator = Model(inputs=[noise, condition], outputs=G)\n",
    "generator.build(370)\n",
    "generator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "\n",
    "image = layers.Input(shape=(368), name=\"Image\")\n",
    "d_condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "d_con = layers.concatenate([image,d_condition])\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(d_con)  \n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(1, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "\n",
    "discriminator = Model(inputs=[image, d_condition], outputs=D)\n",
    "discriminator.build(370)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, loss anf gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def gradient_penalty(f, x_real, x_fake, cond_label, batchsize, D):\n",
    "  alpha = tf.random.uniform([batchsize, 1], minval=0., maxval=1.)\n",
    "\n",
    "  inter = alpha * x_real + (1-alpha) * x_fake\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(inter)\n",
    "    pred = D(inputs=[inter, cond_label])\n",
    "  grad = t.gradient(pred, [inter])[0]\n",
    "  \n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "  gp = 0.00001 * tf.reduce_mean((slopes - 1.)**2) #Lambda\n",
    "  return gp\n",
    "\n",
    "@tf.function\n",
    "def D_loss(x_real, cond_label, batchsize, G, D): \n",
    "  z = tf.random.normal([batchsize, 50], mean=0.5, stddev=0.5, dtype=tf.dtypes.float32) #batch and latent dim\n",
    "  x_fake = G(inputs=[z, cond_label])\n",
    "  D_fake = D(inputs=[x_fake, cond_label])\n",
    "  D_real = D(inputs=[x_real, cond_label])\n",
    "  D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + gradient_penalty(f = partial(D, training=True), x_real = x_real, x_fake = x_fake, cond_label=cond_label, batchsize=batchsize, D=D)\n",
    "  return D_loss, D_fake\n",
    "\n",
    "@tf.function\n",
    "def G_loss(D_fake):\n",
    "  G_loss = -tf.reduce_mean(D_fake)\n",
    "  return G_loss\n",
    "\n",
    "def getTrainData_ultimate( n_iteration, batchsize, dgratio, X ,Labels):\n",
    "  true_batchsize = tf.cast(tf.math.multiply(batchsize, dgratio), tf.int64)\n",
    "  n_samples = tf.cast(tf.gather(tf.shape(X), 0), tf.int64)\n",
    "  n_batch = tf.cast(tf.math.floordiv(n_samples, true_batchsize), tf.int64)\n",
    "  n_shuffles = tf.cast(tf.math.ceil(tf.divide(n_iteration, n_batch)), tf.int64)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((X, Labels))\n",
    "  ds = ds.shuffle(buffer_size = n_samples).repeat(n_shuffles).batch(true_batchsize, drop_remainder=True).prefetch(2)\n",
    "  return iter(ds)\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X_trains, cond_labels, batchsize, dgratio, G, D, generator_optimizer, discriminator_optimizer): \n",
    "  for i in tf.range(dgratio):\n",
    "    print(\"d train: \" + str(i))\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, i), tf.gather(cond_labels, i), batchsize, G, D)\n",
    "      gradients_of_discriminator = disc_tape.gradient(D_loss_curr, D.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, D.trainable_variables))    \n",
    "      \n",
    "  print(\"g train\")\n",
    "  last_index = tf.subtract(dgratio, 1)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    # Need to recompute D_fake, otherwise gen_tape doesn't know the history\n",
    "    (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, last_index), tf.gather(cond_labels, last_index), batchsize, G, D)\n",
    "    G_loss_curr = G_loss(D_fake)\n",
    "    gradients_of_generator = gen_tape.gradient(G_loss_curr, G.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, G.trainable_variables))\n",
    "    return D_loss_curr, G_loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgratio = 5\n",
    "batchsize = 128\n",
    "G_lr = D_lr = 0.0001\n",
    "G_beta1 = D_beta1 = 0.55\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate=G_lr, beta_1=G_beta1)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=D_lr, beta_1=D_beta1)\n",
    "\n",
    "# Prepare for check pointing\n",
    "saver = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                            discriminator_optimizer=discriminator_optimizer,\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "  os.makedirs(checkpoint_dir)\n",
    "\n",
    "print ('training started')\n",
    "dl = DataLoader.DataLoader()\n",
    "\n",
    "start_iteration = 0 \n",
    "max_iterations = 1000\n",
    "\n",
    "for iteration in range(start_iteration,max_iterations): \n",
    "  change_data = (iteration == start_iteration)\n",
    "  \n",
    "  if (change_data == True):\n",
    "    X, Labels = dl.getAllTrainData(8, 9)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    Labels = tf.convert_to_tensor(Labels, dtype=tf.float32)\n",
    " \n",
    "    remained_iteration = tf.constant(max_iterations - iteration, dtype=tf.int64)\n",
    "    ds_iter = getTrainData_ultimate(remained_iteration, batchsize, dgratio, X ,Labels)\n",
    "    print (\"Using \"+ str(X.shape[0])+ \" events\")\n",
    "\n",
    "  X, Labels = ds_iter.get_next()\n",
    "\n",
    "  X_feature_size = tf.gather(tf.shape(X), 1)\n",
    "  Labels_feature_size = tf.gather(tf.shape(Labels), 1)\n",
    "  X_batch_shape = tf.stack((dgratio, batchsize, X_feature_size), axis=0)\n",
    "  Labels_batch_shape = tf.stack((dgratio, batchsize, Labels_feature_size), axis=0)\n",
    "\n",
    "  X_trains    = tf.reshape(X, X_batch_shape)\n",
    "  cond_labels = tf.reshape(Labels, Labels_batch_shape)  \n",
    "\n",
    "  #print(X_trains) \n",
    "  #print(cond_labels) \n",
    "  #print(batchsize) \n",
    "  #print(dgratio) \n",
    "  #generator.summary() \n",
    "  #discriminator.summary() \n",
    "\n",
    "  D_loss_curr, G_loss_curr = train_loop(X_trains, cond_labels, batchsize, dgratio, generator, discriminator,  generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "  if iteration == 0: \n",
    "    print(\"Model and loss values will be saved every 2 iterations.\" )\n",
    "  \n",
    "  if iteration % 2 == 0 and iteration > 0:\n",
    "\n",
    "    try:\n",
    "      saver.save(file_prefix = checkpoint_dir+ '/model')\n",
    "    except:\n",
    "      print(\"Something went wrong in saving iteration %s, moving to next one\" % (iteration))\n",
    "      print(\"exception message \", sys.exc_info()[0])     \n",
    "    \n",
    "    print('Iter: {}; D loss: {:.4}; G_loss:  {:.4}'.format(iteration, D_loss_curr, G_loss_curr))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Noise (InputLayer)             [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 52)           0           ['Noise[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_128 (Dense)              (None, 50)           2650        ['concatenate_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 50)          200         ['dense_128[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 50)           0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " dense_129 (Dense)              (None, 100)          5100        ['activation_112[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 100)         400         ['dense_129[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 100)          0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 200)          20200       ['activation_113[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 200)         800         ['dense_130[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 200)          0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 368)          73968       ['activation_114[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 368)         1472        ['dense_131[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 368)          0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 104,790\n",
      "Trainable params: 103,354\n",
      "Non-trainable params: 1,436\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image (InputLayer)             [(None, 368)]        0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 370)          0           ['Image[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_132 (Dense)              (None, 368)          136528      ['concatenate_33[0][0]']         \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 368)          0           ['dense_132[0][0]']              \n",
      "                                                                                                  \n",
      " dense_133 (Dense)              (None, 368)          135792      ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 368)          0           ['dense_133[0][0]']              \n",
      "                                                                                                  \n",
      " dense_134 (Dense)              (None, 368)          135792      ['activation_117[0][0]']         \n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 368)          0           ['dense_134[0][0]']              \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 1)            369         ['activation_118[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,481\n",
      "Trainable params: 408,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "training started\n",
      "************* DATA READER ***************\n",
      "Loading data\n",
      "[256, 512]\n",
      "----Loading files----\n",
      "Opening file gan_inputs/pid22_E256_eta_20_25_voxalisation.csv\n",
      "Loaded momentum 256\n",
      "from file gan_inputs/pid22_E256_eta_20_25_voxalisation.csv\n",
      "with 10000 events\n",
      "Vector of data of size 368\n",
      "Opening file gan_inputs/pid22_E512_eta_20_25_voxalisation.csv\n",
      "Loaded momentum 512\n",
      "from file gan_inputs/pid22_E512_eta_20_25_voxalisation.csv\n",
      "with 10000 events\n",
      "Vector of data of size 368\n",
      "Data was normalised by 256.000000\n",
      "Data was normalised by 512.000000\n",
      "Using 20000 events\n",
      "Model and loss values will be saved every 2 iterations.\n",
      "Iter: 2; D loss: -3.156; G_loss:  3.173\n",
      "Iter: 4; D loss: -6.733; G_loss:  6.803\n",
      "Iter: 6; D loss: -12.82; G_loss:  12.98\n",
      "Iter: 8; D loss: -24.81; G_loss:  25.07\n",
      "Iter: 10; D loss: -40.62; G_loss:  41.08\n",
      "Iter: 12; D loss: -65.71; G_loss:  66.44\n",
      "Iter: 14; D loss: -105.7; G_loss:  106.7\n",
      "Iter: 16; D loss: -150.7; G_loss:  151.9\n",
      "Iter: 18; D loss: -182.6; G_loss:  184.6\n",
      "Iter: 20; D loss: -237.9; G_loss:  240.5\n",
      "Iter: 22; D loss: -321.7; G_loss:  325.3\n",
      "Iter: 24; D loss: -359.6; G_loss:  365.0\n",
      "Iter: 26; D loss: -360.0; G_loss:  366.6\n",
      "Iter: 28; D loss: -378.4; G_loss:  387.6\n",
      "Iter: 30; D loss: -430.5; G_loss:  442.0\n",
      "Iter: 32; D loss: -426.3; G_loss:  440.6\n",
      "Iter: 34; D loss: -406.1; G_loss:  420.6\n",
      "Iter: 36; D loss: -454.5; G_loss:  471.7\n",
      "Iter: 38; D loss: -443.5; G_loss:  462.5\n",
      "Iter: 40; D loss: -393.2; G_loss:  408.3\n",
      "Iter: 42; D loss: -385.4; G_loss:  398.9\n",
      "Iter: 44; D loss: -386.2; G_loss:  404.1\n",
      "Iter: 46; D loss: -363.6; G_loss:  377.3\n",
      "Iter: 48; D loss: -360.1; G_loss:  369.0\n",
      "Iter: 50; D loss: -344.1; G_loss:  344.3\n",
      "Iter: 52; D loss: -331.6; G_loss:  326.3\n",
      "Iter: 54; D loss: -343.5; G_loss:  329.5\n",
      "Iter: 56; D loss: -320.4; G_loss:  301.6\n",
      "Iter: 58; D loss: -320.1; G_loss:  289.2\n",
      "Iter: 60; D loss: -317.3; G_loss:  276.5\n",
      "Iter: 62; D loss: -353.6; G_loss:  296.0\n",
      "Iter: 64; D loss: -352.9; G_loss:  278.9\n",
      "Iter: 66; D loss: -371.8; G_loss:  281.2\n",
      "Iter: 68; D loss: -395.7; G_loss:  294.2\n",
      "Iter: 70; D loss: -424.1; G_loss:  297.4\n",
      "Iter: 72; D loss: -455.2; G_loss:  312.3\n",
      "Iter: 74; D loss: -442.4; G_loss:  279.6\n",
      "Iter: 76; D loss: -496.0; G_loss:  315.7\n",
      "Iter: 78; D loss: -519.8; G_loss:  319.6\n",
      "Iter: 80; D loss: -581.7; G_loss:  344.1\n",
      "Iter: 82; D loss: -615.1; G_loss:  350.2\n",
      "Iter: 84; D loss: -638.4; G_loss:  358.5\n",
      "Iter: 86; D loss: -728.9; G_loss:  400.3\n",
      "Iter: 88; D loss: -782.2; G_loss:  420.5\n",
      "Iter: 90; D loss: -815.8; G_loss:  421.5\n",
      "Iter: 92; D loss: -897.1; G_loss:  471.2\n",
      "Iter: 94; D loss: -979.2; G_loss:  512.5\n",
      "Iter: 96; D loss: -1.032e+03; G_loss:  541.1\n",
      "Iter: 98; D loss: -1.132e+03; G_loss:  577.0\n",
      "Iter: 100; D loss: -1.161e+03; G_loss:  636.9\n",
      "Iter: 102; D loss: -1.227e+03; G_loss:  628.0\n",
      "Iter: 104; D loss: -1.32e+03; G_loss:  670.1\n",
      "Iter: 106; D loss: -1.335e+03; G_loss:  679.0\n",
      "Iter: 108; D loss: -1.445e+03; G_loss:  736.6\n",
      "Iter: 110; D loss: -1.422e+03; G_loss:  716.5\n",
      "Iter: 112; D loss: -1.496e+03; G_loss:  773.7\n",
      "Iter: 114; D loss: -1.576e+03; G_loss:  813.6\n",
      "Iter: 116; D loss: -1.569e+03; G_loss:  730.3\n",
      "Iter: 118; D loss: -1.669e+03; G_loss:  817.7\n",
      "Iter: 120; D loss: -1.571e+03; G_loss:  745.7\n",
      "Iter: 122; D loss: -1.79e+03; G_loss:  938.2\n",
      "Iter: 124; D loss: -1.691e+03; G_loss:  844.9\n",
      "Iter: 126; D loss: -1.773e+03; G_loss:  890.4\n",
      "Iter: 128; D loss: -1.66e+03; G_loss:  839.8\n",
      "Iter: 130; D loss: -1.717e+03; G_loss:  837.3\n",
      "Iter: 132; D loss: -1.652e+03; G_loss:  808.0\n",
      "Iter: 134; D loss: -1.694e+03; G_loss:  839.2\n",
      "Iter: 136; D loss: -1.662e+03; G_loss:  787.4\n",
      "Iter: 138; D loss: -1.619e+03; G_loss:  761.7\n",
      "Iter: 140; D loss: -1.616e+03; G_loss:  751.8\n",
      "Iter: 142; D loss: -1.585e+03; G_loss:  717.4\n",
      "Iter: 144; D loss: -1.628e+03; G_loss:  797.3\n",
      "Iter: 146; D loss: -1.592e+03; G_loss:  647.7\n",
      "Iter: 148; D loss: -1.481e+03; G_loss:  658.0\n",
      "Iter: 150; D loss: -1.466e+03; G_loss:  692.9\n",
      "Iter: 152; D loss: -1.419e+03; G_loss:  548.4\n",
      "Iter: 154; D loss: -1.218e+03; G_loss:  510.0\n",
      "Iter: 156; D loss: -1.349e+03; G_loss:  548.8\n",
      "Iter: 158; D loss: -1.278e+03; G_loss:  476.9\n",
      "Iter: 160; D loss: -1.312e+03; G_loss:  467.7\n",
      "Iter: 162; D loss: -1.198e+03; G_loss:  436.6\n",
      "Iter: 164; D loss: -1.205e+03; G_loss:  435.8\n",
      "Iter: 166; D loss: -1.197e+03; G_loss:  347.7\n",
      "Iter: 168; D loss: -1.16e+03; G_loss:  345.5\n",
      "Iter: 170; D loss: -1.118e+03; G_loss:  245.0\n",
      "Iter: 172; D loss: -1.086e+03; G_loss:  263.2\n",
      "Iter: 174; D loss: -995.6; G_loss:  220.2\n",
      "Iter: 176; D loss: -928.8; G_loss:  73.08\n",
      "Iter: 178; D loss: -899.9; G_loss:  20.21\n",
      "Iter: 180; D loss: -853.9; G_loss:  36.99\n",
      "Iter: 182; D loss: -895.4; G_loss:  75.31\n",
      "Iter: 184; D loss: -898.9; G_loss:  38.15\n",
      "Iter: 186; D loss: -797.8; G_loss:  -58.47\n",
      "Iter: 188; D loss: -834.3; G_loss:  85.94\n",
      "Iter: 190; D loss: -701.8; G_loss:  -178.2\n",
      "Iter: 192; D loss: -760.5; G_loss:  -68.33\n",
      "Iter: 194; D loss: -742.0; G_loss:  -135.4\n",
      "Iter: 196; D loss: -769.6; G_loss:  -212.1\n",
      "Iter: 198; D loss: -697.1; G_loss:  -275.9\n",
      "Iter: 200; D loss: -665.9; G_loss:  -300.5\n",
      "Iter: 202; D loss: -647.2; G_loss:  -308.0\n",
      "Iter: 204; D loss: -619.9; G_loss:  -271.4\n",
      "Iter: 206; D loss: -603.2; G_loss:  -357.6\n",
      "Iter: 208; D loss: -608.2; G_loss:  -407.5\n",
      "Iter: 210; D loss: -509.9; G_loss:  -499.3\n",
      "Iter: 212; D loss: -582.3; G_loss:  -431.7\n",
      "Iter: 214; D loss: -602.8; G_loss:  -429.4\n",
      "Iter: 216; D loss: -528.6; G_loss:  -460.8\n",
      "Iter: 218; D loss: -521.8; G_loss:  -494.1\n",
      "Iter: 220; D loss: -525.8; G_loss:  -549.0\n",
      "Iter: 222; D loss: -509.8; G_loss:  -566.1\n",
      "Iter: 224; D loss: -531.9; G_loss:  -524.4\n",
      "Iter: 226; D loss: -496.7; G_loss:  -554.6\n",
      "Iter: 228; D loss: -457.7; G_loss:  -680.9\n",
      "Iter: 230; D loss: -482.4; G_loss:  -580.9\n",
      "Iter: 232; D loss: -470.9; G_loss:  -541.7\n",
      "Iter: 234; D loss: -446.7; G_loss:  -661.3\n",
      "Iter: 236; D loss: -418.1; G_loss:  -652.3\n",
      "Iter: 238; D loss: -401.0; G_loss:  -606.3\n",
      "Iter: 240; D loss: -437.1; G_loss:  -645.1\n",
      "Iter: 242; D loss: -413.9; G_loss:  -717.9\n",
      "Iter: 244; D loss: -393.4; G_loss:  -584.1\n",
      "Iter: 246; D loss: -392.9; G_loss:  -743.2\n",
      "Iter: 248; D loss: -381.6; G_loss:  -701.7\n",
      "Iter: 250; D loss: -385.2; G_loss:  -709.1\n",
      "Iter: 252; D loss: -334.7; G_loss:  -769.5\n",
      "Iter: 254; D loss: -349.3; G_loss:  -811.6\n",
      "Iter: 256; D loss: -339.2; G_loss:  -788.7\n",
      "Iter: 258; D loss: -362.8; G_loss:  -746.4\n",
      "Iter: 260; D loss: -309.6; G_loss:  -882.0\n",
      "Iter: 262; D loss: -313.3; G_loss:  -852.5\n",
      "Iter: 264; D loss: -316.4; G_loss:  -799.3\n",
      "Iter: 266; D loss: -289.0; G_loss:  -887.4\n",
      "Iter: 268; D loss: -321.6; G_loss:  -827.8\n",
      "Iter: 270; D loss: -306.6; G_loss:  -819.9\n",
      "Iter: 272; D loss: -325.9; G_loss:  -838.2\n",
      "Iter: 274; D loss: -286.5; G_loss:  -841.8\n",
      "Iter: 276; D loss: -296.4; G_loss:  -830.3\n",
      "Iter: 278; D loss: -309.2; G_loss:  -792.8\n",
      "Iter: 280; D loss: -287.7; G_loss:  -870.0\n",
      "Iter: 282; D loss: -294.0; G_loss:  -881.8\n",
      "Iter: 284; D loss: -254.4; G_loss:  -883.0\n",
      "Iter: 286; D loss: -248.4; G_loss:  -910.1\n",
      "Iter: 288; D loss: -243.8; G_loss:  -868.3\n",
      "Iter: 290; D loss: -260.3; G_loss:  -848.8\n",
      "Iter: 292; D loss: -257.4; G_loss:  -885.4\n",
      "Iter: 294; D loss: -259.3; G_loss:  -892.2\n",
      "Iter: 296; D loss: -249.0; G_loss:  -931.7\n",
      "Iter: 298; D loss: -210.2; G_loss:  -922.5\n",
      "Iter: 300; D loss: -244.4; G_loss:  -953.5\n",
      "Iter: 302; D loss: -219.7; G_loss:  -893.7\n",
      "Iter: 304; D loss: -219.7; G_loss:  -932.3\n",
      "Iter: 306; D loss: -220.1; G_loss:  -907.4\n",
      "Iter: 308; D loss: -203.1; G_loss:  -986.9\n",
      "Iter: 310; D loss: -214.7; G_loss:  -952.3\n",
      "Iter: 312; D loss: -201.1; G_loss:  -1.007e+03\n",
      "Iter: 314; D loss: -197.1; G_loss:  -979.6\n",
      "Iter: 316; D loss: -189.9; G_loss:  -1.003e+03\n",
      "Iter: 318; D loss: -234.6; G_loss:  -890.4\n",
      "Iter: 320; D loss: -199.4; G_loss:  -962.8\n",
      "Iter: 322; D loss: -209.4; G_loss:  -965.8\n",
      "Iter: 324; D loss: -205.1; G_loss:  -944.6\n",
      "Iter: 326; D loss: -191.4; G_loss:  -984.0\n",
      "Iter: 328; D loss: -174.8; G_loss:  -1.048e+03\n",
      "Iter: 330; D loss: -190.4; G_loss:  -1.046e+03\n",
      "Iter: 332; D loss: -185.5; G_loss:  -1.021e+03\n",
      "Iter: 334; D loss: -187.5; G_loss:  -978.1\n",
      "Iter: 336; D loss: -175.9; G_loss:  -978.6\n",
      "Iter: 338; D loss: -194.2; G_loss:  -1e+03\n",
      "Iter: 340; D loss: -173.2; G_loss:  -1.033e+03\n",
      "Iter: 342; D loss: -183.4; G_loss:  -983.3\n",
      "Iter: 344; D loss: -180.1; G_loss:  -1.089e+03\n",
      "Iter: 346; D loss: -158.6; G_loss:  -1.01e+03\n",
      "Iter: 348; D loss: -150.3; G_loss:  -1.048e+03\n",
      "Iter: 350; D loss: -162.6; G_loss:  -1.031e+03\n",
      "Iter: 352; D loss: -144.4; G_loss:  -1.075e+03\n",
      "Iter: 354; D loss: -144.6; G_loss:  -1.002e+03\n",
      "Iter: 356; D loss: -176.4; G_loss:  -1.026e+03\n",
      "Iter: 358; D loss: -168.6; G_loss:  -958.6\n",
      "Iter: 360; D loss: -133.6; G_loss:  -1.079e+03\n",
      "Iter: 362; D loss: -160.7; G_loss:  -1.091e+03\n",
      "Iter: 364; D loss: -142.8; G_loss:  -1.05e+03\n",
      "Iter: 366; D loss: -154.0; G_loss:  -1.002e+03\n",
      "Iter: 368; D loss: -155.6; G_loss:  -1.036e+03\n",
      "Iter: 370; D loss: -141.0; G_loss:  -1.03e+03\n",
      "Iter: 372; D loss: -164.6; G_loss:  -1.058e+03\n",
      "Iter: 374; D loss: -163.7; G_loss:  -960.7\n",
      "Iter: 376; D loss: -155.7; G_loss:  -1.002e+03\n",
      "Iter: 378; D loss: -130.9; G_loss:  -1.038e+03\n",
      "Iter: 380; D loss: -151.7; G_loss:  -1.055e+03\n",
      "Iter: 382; D loss: -136.9; G_loss:  -1.044e+03\n",
      "Iter: 384; D loss: -135.5; G_loss:  -1.051e+03\n",
      "Iter: 386; D loss: -132.6; G_loss:  -1.045e+03\n",
      "Iter: 388; D loss: -141.5; G_loss:  -977.1\n",
      "Iter: 390; D loss: -135.2; G_loss:  -1.066e+03\n",
      "Iter: 392; D loss: -123.7; G_loss:  -1.095e+03\n",
      "Iter: 394; D loss: -125.3; G_loss:  -1.11e+03\n",
      "Iter: 396; D loss: -118.7; G_loss:  -1.082e+03\n",
      "Iter: 398; D loss: -108.0; G_loss:  -1.082e+03\n",
      "Iter: 400; D loss: -97.39; G_loss:  -1.099e+03\n",
      "Iter: 402; D loss: -106.6; G_loss:  -1.1e+03\n",
      "Iter: 404; D loss: -128.8; G_loss:  -1.071e+03\n",
      "Iter: 406; D loss: -100.0; G_loss:  -1.047e+03\n",
      "Iter: 408; D loss: -117.5; G_loss:  -1.106e+03\n",
      "Iter: 410; D loss: -122.7; G_loss:  -1.019e+03\n",
      "Iter: 412; D loss: -105.9; G_loss:  -1.095e+03\n",
      "Iter: 414; D loss: -105.9; G_loss:  -1.086e+03\n",
      "Iter: 416; D loss: -99.91; G_loss:  -1.036e+03\n",
      "Iter: 418; D loss: -97.61; G_loss:  -1.074e+03\n",
      "Iter: 420; D loss: -106.8; G_loss:  -1.136e+03\n",
      "Iter: 422; D loss: -110.5; G_loss:  -1.102e+03\n",
      "Iter: 424; D loss: -104.5; G_loss:  -1.107e+03\n",
      "Iter: 426; D loss: -103.8; G_loss:  -1.104e+03\n",
      "Iter: 428; D loss: -109.2; G_loss:  -1.088e+03\n",
      "Iter: 430; D loss: -103.5; G_loss:  -1.031e+03\n",
      "Iter: 432; D loss: -93.1; G_loss:  -1.133e+03\n",
      "Iter: 434; D loss: -106.4; G_loss:  -1.065e+03\n",
      "Iter: 436; D loss: -121.9; G_loss:  -1.046e+03\n",
      "Iter: 438; D loss: -106.9; G_loss:  -1.072e+03\n",
      "Iter: 440; D loss: -98.16; G_loss:  -1.091e+03\n",
      "Iter: 442; D loss: -104.5; G_loss:  -1.099e+03\n",
      "Iter: 444; D loss: -101.2; G_loss:  -975.1\n",
      "Iter: 446; D loss: -109.2; G_loss:  -1.074e+03\n",
      "Iter: 448; D loss: -96.52; G_loss:  -1.168e+03\n",
      "Iter: 450; D loss: -106.7; G_loss:  -1.086e+03\n",
      "Iter: 452; D loss: -94.07; G_loss:  -1.062e+03\n",
      "Iter: 454; D loss: -96.51; G_loss:  -1.13e+03\n",
      "Iter: 456; D loss: -96.53; G_loss:  -1.101e+03\n",
      "Iter: 458; D loss: -93.3; G_loss:  -1.033e+03\n",
      "Iter: 460; D loss: -94.38; G_loss:  -1.129e+03\n",
      "Iter: 462; D loss: -110.4; G_loss:  -1.076e+03\n",
      "Iter: 464; D loss: -92.34; G_loss:  -1.079e+03\n",
      "Iter: 466; D loss: -89.09; G_loss:  -1.067e+03\n",
      "Iter: 468; D loss: -97.41; G_loss:  -1.03e+03\n",
      "Iter: 470; D loss: -85.54; G_loss:  -1.102e+03\n",
      "Iter: 472; D loss: -82.63; G_loss:  -1.109e+03\n",
      "Iter: 474; D loss: -88.53; G_loss:  -1.157e+03\n",
      "Iter: 476; D loss: -74.85; G_loss:  -1.085e+03\n",
      "Iter: 478; D loss: -82.94; G_loss:  -1.129e+03\n",
      "Iter: 480; D loss: -78.9; G_loss:  -1.162e+03\n",
      "Iter: 482; D loss: -82.09; G_loss:  -1.142e+03\n",
      "Iter: 484; D loss: -88.96; G_loss:  -1.085e+03\n",
      "Iter: 486; D loss: -85.65; G_loss:  -1.143e+03\n",
      "Iter: 488; D loss: -77.44; G_loss:  -1.095e+03\n",
      "Iter: 490; D loss: -79.32; G_loss:  -1.079e+03\n",
      "Iter: 492; D loss: -71.56; G_loss:  -1.131e+03\n",
      "Iter: 494; D loss: -69.07; G_loss:  -1.096e+03\n",
      "Iter: 496; D loss: -81.63; G_loss:  -1.111e+03\n",
      "Iter: 498; D loss: -74.23; G_loss:  -1.09e+03\n",
      "Iter: 500; D loss: -80.54; G_loss:  -1.112e+03\n",
      "Iter: 502; D loss: -89.76; G_loss:  -1.093e+03\n",
      "Iter: 504; D loss: -78.88; G_loss:  -1.127e+03\n",
      "Iter: 506; D loss: -78.86; G_loss:  -1.17e+03\n",
      "Iter: 508; D loss: -78.19; G_loss:  -1.098e+03\n",
      "Iter: 510; D loss: -77.48; G_loss:  -1.103e+03\n",
      "Iter: 512; D loss: -78.61; G_loss:  -1.127e+03\n",
      "Iter: 514; D loss: -74.46; G_loss:  -1.083e+03\n",
      "Iter: 516; D loss: -72.3; G_loss:  -1.097e+03\n",
      "Iter: 518; D loss: -68.94; G_loss:  -1.1e+03\n",
      "Iter: 520; D loss: -67.78; G_loss:  -1.099e+03\n",
      "Iter: 522; D loss: -79.64; G_loss:  -1.088e+03\n",
      "Iter: 524; D loss: -56.83; G_loss:  -1.165e+03\n",
      "Iter: 526; D loss: -73.53; G_loss:  -1.167e+03\n",
      "Iter: 528; D loss: -70.86; G_loss:  -1.121e+03\n",
      "Iter: 530; D loss: -70.19; G_loss:  -1.129e+03\n",
      "Iter: 532; D loss: -63.07; G_loss:  -1.096e+03\n",
      "Iter: 534; D loss: -63.65; G_loss:  -1.073e+03\n",
      "Iter: 536; D loss: -75.69; G_loss:  -1.168e+03\n",
      "Iter: 538; D loss: -64.13; G_loss:  -1.134e+03\n",
      "Iter: 540; D loss: -56.63; G_loss:  -1.118e+03\n",
      "Iter: 542; D loss: -57.15; G_loss:  -1.064e+03\n",
      "Iter: 544; D loss: -74.46; G_loss:  -1.112e+03\n",
      "Iter: 546; D loss: -58.17; G_loss:  -1.131e+03\n",
      "Iter: 548; D loss: -69.48; G_loss:  -1.136e+03\n",
      "Iter: 550; D loss: -52.38; G_loss:  -1.117e+03\n",
      "Iter: 552; D loss: -54.73; G_loss:  -1.11e+03\n",
      "Iter: 554; D loss: -67.3; G_loss:  -1.083e+03\n",
      "Iter: 556; D loss: -57.27; G_loss:  -1.12e+03\n",
      "Iter: 558; D loss: -57.5; G_loss:  -1.113e+03\n",
      "Iter: 560; D loss: -53.75; G_loss:  -1.113e+03\n",
      "Iter: 562; D loss: -67.38; G_loss:  -1.114e+03\n",
      "Iter: 564; D loss: -62.11; G_loss:  -1.081e+03\n",
      "Iter: 566; D loss: -61.07; G_loss:  -1.132e+03\n",
      "Iter: 568; D loss: -56.56; G_loss:  -1.114e+03\n",
      "Iter: 570; D loss: -59.6; G_loss:  -1.151e+03\n",
      "Iter: 572; D loss: -61.46; G_loss:  -1.194e+03\n",
      "Iter: 574; D loss: -63.67; G_loss:  -1.121e+03\n",
      "Iter: 576; D loss: -59.45; G_loss:  -1.123e+03\n",
      "Iter: 578; D loss: -58.73; G_loss:  -1.112e+03\n",
      "Iter: 580; D loss: -58.38; G_loss:  -1.112e+03\n",
      "Iter: 582; D loss: -56.87; G_loss:  -1.169e+03\n",
      "Iter: 584; D loss: -56.25; G_loss:  -1.127e+03\n",
      "Iter: 586; D loss: -70.23; G_loss:  -1.11e+03\n",
      "Iter: 588; D loss: -52.08; G_loss:  -1.122e+03\n",
      "Iter: 590; D loss: -51.18; G_loss:  -1.14e+03\n",
      "Iter: 592; D loss: -53.11; G_loss:  -1.1e+03\n",
      "Iter: 594; D loss: -61.35; G_loss:  -1.101e+03\n",
      "Iter: 596; D loss: -46.03; G_loss:  -1.138e+03\n",
      "Iter: 598; D loss: -53.85; G_loss:  -1.107e+03\n",
      "Iter: 600; D loss: -54.11; G_loss:  -1.127e+03\n",
      "Iter: 602; D loss: -56.58; G_loss:  -1.125e+03\n",
      "Iter: 604; D loss: -54.0; G_loss:  -1.101e+03\n",
      "Iter: 606; D loss: -54.24; G_loss:  -1.146e+03\n",
      "Iter: 608; D loss: -53.3; G_loss:  -1.106e+03\n",
      "Iter: 610; D loss: -50.84; G_loss:  -1.115e+03\n",
      "Iter: 612; D loss: -49.21; G_loss:  -1.098e+03\n",
      "Iter: 614; D loss: -55.41; G_loss:  -1.125e+03\n",
      "Iter: 616; D loss: -48.51; G_loss:  -1.135e+03\n",
      "Iter: 618; D loss: -49.39; G_loss:  -1.101e+03\n",
      "Iter: 620; D loss: -52.85; G_loss:  -1.118e+03\n",
      "Iter: 622; D loss: -46.45; G_loss:  -1.118e+03\n",
      "Iter: 624; D loss: -61.62; G_loss:  -1.108e+03\n",
      "Iter: 626; D loss: -55.38; G_loss:  -1.145e+03\n",
      "Iter: 628; D loss: -47.88; G_loss:  -1.192e+03\n",
      "Iter: 630; D loss: -61.84; G_loss:  -1.106e+03\n",
      "Iter: 632; D loss: -45.51; G_loss:  -1.124e+03\n",
      "Iter: 634; D loss: -48.93; G_loss:  -1.117e+03\n",
      "Iter: 636; D loss: -50.91; G_loss:  -1.147e+03\n",
      "Iter: 638; D loss: -47.61; G_loss:  -1.125e+03\n",
      "Iter: 640; D loss: -46.56; G_loss:  -1.104e+03\n",
      "Iter: 642; D loss: -41.92; G_loss:  -1.152e+03\n",
      "Iter: 644; D loss: -39.59; G_loss:  -1.13e+03\n",
      "Iter: 646; D loss: -47.41; G_loss:  -1.124e+03\n",
      "Iter: 648; D loss: -46.95; G_loss:  -1.125e+03\n",
      "Iter: 650; D loss: -46.04; G_loss:  -1.122e+03\n",
      "Iter: 652; D loss: -49.55; G_loss:  -1.149e+03\n",
      "Iter: 654; D loss: -46.54; G_loss:  -1.117e+03\n",
      "Iter: 656; D loss: -46.02; G_loss:  -1.159e+03\n",
      "Iter: 658; D loss: -36.97; G_loss:  -1.121e+03\n",
      "Iter: 660; D loss: -39.19; G_loss:  -1.119e+03\n",
      "Iter: 662; D loss: -42.47; G_loss:  -1.089e+03\n",
      "Iter: 664; D loss: -42.87; G_loss:  -1.129e+03\n",
      "Iter: 666; D loss: -47.03; G_loss:  -1.149e+03\n",
      "Iter: 668; D loss: -39.48; G_loss:  -1.121e+03\n",
      "Iter: 670; D loss: -44.91; G_loss:  -1.1e+03\n",
      "Iter: 672; D loss: -41.77; G_loss:  -1.136e+03\n",
      "Iter: 674; D loss: -42.93; G_loss:  -1.098e+03\n",
      "Iter: 676; D loss: -38.68; G_loss:  -1.159e+03\n",
      "Iter: 678; D loss: -40.9; G_loss:  -1.141e+03\n",
      "Iter: 680; D loss: -40.75; G_loss:  -1.101e+03\n",
      "Iter: 682; D loss: -46.52; G_loss:  -1.068e+03\n",
      "Iter: 684; D loss: -37.34; G_loss:  -1.126e+03\n",
      "Iter: 686; D loss: -39.0; G_loss:  -1.095e+03\n",
      "Iter: 688; D loss: -44.47; G_loss:  -1.133e+03\n",
      "Iter: 690; D loss: -44.95; G_loss:  -1.165e+03\n",
      "Iter: 692; D loss: -42.94; G_loss:  -1.135e+03\n",
      "Iter: 694; D loss: -34.45; G_loss:  -1.155e+03\n",
      "Iter: 696; D loss: -37.01; G_loss:  -1.165e+03\n",
      "Iter: 698; D loss: -47.71; G_loss:  -1.109e+03\n",
      "Iter: 700; D loss: -43.65; G_loss:  -1.097e+03\n",
      "Iter: 702; D loss: -41.55; G_loss:  -1.136e+03\n",
      "Iter: 704; D loss: -31.71; G_loss:  -1.081e+03\n",
      "Iter: 706; D loss: -40.47; G_loss:  -1.092e+03\n",
      "Iter: 708; D loss: -35.91; G_loss:  -1.108e+03\n",
      "Iter: 710; D loss: -38.31; G_loss:  -1.166e+03\n",
      "Iter: 712; D loss: -38.17; G_loss:  -1.113e+03\n",
      "Iter: 714; D loss: -38.76; G_loss:  -1.116e+03\n",
      "Iter: 716; D loss: -35.73; G_loss:  -1.11e+03\n",
      "Iter: 718; D loss: -32.77; G_loss:  -1.17e+03\n",
      "Iter: 720; D loss: -36.36; G_loss:  -1.109e+03\n",
      "Iter: 722; D loss: -34.65; G_loss:  -1.119e+03\n",
      "Iter: 724; D loss: -40.28; G_loss:  -1.131e+03\n",
      "Iter: 726; D loss: -31.11; G_loss:  -1.132e+03\n",
      "Iter: 728; D loss: -39.3; G_loss:  -1.115e+03\n",
      "Iter: 730; D loss: -38.61; G_loss:  -1.195e+03\n",
      "Iter: 732; D loss: -38.15; G_loss:  -1.106e+03\n",
      "Iter: 734; D loss: -35.99; G_loss:  -1.165e+03\n",
      "Iter: 736; D loss: -39.23; G_loss:  -1.147e+03\n",
      "Iter: 738; D loss: -30.65; G_loss:  -1.155e+03\n",
      "Iter: 740; D loss: -38.79; G_loss:  -1.131e+03\n",
      "Iter: 742; D loss: -37.16; G_loss:  -1.134e+03\n",
      "Iter: 744; D loss: -38.66; G_loss:  -1.129e+03\n",
      "Iter: 746; D loss: -37.59; G_loss:  -1.142e+03\n",
      "Iter: 748; D loss: -34.2; G_loss:  -1.13e+03\n",
      "Iter: 750; D loss: -33.62; G_loss:  -1.123e+03\n",
      "Iter: 752; D loss: -40.13; G_loss:  -1.143e+03\n",
      "Iter: 754; D loss: -34.72; G_loss:  -1.17e+03\n",
      "Iter: 756; D loss: -34.59; G_loss:  -1.119e+03\n",
      "Iter: 758; D loss: -36.78; G_loss:  -1.169e+03\n",
      "Iter: 760; D loss: -33.38; G_loss:  -1.156e+03\n",
      "Iter: 762; D loss: -40.88; G_loss:  -1.159e+03\n",
      "Iter: 764; D loss: -33.76; G_loss:  -1.136e+03\n",
      "Iter: 766; D loss: -39.64; G_loss:  -1.127e+03\n",
      "Iter: 768; D loss: -34.77; G_loss:  -1.157e+03\n",
      "Iter: 770; D loss: -25.24; G_loss:  -1.159e+03\n",
      "Iter: 772; D loss: -30.29; G_loss:  -1.145e+03\n",
      "Iter: 774; D loss: -31.31; G_loss:  -1.11e+03\n",
      "Iter: 776; D loss: -31.16; G_loss:  -1.123e+03\n",
      "Iter: 778; D loss: -32.65; G_loss:  -1.108e+03\n",
      "Iter: 780; D loss: -41.08; G_loss:  -1.17e+03\n",
      "Iter: 782; D loss: -32.73; G_loss:  -1.162e+03\n",
      "Iter: 784; D loss: -30.39; G_loss:  -1.209e+03\n",
      "Iter: 786; D loss: -32.16; G_loss:  -1.125e+03\n",
      "Iter: 788; D loss: -29.81; G_loss:  -1.074e+03\n",
      "Iter: 790; D loss: -36.31; G_loss:  -1.19e+03\n",
      "Iter: 792; D loss: -37.47; G_loss:  -1.12e+03\n",
      "Iter: 794; D loss: -29.74; G_loss:  -1.105e+03\n",
      "Iter: 796; D loss: -32.57; G_loss:  -1.179e+03\n",
      "Iter: 798; D loss: -31.74; G_loss:  -1.118e+03\n",
      "Iter: 800; D loss: -29.56; G_loss:  -1.132e+03\n",
      "Iter: 802; D loss: -34.14; G_loss:  -1.149e+03\n",
      "Iter: 804; D loss: -33.3; G_loss:  -1.149e+03\n",
      "Iter: 806; D loss: -25.4; G_loss:  -1.153e+03\n",
      "Iter: 808; D loss: -33.69; G_loss:  -1.138e+03\n",
      "Iter: 810; D loss: -30.52; G_loss:  -1.135e+03\n",
      "Iter: 812; D loss: -36.08; G_loss:  -1.195e+03\n",
      "Iter: 814; D loss: -30.53; G_loss:  -1.19e+03\n",
      "Iter: 816; D loss: -27.87; G_loss:  -1.161e+03\n",
      "Iter: 818; D loss: -36.27; G_loss:  -1.129e+03\n",
      "Iter: 820; D loss: -29.11; G_loss:  -1.174e+03\n",
      "Iter: 822; D loss: -32.62; G_loss:  -1.172e+03\n",
      "Iter: 824; D loss: -30.91; G_loss:  -1.178e+03\n",
      "Iter: 826; D loss: -29.19; G_loss:  -1.121e+03\n",
      "Iter: 828; D loss: -32.76; G_loss:  -1.174e+03\n",
      "Iter: 830; D loss: -33.53; G_loss:  -1.209e+03\n",
      "Iter: 832; D loss: -32.66; G_loss:  -1.163e+03\n",
      "Iter: 834; D loss: -27.91; G_loss:  -1.141e+03\n",
      "Iter: 836; D loss: -26.05; G_loss:  -1.161e+03\n",
      "Iter: 838; D loss: -32.32; G_loss:  -1.154e+03\n",
      "Iter: 840; D loss: -29.17; G_loss:  -1.165e+03\n",
      "Iter: 842; D loss: -29.64; G_loss:  -1.14e+03\n",
      "Iter: 844; D loss: -29.44; G_loss:  -1.175e+03\n",
      "Iter: 846; D loss: -32.58; G_loss:  -1.212e+03\n",
      "Iter: 848; D loss: -32.02; G_loss:  -1.166e+03\n",
      "Iter: 850; D loss: -28.57; G_loss:  -1.12e+03\n",
      "Iter: 852; D loss: -28.76; G_loss:  -1.185e+03\n",
      "Iter: 854; D loss: -35.38; G_loss:  -1.134e+03\n",
      "Iter: 856; D loss: -39.05; G_loss:  -1.187e+03\n",
      "Iter: 858; D loss: -32.65; G_loss:  -1.143e+03\n",
      "Iter: 860; D loss: -30.49; G_loss:  -1.208e+03\n",
      "Iter: 862; D loss: -26.76; G_loss:  -1.126e+03\n",
      "Iter: 864; D loss: -32.47; G_loss:  -1.204e+03\n",
      "Iter: 866; D loss: -31.24; G_loss:  -1.203e+03\n",
      "Iter: 868; D loss: -41.09; G_loss:  -1.145e+03\n",
      "Iter: 870; D loss: -33.32; G_loss:  -1.171e+03\n",
      "Iter: 872; D loss: -27.25; G_loss:  -1.194e+03\n",
      "Iter: 874; D loss: -30.42; G_loss:  -1.177e+03\n",
      "Iter: 876; D loss: -30.25; G_loss:  -1.182e+03\n",
      "Iter: 878; D loss: -33.96; G_loss:  -1.198e+03\n",
      "Iter: 880; D loss: -28.79; G_loss:  -1.192e+03\n",
      "Iter: 882; D loss: -30.17; G_loss:  -1.181e+03\n",
      "Iter: 884; D loss: -36.65; G_loss:  -1.232e+03\n",
      "Iter: 886; D loss: -27.8; G_loss:  -1.237e+03\n",
      "Iter: 888; D loss: -33.54; G_loss:  -1.169e+03\n",
      "Iter: 890; D loss: -30.1; G_loss:  -1.221e+03\n",
      "Iter: 892; D loss: -35.68; G_loss:  -1.227e+03\n",
      "Iter: 894; D loss: -31.4; G_loss:  -1.204e+03\n",
      "Iter: 896; D loss: -32.82; G_loss:  -1.195e+03\n",
      "Iter: 898; D loss: -28.22; G_loss:  -1.183e+03\n",
      "Iter: 900; D loss: -26.53; G_loss:  -1.215e+03\n",
      "Iter: 902; D loss: -33.11; G_loss:  -1.218e+03\n",
      "Iter: 904; D loss: -31.1; G_loss:  -1.251e+03\n",
      "Iter: 906; D loss: -30.28; G_loss:  -1.269e+03\n",
      "Iter: 908; D loss: -29.1; G_loss:  -1.232e+03\n",
      "Iter: 910; D loss: -32.47; G_loss:  -1.22e+03\n",
      "Iter: 912; D loss: -30.07; G_loss:  -1.276e+03\n",
      "Iter: 914; D loss: -31.69; G_loss:  -1.267e+03\n",
      "Iter: 916; D loss: -29.72; G_loss:  -1.223e+03\n",
      "Iter: 918; D loss: -33.56; G_loss:  -1.227e+03\n",
      "Iter: 920; D loss: -34.71; G_loss:  -1.221e+03\n",
      "Iter: 922; D loss: -32.57; G_loss:  -1.268e+03\n",
      "Iter: 924; D loss: -27.51; G_loss:  -1.294e+03\n",
      "Iter: 926; D loss: -39.85; G_loss:  -1.288e+03\n",
      "Iter: 928; D loss: -33.38; G_loss:  -1.32e+03\n",
      "Iter: 930; D loss: -38.89; G_loss:  -1.276e+03\n",
      "Iter: 932; D loss: -35.93; G_loss:  -1.344e+03\n",
      "Iter: 934; D loss: -35.0; G_loss:  -1.322e+03\n",
      "Iter: 936; D loss: -31.09; G_loss:  -1.291e+03\n",
      "Iter: 938; D loss: -23.14; G_loss:  -1.252e+03\n",
      "Iter: 940; D loss: -24.88; G_loss:  -1.239e+03\n",
      "Iter: 942; D loss: -31.88; G_loss:  -1.225e+03\n",
      "Iter: 944; D loss: -43.67; G_loss:  -1.247e+03\n",
      "Iter: 946; D loss: -44.48; G_loss:  -1.211e+03\n",
      "Iter: 948; D loss: -37.19; G_loss:  -1.218e+03\n",
      "Iter: 950; D loss: -26.17; G_loss:  -1.227e+03\n",
      "Iter: 952; D loss: -29.98; G_loss:  -1.283e+03\n",
      "Iter: 954; D loss: -20.31; G_loss:  -1.292e+03\n",
      "Iter: 956; D loss: -28.45; G_loss:  -1.341e+03\n",
      "Iter: 958; D loss: -33.77; G_loss:  -1.341e+03\n",
      "Iter: 960; D loss: -36.09; G_loss:  -1.329e+03\n",
      "Iter: 962; D loss: -40.46; G_loss:  -1.368e+03\n",
      "Iter: 964; D loss: -29.71; G_loss:  -1.386e+03\n",
      "Iter: 966; D loss: -41.84; G_loss:  -1.383e+03\n",
      "Iter: 968; D loss: -26.18; G_loss:  -1.318e+03\n",
      "Iter: 970; D loss: -31.77; G_loss:  -1.364e+03\n",
      "Iter: 972; D loss: -30.16; G_loss:  -1.321e+03\n",
      "Iter: 974; D loss: -27.42; G_loss:  -1.26e+03\n",
      "Iter: 976; D loss: -25.14; G_loss:  -1.343e+03\n",
      "Iter: 978; D loss: -26.76; G_loss:  -1.278e+03\n",
      "Iter: 980; D loss: -33.11; G_loss:  -1.301e+03\n",
      "Iter: 982; D loss: -37.51; G_loss:  -1.203e+03\n",
      "Iter: 984; D loss: -41.17; G_loss:  -1.22e+03\n",
      "Iter: 986; D loss: -43.17; G_loss:  -1.232e+03\n",
      "Iter: 988; D loss: -39.2; G_loss:  -1.207e+03\n",
      "Iter: 990; D loss: -31.66; G_loss:  -1.251e+03\n",
      "Iter: 992; D loss: -26.76; G_loss:  -1.296e+03\n",
      "Iter: 994; D loss: -26.95; G_loss:  -1.259e+03\n",
      "Iter: 996; D loss: -25.73; G_loss:  -1.34e+03\n",
      "Iter: 998; D loss: -24.83; G_loss:  -1.358e+03\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "\n",
    "gan = conditional_wgangp.WGANGP()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ROOT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f245123dffb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mROOT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ROOT'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse \n",
    "import os,sys,ctypes\n",
    "import ROOT \n",
    "import shutil\n",
    "import ctypes\n",
    "import glob\n",
    "\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "save_plots = True\n",
    "lparams = {'xoffset' : 0.1, 'yoffset' : 0.27, 'width'   : 0.8, 'height'  : 0.35}\n",
    "canvases = []   \n",
    "\n",
    "histos_vox = []\n",
    "input_files_vox = []\n",
    "\n",
    "minFactor = 3\n",
    "maxFactor = 3  \n",
    "particleName=\"#gamma\"\n",
    "particle=\"photons\"\n",
    "\n",
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)\n",
    "\n",
    "maxVoxel = 0\n",
    "midEnergy = 0\n",
    "step = 50\n",
    "\n",
    "wgan = conditional_wgangp.WGANGP()\n",
    "dl = DataLoader.DataLoader()\n",
    "ekins = dl.ekins\n",
    "\n",
    "firstPosition = 0\n",
    "\n",
    "print(\"Opening vox files\")\n",
    "for index, energy in enumerate(dataParameters.momentums):    \n",
    "  #print(\" Energy \", energy)\n",
    "  input_file_vox = ('rootFiles/pid22_E%s_eta_20_25.root' % (energy))\n",
    "  print(\" Opening file: \" + input_file_vox)\n",
    "  infile_vox = ROOT.TFile.Open(input_file_vox, 'read') \n",
    "  input_files_vox.append(infile_vox)\n",
    "  tree = infile_vox.Get('rootTree') \n",
    "  \n",
    "  h = ROOT.TH1F(\"h\",\"\",100,0,energy*2) \n",
    "  tree.Draw(\"etot>>h\",\"\",\"off\")\n",
    "  xmax=h.GetBinCenter(h.GetMaximumBin());\n",
    "  minX = max(0, xmax-minFactor*h.GetRMS()) #max(0, xmax-minFactors[item]*h.GetRMS())\n",
    "  maxX = xmax+maxFactor*h.GetRMS()\n",
    "  print(\"min \"+ str(minX) + \" max \" + str(maxX))\n",
    "      \n",
    "  h_vox = ROOT.TH1F(\"h_vox\",\"\",30,minX/1000,maxX/1000) \n",
    "  tree.Draw(\"etot/1000>>h_vox\",\"\",\"off\")\n",
    "  h_vox.Scale(1/h_vox.GetEntries())\n",
    "  histos_vox.append(h_vox)\n",
    "\n",
    "print (\"Running from %i to %i in step of %i\" %(0, 1000, step))\n",
    "for iteration in range(0, 1000, step):\n",
    "  try:\n",
    "    histos_gan =[]\n",
    "    canvas = ROOT.TCanvas('canvas_h', 'Total Energy comparison plots', 900, 900)\n",
    "    canvas.Divide(4,4)\n",
    "    legendPadIndex = 16\n",
    "\n",
    "    canvases.append(canvas)\n",
    "\n",
    "    chi2_tot = 0.\n",
    "    ndf_tot = 0\n",
    "    input_files_gan = []\n",
    "\n",
    "    for index, energy in enumerate(ekins):     \n",
    "      ekin_sample = ekins[index]\n",
    "      energyArray = np.array([energy] * nevents)\n",
    "      etaArray = np.zeros(nevents) \n",
    "      labels = np.vstack((energyArray, etaArray)).T   \n",
    "\n",
    "      data = wgan.load(iteration, labels, n_events, 'checkpoints')\n",
    "      data = data * ekin_sample       #needed for conditional\n",
    "        \n",
    "      h_vox = histos_vox[index]\n",
    "      h_gan = ROOT.TH1F(\"h_gan\",\"\",30,h_vox.GetXaxis().GetXmin(),h_vox.GetXaxis().GetXmax())\n",
    "\n",
    "      E_tot = data.numpy().sum(axis=1)\n",
    "      for e in E_tot:\n",
    "        h_gan.Fill(e/1000)\n",
    "      \n",
    "      h_gan.Scale(1/h_gan.GetEntries())\n",
    "      h_gan.SetLineColor(ROOT.kRed)\n",
    "      h_gan.SetLineStyle(7)\n",
    "      m = [h_vox.GetBinContent(h_vox.GetMaximumBin()),h_gan.GetBinContent(h_gan.GetMaximumBin())]\n",
    "      h_vox.GetYaxis().SetRangeUser(0,max(m) *1.25)\n",
    "      histos_gan.append(h_gan)\n",
    "      h_vox.GetYaxis().SetTitle(\"Entries\")\n",
    "\n",
    "      xAxisTitle = \"Energy [GeV]\"\n",
    "      h_vox.GetXaxis().SetTitle(xAxisTitle)  \n",
    "      h_vox.GetXaxis().SetNdivisions(506)\n",
    "      chi2 = ctypes.c_double(0.)\n",
    "      ndf = ctypes.c_int(0)\n",
    "      igood = ctypes.c_int(0)\n",
    "      histos_vox[index].Chi2TestX(h_gan, chi2, ndf, igood, \"WW\")\n",
    "      ndf = ndf.value\n",
    "      chi2=chi2.value\n",
    "      chi2_tot += chi2\n",
    "      ndf_tot += ndf\n",
    "\n",
    "      if (ndf != 0):\n",
    "        print(\"Iteration %s Energy %s : chi2/ndf = %.1f / %i = %.1f\\n\" % (iteration, energy, chi2, ndf, chi2/ndf))\n",
    "\n",
    "      # Plotting\n",
    "\n",
    "      canvas.cd(index+1)\n",
    "      histos_vox[index].Draw(\"HIST\")\n",
    "      histos_gan[index].Draw(\"HIST same\")\n",
    "\n",
    "      # Legend box                                                                                                                                                                            \n",
    "      if (energy > 1024):\n",
    "          energy_legend =  str(round(energy/1000,1)) + \" GeV\"\n",
    "      else:\n",
    "          energy_legend =  str(energy) + \" MeV\"\n",
    "      t = ROOT.TLatex()\n",
    "      t.SetNDC()\n",
    "      t.SetTextFont(42)\n",
    "      t.SetTextSize(0.1)\n",
    "      t.DrawLatex(0.2, 0.83, energy_legend)\n",
    "   \n",
    "    # Total Energy chi2\n",
    "    chi2_o_ndf = chi2_tot / ndf_tot\n",
    "    print(\"Iteration %s Total Energy : chi2/ndf = %.1f / %i = %.3f\\n\" % (iteration, chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "    chi2File = \"%s/chi2.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "    if chi2_o_ndf > 0:\n",
    "      f = open(chi2File, 'a')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close()\n",
    "    else:\n",
    "      print(\"Something went wrong, chi2 will not be written. Chi2/ndf is %f \" % (chi2_o_ndf))\n",
    "      print(E_tot)\n",
    "      continue\n",
    "\n",
    "    # Legend box particle\n",
    "    leg = MakeLegend( lparams )\n",
    "    leg.SetTextFont( 42 )\n",
    "    leg.SetTextSize(0.1)\n",
    "    canvas.cd(legendPadIndex)\n",
    "    leg.AddEntry(h_vox,\"Geant4\",\"l\") #Geant4\n",
    "    leg.Draw()\n",
    "    leg.AddEntry(h_gan,\"GAN\",\"l\")  #WGAN-GP\n",
    "    leg.Draw('same')\n",
    "    legend = (particleName + \", \" + str('{:.2f}'.format(int(20)/100,2)) + \"<|#eta|<\" + str('{:.2f}'.format((int(20)+5)/100,2)))\n",
    "    ROOT.ATLAS_LABEL_BIG( 0.1, 0.9, ROOT.kBlack, legend )\n",
    "\n",
    "    # Legend box Epoc&chi2 \n",
    "\n",
    "    t = ROOT.TLatex()\n",
    "    t.SetNDC()\n",
    "    t.SetTextFont(42)\n",
    "    t.SetTextSize(0.1)\n",
    "    t.DrawLatex(0.1, 0.18, \"Iter: %s\" % (iteration))\n",
    "    t.DrawLatex(0.1, 0.07, \"#scale[0.8]{#chi^{2}/NDF = %.0f/%i = %.1f}\" % (chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "\n",
    "\n",
    "    #Copy best epoch files, including plots\n",
    "    epochs, chi2_o_ndf_list = np.loadtxt(chi2File, delimiter=' ', unpack=True)\n",
    "    \n",
    "    checkpointName =  \"Plot_comparison_tot_energy\"\n",
    "      \n",
    "    if round(chi2_o_ndf,3) <= np.amin(chi2_o_ndf_list) and chi2_o_ndf > 0:\n",
    "      print (\"Better chi2, creating plots\")\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "     \n",
    "      print(\"Epoch with lowest chi2/ndf is %s with a value of %.3f\" % (epoch, chi2_o_ndf))\n",
    "      #Now save best epoch number to file\n",
    "      chi2File = \"%s/chi2/epoch_best_chi2_%s_%s_%s.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "      f = open(chi2File, 'w')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close() \n",
    "\n",
    "    if (save_plots) :\n",
    "      checkpointName =  \"Plot_comparison_tot_energy_%i\" % (iteration)\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "\n",
    "  except:\n",
    "    print(\"Something went wrong in iteration %s, moving to next one\" % (iteration))\n",
    "    print(\"exception message \", sys.exc_info()[0])     \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba67b957603c6589b793cbc779fadd4d74491f4ed475d4948a7778f403f5ead2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
