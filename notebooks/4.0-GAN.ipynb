{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations\n",
    "The data is available in the CERN [OpenData](http://opendata.cern.ch/record/15012)\n",
    "The code is based on the FastCaloGAN code which is available on [Zenodo](https://zenodo.org/record/5589623) with the latest development available for ATLAS member on the [FCS git repository](https://gitlab.cern.ch/atlas-simulation-fastcalosim/fastcalogan)\n",
    "\n",
    "The data and code are also the case for the [#calochallenge](https://github.com/CaloChallenge/homepage)\n",
    "\n",
    "Download files into the csv_inputs folder to run the example from the first link or \n",
    "[128 MeV](http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E256_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=bSSGDUvNFuCxHHz3YlCqga0Jq0g%3D&Expires=1829145580)\n",
    "[256 MeV](http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E512_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=Dr3A32ycujBSm4bQ14l%2BHvEf1Ig%3D&Expires=1829145645)\n",
    "\n",
    "The code is saved to run on only two samples; if you want to run on all samples, you will need to change maxExp in DataLoader to 23 ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import DataLoader \n",
    "import importlib\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from functools import partial\n",
    "tf.keras.backend.set_floatx('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "noise = layers.Input(shape=(50), name=\"Noise\")\n",
    "condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "con = layers.concatenate([noise,condition])\n",
    "G = layers.Dense(50, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(con)  \n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(100, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(200, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "\n",
    "generator = Model(inputs=[noise, condition], outputs=G)\n",
    "generator.build(370)\n",
    "generator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator network\n",
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "\n",
    "image = layers.Input(shape=(368), name=\"Image\")\n",
    "d_condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "d_con = layers.concatenate([image,d_condition])\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(d_con)  \n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(1, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "\n",
    "discriminator = Model(inputs=[image, d_condition], outputs=D)\n",
    "discriminator.build(370)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, loss anf gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def gradient_penalty(f, x_real, x_fake, cond_label, batchsize, D):\n",
    "  alpha = tf.random.uniform([batchsize, 1], minval=0., maxval=1.)\n",
    "\n",
    "  inter = alpha * x_real + (1-alpha) * x_fake\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(inter)\n",
    "    pred = D(inputs=[inter, cond_label])\n",
    "  grad = t.gradient(pred, [inter])[0]\n",
    "  \n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "  gp = 0.00001 * tf.reduce_mean((slopes - 1.)**2) #Lambda\n",
    "  return gp\n",
    "\n",
    "@tf.function\n",
    "def D_loss(x_real, cond_label, batchsize, G, D): \n",
    "  z = tf.random.normal([batchsize, 50], mean=0.5, stddev=0.5, dtype=tf.dtypes.float32) #batch and latent dim\n",
    "  x_fake = G(inputs=[z, cond_label])\n",
    "  D_fake = D(inputs=[x_fake, cond_label])\n",
    "  D_real = D(inputs=[x_real, cond_label])\n",
    "  D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + gradient_penalty(f = partial(D, training=True), x_real = x_real, x_fake = x_fake, cond_label=cond_label, batchsize=batchsize, D=D)\n",
    "  return D_loss, D_fake\n",
    "\n",
    "@tf.function\n",
    "def G_loss(D_fake):\n",
    "  G_loss = -tf.reduce_mean(D_fake)\n",
    "  return G_loss\n",
    "\n",
    "def getTrainData_ultimate( n_iteration, batchsize, dgratio, X ,Labels):\n",
    "  true_batchsize = tf.cast(tf.math.multiply(batchsize, dgratio), tf.int64)\n",
    "  n_samples = tf.cast(tf.gather(tf.shape(X), 0), tf.int64)\n",
    "  n_batch = tf.cast(tf.math.floordiv(n_samples, true_batchsize), tf.int64)\n",
    "  n_shuffles = tf.cast(tf.math.ceil(tf.divide(n_iteration, n_batch)), tf.int64)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((X, Labels))\n",
    "  ds = ds.shuffle(buffer_size = n_samples).repeat(n_shuffles).batch(true_batchsize, drop_remainder=True).prefetch(2)\n",
    "  return iter(ds)\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X_trains, cond_labels, batchsize, dgratio, G, D, generator_optimizer, discriminator_optimizer): \n",
    "  for i in tf.range(dgratio):\n",
    "    print(\"d train: \" + str(i))\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, i), tf.gather(cond_labels, i), batchsize, G, D)\n",
    "      gradients_of_discriminator = disc_tape.gradient(D_loss_curr, D.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, D.trainable_variables))    \n",
    "      \n",
    "  print(\"g train\")\n",
    "  last_index = tf.subtract(dgratio, 1)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    # Need to recompute D_fake, otherwise gen_tape doesn't know the history\n",
    "    (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, last_index), tf.gather(cond_labels, last_index), batchsize, G, D)\n",
    "    G_loss_curr = G_loss(D_fake)\n",
    "    gradients_of_generator = gen_tape.gradient(G_loss_curr, G.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, G.trainable_variables))\n",
    "    return D_loss_curr, G_loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgratio = 5\n",
    "batchsize = 128\n",
    "G_lr = D_lr = 0.0001\n",
    "G_beta1 = D_beta1 = 0.55\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate=G_lr, beta_1=G_beta1)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=D_lr, beta_1=D_beta1)\n",
    "\n",
    "# Prepare for check pointing\n",
    "saver = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                            discriminator_optimizer=discriminator_optimizer,\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "  os.makedirs(checkpoint_dir)\n",
    "\n",
    "print ('training started')\n",
    "dl = DataLoader.DataLoader()\n",
    "\n",
    "start_iteration = 0 \n",
    "max_iterations = 1000\n",
    "\n",
    "for iteration in range(start_iteration,max_iterations): \n",
    "  change_data = (iteration == start_iteration)\n",
    "  \n",
    "  if (change_data == True):\n",
    "    X, Labels = dl.getAllTrainData(8, 9)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    Labels = tf.convert_to_tensor(Labels, dtype=tf.float32)\n",
    " \n",
    "    remained_iteration = tf.constant(max_iterations - iteration, dtype=tf.int64)\n",
    "    ds_iter = getTrainData_ultimate(remained_iteration, batchsize, dgratio, X ,Labels)\n",
    "    print (\"Using \"+ str(X.shape[0])+ \" events\")\n",
    "\n",
    "  X, Labels = ds_iter.get_next()\n",
    "\n",
    "  X_feature_size = tf.gather(tf.shape(X), 1)\n",
    "  Labels_feature_size = tf.gather(tf.shape(Labels), 1)\n",
    "  X_batch_shape = tf.stack((dgratio, batchsize, X_feature_size), axis=0)\n",
    "  Labels_batch_shape = tf.stack((dgratio, batchsize, Labels_feature_size), axis=0)\n",
    "\n",
    "  X_trains    = tf.reshape(X, X_batch_shape)\n",
    "  cond_labels = tf.reshape(Labels, Labels_batch_shape)  \n",
    "\n",
    "  #print(X_trains) \n",
    "  #print(cond_labels) \n",
    "  #print(batchsize) \n",
    "  #print(dgratio) \n",
    "  #generator.summary() \n",
    "  #discriminator.summary() \n",
    "\n",
    "  D_loss_curr, G_loss_curr = train_loop(X_trains, cond_labels, batchsize, dgratio, generator, discriminator,  generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "  if iteration == 0: \n",
    "    print(\"Model and loss values will be saved every 2 iterations.\" )\n",
    "  \n",
    "  if iteration % 2 == 0 and iteration > 0:\n",
    "\n",
    "    try:\n",
    "      saver.save(file_prefix = checkpoint_dir+ '/model')\n",
    "    except:\n",
    "      print(\"Something went wrong in saving iteration %s, moving to next one\" % (iteration))\n",
    "      print(\"exception message \", sys.exc_info()[0])     \n",
    "    \n",
    "    print('Iter: {}; D loss: {:.4}; G_loss:  {:.4}'.format(iteration, D_loss_curr, G_loss_curr))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "\n",
    "gan = conditional_wgangp.WGANGP()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse \n",
    "import os,sys,ctypes\n",
    "import ROOT \n",
    "import shutil\n",
    "import ctypes\n",
    "import glob\n",
    "\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "save_plots = True\n",
    "lparams = {'xoffset' : 0.1, 'yoffset' : 0.27, 'width'   : 0.8, 'height'  : 0.35}\n",
    "canvases = []   \n",
    "\n",
    "histos_vox = []\n",
    "input_files_vox = []\n",
    "\n",
    "minFactor = 3\n",
    "maxFactor = 3  \n",
    "particleName=\"#gamma\"\n",
    "particle=\"photons\"\n",
    "\n",
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)\n",
    "\n",
    "maxVoxel = 0\n",
    "midEnergy = 0\n",
    "step = 50\n",
    "\n",
    "wgan = conditional_wgangp.WGANGP()\n",
    "dl = DataLoader.DataLoader()\n",
    "ekins = dl.ekins\n",
    "\n",
    "firstPosition = 0\n",
    "\n",
    "print(\"Opening vox files\")\n",
    "for index, energy in enumerate(ekins):    \n",
    "  #print(\" Energy \", energy)\n",
    "  input_file_vox = ('rootFiles/pid22_E%s_eta_20_25.root' % (energy))\n",
    "  print(\" Opening file: \" + input_file_vox)\n",
    "  infile_vox = ROOT.TFile.Open(input_file_vox, 'read') \n",
    "  input_files_vox.append(infile_vox)\n",
    "  tree = infile_vox.Get('rootTree') \n",
    "  \n",
    "  h = ROOT.TH1F(\"h\",\"\",100,0,energy*2) \n",
    "  tree.Draw(\"etot>>h\",\"\",\"off\")\n",
    "  xmax=h.GetBinCenter(h.GetMaximumBin());\n",
    "  minX = max(0, xmax-minFactor*h.GetRMS()) #max(0, xmax-minFactors[item]*h.GetRMS())\n",
    "  maxX = xmax+maxFactor*h.GetRMS()\n",
    "  print(\"min \"+ str(minX) + \" max \" + str(maxX))\n",
    "      \n",
    "  h_vox = ROOT.TH1F(\"h_vox\",\"\",30,minX/1000,maxX/1000) \n",
    "  tree.Draw(\"etot/1000>>h_vox\",\"\",\"off\")\n",
    "  h_vox.Scale(1/h_vox.GetEntries())\n",
    "  histos_vox.append(h_vox)\n",
    "\n",
    "print (\"Running from %i to %i in step of %i\" %(0, 1000, step))\n",
    "for iteration in range(0, 1000, step):\n",
    "  try:\n",
    "    histos_gan =[]\n",
    "    canvas = ROOT.TCanvas('canvas_h', 'Total Energy comparison plots', 900, 900)\n",
    "    canvas.Divide(4,4)\n",
    "    legendPadIndex = 16\n",
    "\n",
    "    canvases.append(canvas)\n",
    "\n",
    "    chi2_tot = 0.\n",
    "    ndf_tot = 0\n",
    "    input_files_gan = []\n",
    "\n",
    "    for index, energy in enumerate(ekins):     \n",
    "      ekin_sample = ekins[index]\n",
    "      energyArray = np.array([energy] * nevents)\n",
    "      etaArray = np.zeros(nevents) \n",
    "      labels = np.vstack((energyArray, etaArray)).T   \n",
    "\n",
    "      data = wgan.load(iteration, labels, n_events, 'checkpoints')\n",
    "      data = data * ekin_sample       #needed for conditional\n",
    "        \n",
    "      h_vox = histos_vox[index]\n",
    "      h_gan = ROOT.TH1F(\"h_gan\",\"\",30,h_vox.GetXaxis().GetXmin(),h_vox.GetXaxis().GetXmax())\n",
    "\n",
    "      E_tot = data.numpy().sum(axis=1)\n",
    "      for e in E_tot:\n",
    "        h_gan.Fill(e/1000)\n",
    "      \n",
    "      h_gan.Scale(1/h_gan.GetEntries())\n",
    "      h_gan.SetLineColor(ROOT.kRed)\n",
    "      h_gan.SetLineStyle(7)\n",
    "      m = [h_vox.GetBinContent(h_vox.GetMaximumBin()),h_gan.GetBinContent(h_gan.GetMaximumBin())]\n",
    "      h_vox.GetYaxis().SetRangeUser(0,max(m) *1.25)\n",
    "      histos_gan.append(h_gan)\n",
    "      h_vox.GetYaxis().SetTitle(\"Entries\")\n",
    "\n",
    "      xAxisTitle = \"Energy [GeV]\"\n",
    "      h_vox.GetXaxis().SetTitle(xAxisTitle)  \n",
    "      h_vox.GetXaxis().SetNdivisions(506)\n",
    "      chi2 = ctypes.c_double(0.)\n",
    "      ndf = ctypes.c_int(0)\n",
    "      igood = ctypes.c_int(0)\n",
    "      histos_vox[index].Chi2TestX(h_gan, chi2, ndf, igood, \"WW\")\n",
    "      ndf = ndf.value\n",
    "      chi2=chi2.value\n",
    "      chi2_tot += chi2\n",
    "      ndf_tot += ndf\n",
    "\n",
    "      if (ndf != 0):\n",
    "        print(\"Iteration %s Energy %s : chi2/ndf = %.1f / %i = %.1f\\n\" % (iteration, energy, chi2, ndf, chi2/ndf))\n",
    "\n",
    "      # Plotting\n",
    "\n",
    "      canvas.cd(index+1)\n",
    "      histos_vox[index].Draw(\"HIST\")\n",
    "      histos_gan[index].Draw(\"HIST same\")\n",
    "\n",
    "      # Legend box                                                                                                                                                                            \n",
    "      if (energy > 1024):\n",
    "          energy_legend =  str(round(energy/1000,1)) + \" GeV\"\n",
    "      else:\n",
    "          energy_legend =  str(energy) + \" MeV\"\n",
    "      t = ROOT.TLatex()\n",
    "      t.SetNDC()\n",
    "      t.SetTextFont(42)\n",
    "      t.SetTextSize(0.1)\n",
    "      t.DrawLatex(0.2, 0.83, energy_legend)\n",
    "   \n",
    "    # Total Energy chi2\n",
    "    chi2_o_ndf = chi2_tot / ndf_tot\n",
    "    print(\"Iteration %s Total Energy : chi2/ndf = %.1f / %i = %.3f\\n\" % (iteration, chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "    chi2File = \"%s/chi2.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "    if chi2_o_ndf > 0:\n",
    "      f = open(chi2File, 'a')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close()\n",
    "    else:\n",
    "      print(\"Something went wrong, chi2 will not be written. Chi2/ndf is %f \" % (chi2_o_ndf))\n",
    "      print(E_tot)\n",
    "      continue\n",
    "\n",
    "    # Legend box particle\n",
    "    leg = MakeLegend( lparams )\n",
    "    leg.SetTextFont( 42 )\n",
    "    leg.SetTextSize(0.1)\n",
    "    canvas.cd(legendPadIndex)\n",
    "    leg.AddEntry(h_vox,\"Geant4\",\"l\") #Geant4\n",
    "    leg.Draw()\n",
    "    leg.AddEntry(h_gan,\"GAN\",\"l\")  #WGAN-GP\n",
    "    leg.Draw('same')\n",
    "    legend = (particleName + \", \" + str('{:.2f}'.format(int(20)/100,2)) + \"<|#eta|<\" + str('{:.2f}'.format((int(20)+5)/100,2)))\n",
    "    ROOT.ATLAS_LABEL_BIG( 0.1, 0.9, ROOT.kBlack, legend )\n",
    "\n",
    "    # Legend box Epoc&chi2 \n",
    "\n",
    "    t = ROOT.TLatex()\n",
    "    t.SetNDC()\n",
    "    t.SetTextFont(42)\n",
    "    t.SetTextSize(0.1)\n",
    "    t.DrawLatex(0.1, 0.18, \"Iter: %s\" % (iteration))\n",
    "    t.DrawLatex(0.1, 0.07, \"#scale[0.8]{#chi^{2}/NDF = %.0f/%i = %.1f}\" % (chi2_tot, ndf_tot, chi2_o_ndf))\n",
    "\n",
    "\n",
    "    #Copy best epoch files, including plots\n",
    "    epochs, chi2_o_ndf_list = np.loadtxt(chi2File, delimiter=' ', unpack=True)\n",
    "    \n",
    "    checkpointName =  \"Plot_comparison_tot_energy\"\n",
    "      \n",
    "    if round(chi2_o_ndf,3) <= np.amin(chi2_o_ndf_list) and chi2_o_ndf > 0:\n",
    "      print (\"Better chi2, creating plots\")\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "     \n",
    "      print(\"Epoch with lowest chi2/ndf is %s with a value of %.3f\" % (epoch, chi2_o_ndf))\n",
    "      #Now save best epoch number to file\n",
    "      chi2File = \"%s/chi2/epoch_best_chi2_%s_%s_%s.txt\" % (output_best_checkpoints, pid, eta_min, eta_max)\n",
    "      f = open(chi2File, 'w')\n",
    "      f.write(\"%s %.3f\\n\" % (iteration, chi2_o_ndf))\n",
    "      f.close() \n",
    "\n",
    "    if (save_plots) :\n",
    "      checkpointName =  \"Plot_comparison_tot_energy_%i\" % (iteration)\n",
    "      inputFile_Plot_png=\"%s/%s.png\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_eps=\"%s/%s.eps\" % (output_best_checkpoints, checkpointName)\n",
    "      inputFile_Plot_pdf=\"%s/%s.pdf\" % (output_best_checkpoints, checkpointName)\n",
    "      canvas.SaveAs(inputFile_Plot_png) \n",
    "      canvas.SaveAs(inputFile_Plot_eps) \n",
    "      canvas.SaveAs(inputFile_Plot_pdf) \n",
    "\n",
    "  except:\n",
    "    print(\"Something went wrong in iteration %s, moving to next one\" % (iteration))\n",
    "    print(\"exception message \", sys.exc_info()[0])     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training and evaluation with 1M iterations\n",
    "The GAN learn but with significant fluctuations [plot](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SIMU-2018-04/fig_09.png)\n",
    "Animated gif with all energies [gif](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-SOFT-PUB-2020-006/fig_37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest results\n",
    "![pions](imgs/latest_pions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba67b957603c6589b793cbc779fadd4d74491f4ed475d4948a7778f403f5ead2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
