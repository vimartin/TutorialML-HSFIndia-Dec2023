{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of the energy response distribution\n",
    "Here we will try not just to evaluate the best calibrated energy, but to get the distribution of the response. Basically we will get the distribution of $E_\\text{raw}/E_\\text{true}$ (and not only its best value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import scipy\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Use the same inputs as in the previous simpler energy regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=U%2BMJxVi5El1wxtCz%2B45VqLmUuok%3D&Expires=1828739034')\n",
    "df_test = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=YKG4lzc%2FI0%2BcJZRnQG350DnVVK4%3D&Expires=1828739085')\n",
    "\n",
    "#df_train = pd.read_csv('train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "#df_test = pd.read_csv('test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "\n",
    "df_train['el_rawcl_Es1Over2'] = df_train['el_rawcl_Es1'] / df_train['el_rawcl_Es2']\n",
    "df_test['el_rawcl_Es1Over2'] = df_test['el_rawcl_Es1'] / df_test['el_rawcl_Es2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_X = ['el_rawcl_E', 'el_rawcl_Es1Over2', 'el_f0', 'el_cl_aeta', ]  # input variables\n",
    "column_y = 'el_erawOverEtrue'\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "normalizer.adapt(np.array(df_train[columns_X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pdf to be used\n",
    "We have to choose the type of the pdf to use. To be generic, use a mixture of normal distributions, e.g.\n",
    "\n",
    "$$\n",
    "  \\sum_{i=1}^3 \\alpha_i \\times N[\\mu_i, \\sigma^2_i]\n",
    "$$\n",
    "\n",
    "The algorithm will learn the parameters (9 in this case) as a function of the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_shape = [1]  # the energy if 1D scalar quantity -> 1D pdf\n",
    "num_components = 3 # number of component of the pdf mixture (3 gaussian)\n",
    "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
    "params_size  # number of parameter of the final pdf (3 fractions, 3 means, 3 variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_template = tfp.layers.MixtureNormal(num_components, event_shape)\n",
    "#tfp.layers.MixtureSameFamily(num_components, tfp.layers.IndependentNormal(event_shape))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=len(columns_X)),\n",
    "    normalizer,    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(params_size, activation=None),\n",
    "    pdf_template,\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss\n",
    "This time we will define manually the loss. We will use the negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "# note: it would be better to add some regularization terms, constraining the parameters to don't be degenerate, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=negloglik)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(df_train[columns_X].values, df_train[column_y].values,\n",
    "                    epochs=50, batch_size=1024, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to the test sample\n",
    "Note that the output is not a value, but a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(df_test[columns_X].values)  # note we are not using predict\n",
    "df_results_tf = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the parameters of the estimated distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = yhat.mixture_distribution.probs_parameter().numpy()\n",
    "means = np.squeeze(yhat.components_distribution.mean().numpy())\n",
    "stds = np.sqrt(np.squeeze(yhat.components_distribution.variance().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, values, name in zip(axs.flat, (alphas, means, stds), ('alphas', 'means', 'stds')):\n",
    "    m, M = np.quantile(values, (0.01, 0.99))\n",
    "    for icomponent in range(num_components):\n",
    "        ax.hist(values[:, icomponent], bins=np.linspace(m, M, 100),\n",
    "                histtype='stepfilled',\n",
    "                label=f'component {icomponent}')\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_title(name, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the mean and std of the estimated response\n",
    "For each event we can estimated the resolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "df_test['NN_mean'] = yhat.mean().numpy().flatten()\n",
    "df_test['NN_std'] = np.sqrt(yhat.variance().numpy().flatten())\n",
    "\n",
    "\n",
    "axs[0].hist(df_test['NN_mean'], bins=100, density=True)\n",
    "axs[0].set_xlabel('mean of the estimated response', fontsize=15)\n",
    "\n",
    "axs[1].hist(df_test['NN_std'], bins=100)\n",
    "axs[1].set_xlabel('std of the estimated response', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respose of the first events\n",
    "As example plot the response evaluated by the model for the fist events in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.2, 1.3, 100)\n",
    "\n",
    "for idx in range(3):\n",
    "    ys = scipy.stats.norm(means[idx], stds[idx]).pdf(np.tile(xspace, (3, 1)).T)\n",
    "    ys = ys * alphas[idx]\n",
    "    y = ys.sum(axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.fill_between(xspace, y, color='0.7', label='NN')\n",
    "    ax.plot(xspace, ys, label=[f'NN component {icomp}' for icomp in range(num_components)])\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_title('event %d, rawE = %.0f GeV, |eta| = %.1f' % (idx, df_test.loc[idx, \"el_rawcl_E\"] / 1E3, df_test.loc[idx, \"el_cl_aeta\"]))\n",
    "    ax.set_xlabel('response = Eraw / Etrue', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange component is very small, probably it would be good to introduce a regularization to avoid it, or maybe it is not needed. It is also very large: it is describing outliers\n",
    "\n",
    "The green and blue have very different mean, which means that the model is able to describe the asymmetri of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model response with the test sample\n",
    "Evalue if the model is able to reproduce the distribution of the response from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.1, 1.4, 100)\n",
    "\n",
    "y = model(df_test[columns_X].values).tensor_distribution.prob(xspace.reshape(-1, 1, 1))\n",
    "ysum = np.sum(y, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(xspace, ysum / len(df_test), label='NN model')\n",
    "ax.hist(df_test[column_y], bins=50, density=True, label='test sample')\n",
    "ax.legend(loc=0, fontsize=14)\n",
    "ax.set_xlabel('response = Eraw / Etrue', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_yscale('log')  # look at the tails, quite well modelled\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution estimation\n",
    "We are able to estimate the resolution event by event! Check it differentially as a function of the input variables\n",
    "\n",
    "Note how smooth is the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile(df, xvar, energy_var, xedges, estimators=('median', 'sem')):\n",
    "    df_agg = df_test.groupby(np.digitize(df_test[xvar], xedges)).apply(\n",
    "        # approximate the error of the median as the standard error on the mean (sem)\n",
    "        lambda df: (df[energy_var] / df['el_truth_E']).apply(estimators))\n",
    "    df_agg = df_agg.reindex(range(1, len(xedges)))\n",
    "    xmidpoints = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "    df_agg.index = xmidpoints\n",
    "    return df_agg\n",
    "\n",
    "for xvar in columns_X:\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 20)).values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "    df_agg1 = get_profile(df_test, xvar, 'el_rawcl_E', xedges, ('std',) )\n",
    "    ax.plot(df_agg1.index, df_agg1.values, '.-', label='test sample')\n",
    "\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 50)).values\n",
    "    df_agg2 = df_test.groupby(np.digitize(df_test[xvar], xedges))['NN_std'].mean()\n",
    "    df_agg2 = df_agg2.reindex(range(1, len(xedges)))\n",
    "    ax.plot(0.5 * (xedges[1:] + xedges[:-1]), df_agg2.values, '.-', label='NN')\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.set_xlabel(xvar, fontsize=14)\n",
    "    ax.set_ylabel('resolution', fontsize=14)\n",
    "    ax.set_ylim(0.05, 0.13)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.hist(df_test[xvar], 50, color='0.7')\n",
    "    ax2.set_ylim(0, ax2.get_ylim()[1] * 1.5)\n",
    "    \n",
    "    ax.set_zorder(ax2.get_zorder() + 1)\n",
    "    ax.set_frame_on(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the best energy\n",
    "For each event we can estimate the best energy\n",
    "\n",
    "   * for comparison, the raw energy scaled by the  median response (evaluated on the test dataset)\n",
    "   * using as correction the mean of the estimated distribution\n",
    "   * using as correction the mean of the largest component of the estimated distribution (as an approximation of the mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "bins = np.linspace(0.5, 1.4, 100)\n",
    "\n",
    "ax.hist(df_test['el_rawcl_E'] / df_test['el_erawOverEtrue'].median() / df_test['el_truth_E'], label='raw median scaled', bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "correction_mean = yhat.mean().numpy().flatten()\n",
    "ax.hist(df_test['el_rawcl_E'] / correction_mean / df_test['el_truth_E'], label='NN from mean', bins=bins, density=True, histtype='step', lw=2)\n",
    "\n",
    "correction_dominant_mean = yhat.components_distribution.mean().numpy()[:, 1, 0]\n",
    "ax.hist(df_test['el_rawcl_E'] / correction_dominant_mean / df_test['el_truth_E'], label='NN from dominant mean', bins=bins, density=True, histtype='step', lw=2)\n",
    "\n",
    "ax.axvline(1, ls='--', color='0.3')\n",
    "ax.legend(loc=2, fontsize=13)\n",
    "ax.set_xlabel('E / Etrue', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
