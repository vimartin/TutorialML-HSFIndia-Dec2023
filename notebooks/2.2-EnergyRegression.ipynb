{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxHqdcFoCVYa"
   },
   "source": [
    "## Regression of the energy response distribution\n",
    "\n",
    "☢️☢️☢️ ***This can be a nice Qualification Task in egamma calibration*** ☢️☢️☢️\n",
    "\n",
    "Here we will try not just to evaluate the best calibrated energy, but to get the distribution of the response. Basically we will get the distribution of $E_\\text{raw}/E_\\text{true}$ (and not only its best value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooSgQmCsCVYg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import scipy\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uxyP9dBCVYj"
   },
   "source": [
    "## Import data\n",
    "Use the same inputs as in the previous simpler energy regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXeijOSrCVYk"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=U%2BMJxVi5El1wxtCz%2B45VqLmUuok%3D&Expires=1828739034')\n",
    "df_test = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=YKG4lzc%2FI0%2BcJZRnQG350DnVVK4%3D&Expires=1828739085')\n",
    "\n",
    "#df_train = pd.read_csv('train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "#df_test = pd.read_csv('test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "\n",
    "df_train['el_rawcl_Es1Over2'] = df_train['el_rawcl_Es1'] / df_train['el_rawcl_Es2']\n",
    "df_test['el_rawcl_Es1Over2'] = df_test['el_rawcl_Es1'] / df_test['el_rawcl_Es2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vEhvEXlCVYl"
   },
   "outputs": [],
   "source": [
    "columns_X = ['el_rawcl_E', 'el_rawcl_Es1Over2', 'el_f0', 'el_cl_aeta', ]  # input variables\n",
    "column_y = 'el_erawOverEtrue'\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "normalizer.adapt(np.array(df_train[columns_X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg6ZM6noCVYm"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unzuM5J2CVYm"
   },
   "source": [
    "### Pdf to be used\n",
    "We have to choose the type of the pdf to use. To be generic, use a mixture of normal distributions, e.g.\n",
    "\n",
    "$$\n",
    "  \\sum_{i=1}^3 \\alpha_i \\times N[\\mu_i, \\sigma^2_i]\n",
    "$$\n",
    "\n",
    "The algorithm will learn the parameters (9 in this case) as a function of the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mc2Be1WXCVYo"
   },
   "outputs": [],
   "source": [
    "event_shape = [1]  # the energy if 1D scalar quantity -> 1D pdf\n",
    "num_components = 3 # number of component of the pdf mixture (3 gaussian)\n",
    "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)\n",
    "params_size  # number of parameter of the final pdf (3 fractions, 3 means, 3 variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2L4J4UPsCVYo"
   },
   "outputs": [],
   "source": [
    "pdf_template = tfp.layers.MixtureNormal(num_components, event_shape)\n",
    "#tfp.layers.MixtureSameFamily(num_components, tfp.layers.IndependentNormal(event_shape))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=len(columns_X)),\n",
    "    normalizer,    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(params_size, activation=None),\n",
    "    pdf_template,\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCC2t8DTCVYq"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6NMwcjICVYr"
   },
   "source": [
    "## Define the loss\n",
    "This time we will define manually the loss. We will use the negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSwz4EohCVYr"
   },
   "outputs": [],
   "source": [
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "# note: it would be better to add some regularization terms, constraining the parameters to don't be degenerate, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAvbQKmlCVYs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emk9V6MhCVYt"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=negloglik)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(df_train[columns_X].values, df_train[column_y].values,\n",
    "                    epochs=50, batch_size=1024, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvqW5MyLCVYt"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CdYlnL7CVYu"
   },
   "source": [
    "## Apply the model to the test sample\n",
    "Note that the output is not a value, but a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mNacIDUCVYv"
   },
   "outputs": [],
   "source": [
    "yhat = model(df_test[columns_X].values)  # note we are not using predict\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOCPA8CoCVYx"
   },
   "source": [
    "## Plot the parameters of the estimated distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szVnvgeHCVYx"
   },
   "outputs": [],
   "source": [
    "alphas = yhat.mixture_distribution.probs_parameter().numpy()\n",
    "means = np.squeeze(yhat.components_distribution.mean().numpy())\n",
    "stds = np.sqrt(np.squeeze(yhat.components_distribution.variance().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g662Q_AOCVYy"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, values, name in zip(axs.flat, (alphas, means, stds), ('alphas', 'means', 'stds')):\n",
    "    m, M = np.quantile(values, (0.01, 0.99))\n",
    "    for icomponent in range(num_components):\n",
    "        ax.hist(values[:, icomponent], bins=np.linspace(m, M, 100),\n",
    "                histtype='stepfilled',\n",
    "                label=f'component {icomponent}')\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_title(name, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly one component is modelling the bulk, another the tail, another the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7FPu-anCVYz"
   },
   "source": [
    "## Plot the mean and std of the estimated response\n",
    "For each event we can estimated the resolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zFjGUuNCVYz"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "df_test['NN_mean'] = yhat.mean().numpy().flatten()\n",
    "df_test['NN_std'] = np.sqrt(yhat.variance().numpy().flatten())\n",
    "\n",
    "\n",
    "axs[0].hist(df_test['NN_mean'], bins=100, density=True)\n",
    "axs[0].set_xlabel('mean of the estimated response', fontsize=15)\n",
    "\n",
    "axs[1].hist(df_test['NN_std'], bins=100)\n",
    "axs[1].set_xlabel('std of the estimated response', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q28kwvqvCVY0"
   },
   "source": [
    "## Respose of the first events\n",
    "As example plot the response evaluated by the model for the fist events in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AEiGR5ACVY0"
   },
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.2, 1.3, 100)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "\n",
    "for idx, ax in enumerate(axs.flat):\n",
    "    ys = scipy.stats.norm(means[idx], stds[idx]).pdf(np.tile(xspace, (3, 1)).T)\n",
    "    ys = ys * alphas[idx]\n",
    "    y = ys.sum(axis=1)\n",
    "    ax.fill_between(xspace, y, color='0.7', label='NN')\n",
    "    ax.plot(xspace, ys, label=[f'NN comp. {icomp}' for icomp in range(num_components)])\n",
    "    if idx == 0:\n",
    "        ax.legend(loc=0, fontsize=13)\n",
    "    ax.set_title('event %d, rawE = %.0f GeV, |eta| = %.1f' % (idx, df_test.loc[idx, \"el_rawcl_E\"] / 1E3, df_test.loc[idx, \"el_cl_aeta\"]))\n",
    "    ax.set_xlabel('response = Eraw / Etrue', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0bKdVHcCVY1"
   },
   "source": [
    "Probably it would be good to introduce a regularization to very small components (maybe it is not needed?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkCSoN69CVY1"
   },
   "source": [
    "## Check the model response with the test sample\n",
    "Evalue if the model is able to reproduce the distribution of the response from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsJbgOfBCVY1"
   },
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.1, 1.4, 100)\n",
    "\n",
    "y = model(df_test[columns_X].values).tensor_distribution.prob(xspace.reshape(-1, 1, 1))\n",
    "ysum = np.sum(y, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(xspace, ysum / len(df_test), label='NN model')\n",
    "ax.hist(df_test[column_y], bins=50, density=True, label='test sample')\n",
    "ax.legend(loc=0, fontsize=14)\n",
    "ax.set_xlabel('response = Eraw / Etrue', fontsize=14)\n",
    "ax.set_title('all events', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pXfxajSCVY2"
   },
   "outputs": [],
   "source": [
    "ax.set_yscale('log')  # look at the tails, quite well modelled\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5WRGna2CVY2"
   },
   "source": [
    "## Resolution estimation\n",
    "We are able to estimate the resolution event by event! Check it differentially as a function of the input variables. Not perfect, but it is able to get the features.\n",
    "\n",
    "Note how smooth is the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P-jaXnHCVY2"
   },
   "outputs": [],
   "source": [
    "def get_profile(df, xvar, energy_var, xedges, estimators=('median', 'sem')):\n",
    "    df_agg = df_test.groupby(np.digitize(df_test[xvar], xedges)).apply(\n",
    "        # approximate the error of the median as the standard error on the mean (sem)\n",
    "        lambda df: (df[energy_var] / df['el_truth_E']).apply(estimators))\n",
    "    df_agg = df_agg.reindex(range(1, len(xedges)))\n",
    "    xmidpoints = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "    df_agg.index = xmidpoints\n",
    "    return df_agg\n",
    "\n",
    "for xvar in columns_X:\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 20)).values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "    df_agg1 = get_profile(df_test, xvar, 'el_rawcl_E', xedges, ('std',) )\n",
    "    ax.plot(df_agg1.index, df_agg1.values, '.-', label='test sample')\n",
    "\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 50)).values\n",
    "    df_agg2 = df_test.groupby(np.digitize(df_test[xvar], xedges))['NN_std'].mean()\n",
    "    df_agg2 = df_agg2.reindex(range(1, len(xedges)))\n",
    "    ax.plot(0.5 * (xedges[1:] + xedges[:-1]), df_agg2.values, '.-', label='NN')\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.set_xlabel(xvar, fontsize=14)\n",
    "    ax.set_ylabel('resolution', fontsize=14)\n",
    "    ax.set_ylim(0.05, 0.13)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.hist(df_test[xvar], 50, color='0.7', density=True)\n",
    "    ax2.set_ylim(0, ax2.get_ylim()[1] * 1.5)\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    ax.set_zorder(ax2.get_zorder() + 1)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_xlim(xedges[0], xedges[-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7-s1zxsCVY2"
   },
   "source": [
    "## Estimate the best energy\n",
    "For each event we can estimate the best energy\n",
    "\n",
    "   * for comparison, the raw energy scaled by the  median response (evaluated on the test dataset)\n",
    "   * using as correction the mean of the estimated distribution\n",
    "   * using as correction the mean of the largest component of the estimated distribution (as an approximation of the mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pgs4jidCVY3"
   },
   "outputs": [],
   "source": [
    "dominant_component = int(np.median(alphas.argmax(axis=1)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "bins = np.linspace(0.5, 1.4, 100)\n",
    "\n",
    "ax.hist(df_test['el_rawcl_E'] / df_test['el_erawOverEtrue'].median() / df_test['el_truth_E'], label='raw median scaled', bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "correction_mean = yhat.mean().numpy().flatten()\n",
    "ax.hist(df_test['el_rawcl_E'] / correction_mean / df_test['el_truth_E'], label='NN from mean', bins=bins, density=True, histtype='step', lw=2)\n",
    "\n",
    "correction_dominant_mean = yhat.components_distribution.mean().numpy()[:, dominant_component, 0]\n",
    "ax.hist(df_test['el_rawcl_E'] / correction_dominant_mean / df_test['el_truth_E'], label='NN from dominant mean', bins=bins, density=True, histtype='step', lw=2)\n",
    "\n",
    "ax.axvline(1, ls='--', color='0.3')\n",
    "ax.legend(loc=2, fontsize=13)\n",
    "ax.set_xlabel(r'$E / E_\\mathrm{true}$', fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2.2-EnergyRegression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
