{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wiso/TutorialML-AtlasItalia2022/blob/main/notebooks/0.2-IntroKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea4ec6",
   "metadata": {
    "id": "14ea4ec6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfjdfc78ZIDE",
   "metadata": {
    "id": "yfjdfc78ZIDE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ingrediets for ML\n",
    "\n",
    "   * A large, curated dataset\n",
    "   * A model, taking inputs and making predictions (e.g. a neural network)\n",
    "   * A loss, evaluating how well the model is performing, including a regularization to contrain the model\n",
    "   * A minimization procedure, to optimize the loss tuning the model parameters\n",
    "   * Several metrics, to evaluate the performance of the trained model\n",
    "   * Powerful hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "As model here we consider only neural networks, but many ML models are on the market. For regression and classification the main competitors are Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1BhIfZTyZIDP",
   "metadata": {
    "id": "1BhIfZTyZIDP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest neural network\n",
    "\n",
    "<img src=\"imgs/1neuron.png\" />\n",
    "<img src=\"imgs/dense.png\" />\n",
    "\n",
    "Consider a fully connected neural network with one single layer. Each neuron $i$ takes as input a vector $x\\in \\mathbb{R}^N$ and returns as output $\\sigma(W^{(i)} \\cdot  x + b^{(i)})$, where $W^{(i)}\\in\\mathbb{R}^N$ is a vector of weights and $b^{(i)}\\in\\mathbb{R}$ is the bias. $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is the response function and it must be non-linear. If we stack the output of all the $L$ neurons in a vector $y$ (the response of the layer):\n",
    "\n",
    "$$\n",
    "y = \\sigma(W x + b)\n",
    "$$\n",
    "\n",
    "in the formula above $\\sigma$ is applied on each elements in the parenthesis (elementwise). Here $W\\in\\mathbb{R}^{L\\times N}$ while $y, b\\in\\mathbb{R}^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATwH_YGhZIDQ",
   "metadata": {
    "id": "ATwH_YGhZIDQ"
   },
   "source": [
    "## Deep dense neural network\n",
    "\n",
    "\n",
    "<img src=\"imgs/Fullyconnected.png\" />\n",
    "\n",
    "We can stack several layers:\n",
    "\n",
    "$$\n",
    "y_1 = \\sigma_{L1}(W^{L1} x + b^{L1}) \\\\\n",
    "y_2 = \\sigma_{L2}(W^{L2} y_1 + b^{L2}) \\\\\n",
    "\\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z_wTsSsVZIDQ",
   "metadata": {
    "id": "z_wTsSsVZIDQ"
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jur3WTX_ZIDR",
   "metadata": {
    "id": "jur3WTX_ZIDR"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xspace = np.linspace(-3, 3, 100)\n",
    "for fname in 'relu', 'elu', 'gelu', 'selu', 'swish', 'tanh', 'sigmoid':\n",
    "    f = getattr(tf.keras.activations, fname)\n",
    "    ax.plot(xspace, f(xspace), label=fname)\n",
    "ax.legend(ncol=2, fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WOotheEQZIDS",
   "metadata": {
    "id": "WOotheEQZIDS"
   },
   "source": [
    "## Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6yNZNH-2ZIDS",
   "metadata": {
    "id": "6yNZNH-2ZIDS"
   },
   "outputs": [],
   "source": [
    "x_input = tf.keras.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x_input)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=x_input, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C1pIdjrAZIDT",
   "metadata": {
    "id": "C1pIdjrAZIDT"
   },
   "outputs": [],
   "source": [
    "test_input = np.array([[1, 2, 3, 4]])  # note the [[  ]]\n",
    "model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cukl7P00ZIDT",
   "metadata": {
    "id": "cukl7P00ZIDT"
   },
   "outputs": [],
   "source": [
    "test_input = np.array([[1, 2, 3, 4], [1, 2, 3, 4]])  \n",
    "model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n_SsayXoZIDU",
   "metadata": {
    "id": "n_SsayXoZIDU"
   },
   "source": [
    "## Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cNe7aGIeZIDU",
   "metadata": {
    "id": "cNe7aGIeZIDU"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(4, )),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u3ykOwFvZIDU",
   "metadata": {
    "id": "u3ykOwFvZIDU"
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "with open('model_description.json', 'w') as fn:\n",
    "    fn.write(model.to_json(indent=2))\n",
    "\n",
    "# load the model\n",
    "# from keras.models import model_from_json\n",
    "# with open(model_path,'r') as model_file:\n",
    "#     model = model_from_json(model_file.read())\n",
    "# model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics and losses\n",
    "Many losses exists and they are related to the metric we want to achieve, which is related to the specifi problem. Usually the metric cannot be used directly as a loss. The point is that the loss is a function on the whole sample, while the loss can be evaluated on each element of the sample, or at least on a mini-batch. In this way the loss of the sample can be defined as the sum of the losses for each element of the sample:\n",
    "\n",
    "$$L = \\sum_i l(\\hat y_i)$$\n",
    "\n",
    "where $\\hat y_i$ is the $i$-th output of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Metrics\n",
    "\n",
    "   * for classification: accuracy (fraction of correct guesses), signal significance, area under the curve, ...\n",
    "   * for regression: the resolution of the estimated quantity\n",
    "\n",
    "Remember that ML is not just classification and regression..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Losses\n",
    "\n",
    "   * for classification: for exampes for signal vs background: binary cross entropy (e.g. $y_i = 0$ or $1$)\n",
    "   $$\n",
    "   −\\sum_i (y_i \\log(\\hat y_i) + (1−y_i) \\log(1−\\hat y_i))\n",
    "   $$\n",
    "   * for regression: mean squared error, mean absolute error, ...\n",
    "   \n",
    "In the loss many other regularization terms may be added, for example for each layer $L_2 = \\sum_{ij} |W_{ij}|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimization\n",
    "The training procedure updates the parameters of the model to minimize the loss evaluated on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe86e5",
   "metadata": {
    "id": "0afe86e5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Automatic differentiation\n",
    "The key ingredient for optimize neural network is the ability to compute the gradient of the loss with respect to the parameters of the model. This is achived with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfSdrYXdZIDH",
   "metadata": {
    "id": "mfSdrYXdZIDH"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 1.) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc6aea",
   "metadata": {
    "id": "70dc6aea"
   },
   "source": [
    "### In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b7c26",
   "metadata": {
    "id": "c22b7c26"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3mAwO5w4ZIDK",
   "metadata": {
    "id": "3mAwO5w4ZIDK"
   },
   "source": [
    "### In Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yvw5lMv2ZIDL",
   "metadata": {
    "id": "Yvw5lMv2ZIDL"
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "\n",
    "f_dx = autograd.grad(f)\n",
    "f_dx(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08bc26",
   "metadata": {
    "id": "6e08bc26"
   },
   "source": [
    "### In Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00JcSIYLZIDM",
   "metadata": {
    "id": "00JcSIYLZIDM"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "f_dx = jax.grad(f)\n",
    "f_dx(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MNH-ZLGzZIDN",
   "metadata": {
    "id": "MNH-ZLGzZIDN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using control flow\n",
    "You can differentiate functions with `if`/`for`/..., recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9f9ba",
   "metadata": {
    "id": "97f9f9ba"
   },
   "outputs": [],
   "source": [
    "# complicated function with if and for\n",
    "def f(x):\n",
    "    if x > 0:                        # condition\n",
    "        for i in range(2):           # for loop\n",
    "            x += jax.numpy.sqrt(x)\n",
    "        return x / 10.\n",
    "    else:\n",
    "        return f(f(x ** 2)) + 1      # recursion\n",
    "    \n",
    "f_dx = jax.grad(f)\n",
    "f_dx(3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kbfcCf2uZIDP",
   "metadata": {
    "id": "kbfcCf2uZIDP"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "xspace = np.linspace(-2., 5, 500)\n",
    "yi = np.asarray([f(xx) for xx in xspace])\n",
    "ax.plot(xspace, yi, label='function')\n",
    "yi = np.asarray([f_dx(xx) for xx in xspace])\n",
    "ax.plot(xspace, yi, label='derivative')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2j8j6y-dZIDV",
   "metadata": {
    "id": "2j8j6y-dZIDV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimizers\n",
    "\n",
    "Once you have defined the loss, you want to optimize the parameters of the model to minimize the loss.\n",
    "\n",
    "Many optimizers are based on Stocatstic Gradient Descend. Usually our loss is defiend as\n",
    "\n",
    "$$\n",
    "L = \\sum_q l_w(x_q)\n",
    "$$\n",
    "\n",
    "where the sum is over the element of the training sample. We can apply gradient descend to optimize it updating in  iterative way the weights $w$:\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_{i} - \\eta \\nabla_w L = w_{i} - \\eta \\nabla_w \\sum_q l_w(x_q)\n",
    "$$\n",
    "\n",
    "Here we need to compute the gradient for all the elements. Instead in SGD we can consider only one (in random order):\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_{i} - \\eta \\nabla_w l_w(x_{i+1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_hyhD0syZIDV",
   "metadata": {
    "id": "_hyhD0syZIDV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Minimize a function without data\n",
    "Let minimize a function (the loss) which have only parameters (and no data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XrDmaS37ZIDV",
   "metadata": {
    "id": "XrDmaS37ZIDV"
   },
   "outputs": [],
   "source": [
    "# define a function without data\n",
    "var = tf.Variable(starting_point := 1.0)\n",
    "loss = lambda: (var ** 2)\n",
    "\n",
    "def minimize_with_history(loss, opt, nepochs):\n",
    "    history_steps = [(var.numpy(), var.numpy() ** 2)]\n",
    "    for epoch in range(nepochs):\n",
    "        opt.minimize(loss, [var])\n",
    "        history_steps.append([var.numpy(), loss()])\n",
    "    return np.asarray(history_steps)\n",
    "\n",
    "# this represent just one minimization step (not the full minimization)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "history_steps1 = minimize_with_history(loss, opt, 50)\n",
    "\n",
    "# reset and use a huge learning_rate\n",
    "var = tf.Variable(starting_point)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.99)\n",
    "history_steps2 = minimize_with_history(loss, opt, 50)\n",
    "\n",
    "# reset and use a tiny learning_rate\n",
    "var = tf.Variable(starting_point)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "history_steps3 = minimize_with_history(loss, opt, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0siXChkMZIDW",
   "metadata": {
    "id": "0siXChkMZIDW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "xspace = np.linspace(-1, 1, 100)\n",
    "ax.plot(xspace, xspace ** 2, color='0.7')\n",
    "ax.plot(*history_steps1.T, '.-', label='0.1')\n",
    "ax.plot(*history_steps2.T, '.-', label='0.9')\n",
    "ax.plot(*history_steps3.T, '.-', label='0.001')\n",
    "ax.legend(title='learning rate', loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahwdgZHlZIDW",
   "metadata": {
    "id": "ahwdgZHlZIDW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear fit 1D\n",
    "Minimize the usual \n",
    "$$L(m, q) = \\sum_i(f(m, q, x_i) - y_i)^2$$\n",
    "\n",
    "where $f$ is our model $f(x) = mx+q$. In this case we need to feed the data to the model. Let use minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KvKeRFnrZIDW",
   "metadata": {
    "id": "KvKeRFnrZIDW"
   },
   "outputs": [],
   "source": [
    "data_x = np.arange(0, 1000.)  # 1000 points\n",
    "data_y = data_x * 2.5 + 1.2  # true-y\n",
    "data_y = np.random.normal(data_y, data_y * 0.1 + 0.1)\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.m = tf.Variable(5.)  # random numbers (use floats)\n",
    "        self.q = tf.Variable(10.)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.m * x + self.q\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.000001),\n",
    "    loss=tf.keras.losses.mean_squared_error,\n",
    ")\n",
    "model.fit(data_x, data_y, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tfZf6PNZIDX",
   "metadata": {
    "id": "6tfZf6PNZIDX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y)\n",
    "plt.plot(data_x, model.predict(data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4MPvjQQZIDX",
   "metadata": {
    "id": "N4MPvjQQZIDX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not only ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6NQYmA8tZIDX",
   "metadata": {
    "id": "6NQYmA8tZIDX"
   },
   "source": [
    "### Statistics\n",
    "\n",
    "Let define the likeilhood of a counting experiments, one category, one signal, background uncertainty. The parameters are the POI (signal strenght) and the NP about the background uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f938b",
   "metadata": {
    "id": "ad4f938b",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pyhf\n",
    "pyhf.set_backend('jax')\n",
    "\n",
    "# make a counting experiment\n",
    "model = pyhf.simplemodels.uncorrelated_background(signal=[5.], bkg=[10.], bkg_uncertainty=[3.5])\n",
    "pars = jax.numpy.array(model.config.suggested_init())\n",
    "\n",
    "# generate an Asimov dataset (e.g. 15 events observed)\n",
    "data = jax.numpy.array(model.expected_data(model.config.suggested_init()))\n",
    "\n",
    "bestfit = pyhf.infer.mle.fit(data, model)  # not really needed since it is an Asimov\n",
    "bestfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0374",
   "metadata": {
    "id": "548f0374",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "H = -2 * jax.hessian(model.logpdf)(bestfit, data)[0]\n",
    "cov = np.linalg.inv(H)\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27072e",
   "metadata": {
    "id": "7a27072e"
   },
   "source": [
    "You can compute the Hessian of the likelihood with autodifferentiation.\n",
    "\n",
    "If you have a likelihood, you don't need any minimization to compute the expected errors!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52UOTqFbZIDY",
   "metadata": {
    "id": "52UOTqFbZIDY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f07520",
   "metadata": {
    "id": "c1f07520"
   },
   "outputs": [],
   "source": [
    "grid = x, y = np.meshgrid(np.linspace(0.5, 1.5, 101), np.linspace(0.5, 1.5, 101))\n",
    "\n",
    "def get_ellispse_from_covariance(x, y, covariance, z=1, *args, **kwargs):\n",
    "    from matplotlib.patches import Ellipse\n",
    "    ls, vs = np.linalg.eig(covariance)\n",
    "\n",
    "    vs_max = vs[np.argmax(np.abs(ls))]\n",
    "    vs_min = vs[np.argmin(np.abs(ls))]\n",
    "    angle = np.arctan(vs_max[0] / vs_max[1])\n",
    "\n",
    "    return Ellipse((x, y),\n",
    "                   np.sqrt(max(np.min(ls), 0)) * z,\n",
    "                   np.sqrt(np.max(ls)) * z,\n",
    "                   angle / np.pi * 180, *args, **kwargs)\n",
    "\n",
    "ellipse = get_ellispse_from_covariance(bestfit[0], bestfit[1], cov, fill=False, edgecolor='k', lw=2)\n",
    "\n",
    "points = np.swapaxes(grid, 0, -1).reshape(-1, 2)\n",
    "v = jax.vmap(model.logpdf, in_axes=(0, None))(points, data)\n",
    "v = np.swapaxes(v.reshape(101, 101), 0, -1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.pcolormesh(x, y, v)\n",
    "\n",
    "grid = x, y = np.meshgrid(np.linspace(0.5, 1.5, 11), np.linspace(0.5, 1.5, 11))\n",
    "points = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "values, gradients = jax.vmap(\n",
    "    jax.value_and_grad(lambda p,d: model.logpdf(p,d)[0]),\n",
    "    in_axes = (0,None))(points, data)\n",
    "\n",
    "ax.quiver(points[:,0], points[:,1], gradients[:,0], gradients[:,1],\n",
    "          angles = 'xy', scale = 75)\n",
    "ax.scatter(bestfit[0], bestfit[1], c='r')\n",
    "ax.add_patch(ellipse)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebe1be",
   "metadata": {
    "id": "c1ebe1be",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heavy number crunching\n",
    "Even if the interface to most of the ML is in python, the expressions (the model, but also the minimization steps, the preprocessing, ...) are represented as a computational graph, which is optimized, compiled and distributed to the optimal hardware (CPU/GPU/TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d702e6e",
   "metadata": {
    "id": "5d702e6e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ymin, ymax = -1.5, 1.5\n",
    "xmin, xmax = -1.5, 1.5\n",
    "\n",
    "nx, ny = 500, 500\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(xmin, xmax, nx), np.linspace(ymin, ymax, ny))\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Grid of complex numbers\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "\n",
    "# Z-values for determining divergence; initialized at zero\n",
    "zs = tf.zeros_like(xs)\n",
    "\n",
    "# N-values store the number of iterations taken before divergence\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "def step(c, z, n):\n",
    "    z = z * z + c\n",
    "    \n",
    "    not_diverged = tf.abs(z) < 4\n",
    "    n = tf.add(n, tf.cast(not_diverged, tf.float32))\n",
    "    \n",
    "    return c, z, n\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "iterations = 1000\n",
    "\n",
    "# mandelbrot\n",
    "for _ in range(iterations): \n",
    "    xs, zs, ns = step(xs, zs, ns)\n",
    "\n",
    "def shade_fractal(fractal):\n",
    "    fractal = np.where(fractal == 0, iterations, fractal)\n",
    "    fractal = fractal / fractal.max()\n",
    "    fractal = np.log10(fractal)  \n",
    "    return fractal\n",
    "\n",
    "axs[0].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "#julia\n",
    "zs = tf.zeros_like(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "for _ in range(iterations): \n",
    "    zs, xs, ns = step(-0.7269 + 0.1889j, xs, ns)\n",
    "    \n",
    "axs[1].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i3OE7GMtZIDZ",
   "metadata": {
    "id": "i3OE7GMtZIDZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hardware\n",
    "Computing power to train the model (in PFlop/s $\\times$ day). 3.4-month doubling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gBJr-wm5ZIDZ",
   "metadata": {
    "id": "gBJr-wm5ZIDZ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"1000\" src=\"imgs/ai-and-compute-all-error-no-title.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyGp8CS5ZIDa",
   "metadata": {
    "id": "wyGp8CS5ZIDa",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from https://arxiv.org/abs/2005.04305\n",
    "<img src=\"imgs/ai-and-efficiency-compute.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qbEcHWSoZIDa",
   "metadata": {
    "id": "qbEcHWSoZIDa"
   },
   "source": [
    "GPT-3 175B model (175B parameters) required $3.14\\times 10^{23}$ flop for training. Even at theoretical 28 TFLOPS for V100 GPU (1 = 10k\\\\$) and lowest 3 year reserved cloud pricing we could find, this will take 355 GPU-years and cost \\\\$4.6M for a single training.\n",
    "\n",
    "<img src=\"imgs/gpt3_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oQ2sxQTNZIDa",
   "metadata": {
    "id": "oQ2sxQTNZIDa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU\n",
    "\n",
    "GPU exposes massive parallelism\n",
    "\n",
    "<img src=\"imgs/A100.png\">\n",
    "\n",
    "The block diagram for A100 below shows an architecture with 128 streaming multiprocessor (SMs) (though only 108 are actually enabled in a production A100 chip).\n",
    "\n",
    "<img src=\"imgs/A100_block_diagram.png\">\n",
    "\n",
    "### Pascal / Volta / Ampere SM (Compute Capability 6.0, 7.0, 8.0)\n",
    "\n",
    "- 64 SP units (\"CUDA cores\")\n",
    "- 32 DP units\n",
    "- LD/ST units\n",
    "- FP16 at twice the SP rate\n",
    "- Pascal: 2 warp schedulers; Volta, Ampere: 4 warp schedulers\n",
    "- Tensor Cores (in the Volta and Ampere variants)\n",
    "- INT32 units (in the Volta and Ampere variants)\n",
    "- P100: 56 SMs, 16 GB\n",
    "- V100: 80 SMs, 16/32 GB\n",
    "- A100: 108 SMs, 40 GB\n",
    "\n",
    "<img src=\"imgs/volta_sm.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aq_6xWiLZIDa",
   "metadata": {
    "id": "Aq_6xWiLZIDa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Serverless solutions\n",
    "\n",
    "Google Cloud Platform, AWS Lambda, IBM Watson, Microsoft Azure, Lambdalabs, Paperspace, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duylgh72ZIDa",
   "metadata": {
    "id": "duylgh72ZIDa",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Swan (https://swan-k8s.cern.ch)\n",
    "\n",
    "<img src=\"imgs/swan.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1MAIocuVZIDb",
   "metadata": {
    "id": "1MAIocuVZIDb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### INFN Cloud\n",
    "<img src=\"imgs/infn_cloud.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EeWUQUapZIDb",
   "metadata": {
    "id": "EeWUQUapZIDb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But don't be scared. For a simple NN with tens of inputs your laptop is usually ok"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "include_colab_link": true,
   "name": "0.2-IntroKeras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
