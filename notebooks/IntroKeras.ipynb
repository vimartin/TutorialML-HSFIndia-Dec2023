{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe86e5",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "The key ingredient for optimize neural network is the ability to compute gradient with respect to the parameters of the mode. This is achived with automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc6aea",
   "metadata": {},
   "source": [
    "### In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "def f(x):\n",
    "    return (x - 1) ** 2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08bc26",
   "metadata": {},
   "source": [
    "### In Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dx = grad(f)\n",
    "f_dx(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return ((x[0] - 0.) + (x[1] - 1.) + (x[2] - 2.) + (x[3] - 3.)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "f_dx = grad(f)\n",
    "f_dx([1., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625688dd",
   "metadata": {},
   "source": [
    "### Minimize the function with and without the exact gradient\n",
    "Start the minimization very far from the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2108bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(f, x0=[0., 2, 2, 2], method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2942e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(f, x0=(2, 2, 2, 2), jac=jax.jit(f_dx), method=\"Newton-CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c23ddd",
   "metadata": {},
   "source": [
    "If we now the gradient we can optimize a function in less steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x > 2:\n",
    "        for i in range(10):\n",
    "            x += jnp.sqrt(x)\n",
    "        return x\n",
    "    else:\n",
    "        return jnp.cos(x ** 3)\n",
    "    \n",
    "f_dx = jax.grad(f)\n",
    "    \n",
    "xspace = jnp.linspace(-2, 5, 200)\n",
    "yi = np.asarray([f_dx(xx) for xx in xspace])\n",
    "plt.plot(xspace, yi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dx(np.linspace(0., 1., 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f33d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize_scalar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f623ce6",
   "metadata": {},
   "source": [
    "## Not only ML\n",
    "\n",
    "### Statistics\n",
    "\n",
    "Let define the likeilhood of a counting experiments, one category, one signal, background uncertainty. The parameters are the POI (signal strenght) and the NP about the background uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "pyhf.set_backend('jax')\n",
    "\n",
    "# make a counting experiment\n",
    "model = pyhf.simplemodels.uncorrelated_background(signal=[5.], bkg=[10.], bkg_uncertainty=[3.5])\n",
    "pars = jnp.array(m.config.suggested_init())\n",
    "\n",
    "# generate an Asimov dataset (e.g. 15 events observed)\n",
    "data = jnp.array(model.expected_data(model.config.suggested_init()))\n",
    "\n",
    "bestfit = pyhf.infer.mle.fit(data, m)  # not really needed since it is an Asimov\n",
    "bestfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = -2 * jax.hessian(model.logpdf)(bestfit, data)[0]\n",
    "np.linalg.inv(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27072e",
   "metadata": {},
   "source": [
    "We are able to compute the expected errros without any minimization!\n",
    "\n",
    "Plot the likelihood as a function of the parameters ***the gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f07520",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = x, y = np.meshgrid(np.linspace(0.5, 1.5, 101), np.linspace(0.5, 1.5, 101))\n",
    "\n",
    "points = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "v = jax.vmap(model.logpdf, in_axes = (0,None))(points,data)\n",
    "v = np.swapaxes(v.reshape(101,101),0,-1)\n",
    "plt.contourf(x,y,v, levels = 100)\n",
    "plt.contour(x,y,v, levels = 20, colors = 'w')\n",
    "\n",
    "\n",
    "grid = x,y = np.meshgrid(np.linspace(0.5, 1.5, 11), np.linspace(0.5, 1.5, 11))\n",
    "points = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "values, gradients = jax.vmap(\n",
    "    jax.value_and_grad(\n",
    "        lambda p,d: model.logpdf(p,d)[0]\n",
    "    ), in_axes = (0,None)\n",
    ")(points,data)\n",
    "\n",
    "plt.quiver(\n",
    "    points[:,0],\n",
    "    points[:,1],\n",
    "    gradients[:,0],\n",
    "    gradients[:,1],\n",
    "    angles = 'xy',\n",
    "    scale = 75\n",
    ")\n",
    "plt.scatter(bestfit[0],bestfit[1], c = 'r')\n",
    "\n",
    "plt.xlim(0.5,1.5)\n",
    "plt.ylim(0.5,1.5)\n",
    "plt.gcf().set_size_inches(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebe1be",
   "metadata": {},
   "source": [
    "## Heavy number crunching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d702e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymin, ymax = -1.5, 1.5\n",
    "xmin, xmax = -1.5, 1.5\n",
    "\n",
    "nx, ny = 500, 500\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(xmin, xmax, nx), np.linspace(ymin, ymax, ny))\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Grid of complex numbers\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "\n",
    "# Z-values for determining divergence; initialized at zero\n",
    "zs = tf.zeros_like(xs)\n",
    "\n",
    "# N-values store the number of iterations taken before divergence\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "def step(c, z, n):\n",
    "    z = z * z + c\n",
    "    \n",
    "    not_diverged = tf.abs(z) < 4\n",
    "    n = tf.add(n, tf.cast(not_diverged, tf.float32))\n",
    "    \n",
    "    return c, z, n\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "iterations = 1000\n",
    "\n",
    "# mandelbrot\n",
    "for _ in range(iterations): \n",
    "    xs, zs, ns = step(xs, zs, ns)\n",
    "\n",
    "def shade_fractal(fractal):\n",
    "    fractal = np.where(fractal == 0, iterations, fractal)\n",
    "    fractal = fractal / fractal.max()\n",
    "    fractal = np.log10(fractal)  \n",
    "    return fractal\n",
    "\n",
    "axs[0].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "#julia\n",
    "zs = tf.zeros_like(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "for _ in range(iterations): \n",
    "    zs, xs, ns = step(-0.7269 + 0.1889j, xs, ns)\n",
    "    \n",
    "axs[1].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_aspect('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
