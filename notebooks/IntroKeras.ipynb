{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea4ec6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ingrediet for ML\n",
    "\n",
    "   * A large, curated dataset\n",
    "   * A model, taking inputs and making predictions (e.g. a neural network)\n",
    "   * A loss, evaluating how well the model is performing, including a regularization to contrain the model\n",
    "   * A minimization procedure, to optimize the loss tuning the model parameters\n",
    "   * Several metrics, to evaluate the performance of the trained model\n",
    "   * Powerful hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe86e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation\n",
    "The key ingredient for optimize neural network is the ability to compute the gradient of the loss with respect to the parameters of the model. This is achived with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 1) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc6aea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b7c26",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "\n",
    "f_dx = autograd.grad(f)\n",
    "f_dx(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08bc26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "f_dx = jax.grad(f)\n",
    "f_dx(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using control flow\n",
    "You can differentiate function with `if`/`for`/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x > 2:\n",
    "        for i in range(10):\n",
    "            x += jax.numpy.sqrt(x)\n",
    "        return x / 10.\n",
    "    else:\n",
    "        return jax.numpy.cos(x ** 3)\n",
    "    \n",
    "f_dx = jax.grad(f)\n",
    "    \n",
    "xspace = np.linspace(-2., 5, 500)\n",
    "yi = np.asarray([f(xx) for xx in xspace])\n",
    "plt.plot(xspace, yi, label='function')\n",
    "yi = np.asarray([f_dx(xx) for xx in xspace])\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(xspace, yi, label='derivative')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest neural network\n",
    "Consider a fully connected neural network with one single layer. Each neuron $i$ takes a vector $x\\in \\mathbb{R}^N$ and returns as output $\\sigma(W^{(i)} \\cdot  x + b^{(i)})$, where $W^{(i)}\\in\\mathbb{R}^N$ is a vector of weights and $b^{(i)}\\in\\mathbb{R}$ is the bias. $\\sigma$ is the response function and it must be non-linear. If we stack the output of all the $L$ neurons in a vector $y$ (the response of the layer):\n",
    "\n",
    "$$\n",
    "y = \\sigma(W x + b)\n",
    "$$\n",
    "\n",
    "in the formula above $\\sigma$ is applied on each elements in the parenthesis. Here $W\\in\\mathbb{R}^{L\\times N}$ while $y, b\\in\\mathbb{R}^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep dense neural network\n",
    "\n",
    "We can stack several layers:\n",
    "\n",
    "$$\n",
    "y_1 = \\sigma_{L1}(W^{L1} x + b^{L1}) \\\\\n",
    "y_2 = \\sigma_{L2}(W^{L2} y_1 + b^{L2}) \\\\\n",
    "\\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xspace = np.linspace(-2, 2, 100)\n",
    "for fname in 'relu', 'elu', 'gelu', 'selu', 'swish', 'tanh':\n",
    "    f = getattr(tf.keras.activations, fname)\n",
    "    ax.plot(xspace, f(xspace), label=fname)\n",
    "ax.legend(ncol=2, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.keras.Input(shape=(4))\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x_input)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=x_input, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_input = np.array([[1, 2, 3, 4]])  # note the [[  ]]\n",
    "model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.array([[1, 2, 3, 4], [1, 2, 3, 4]])  \n",
    "model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(4, )),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not only ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistics\n",
    "\n",
    "Let define the likeilhood of a counting experiments, one category, one signal, background uncertainty. The parameters are the POI (signal strenght) and the NP about the background uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "pyhf.set_backend('jax')\n",
    "\n",
    "# make a counting experiment\n",
    "model = pyhf.simplemodels.uncorrelated_background(signal=[5.], bkg=[10.], bkg_uncertainty=[3.5])\n",
    "pars = jnp.array(model.config.suggested_init())\n",
    "\n",
    "# generate an Asimov dataset (e.g. 15 events observed)\n",
    "data = jnp.array(model.expected_data(model.config.suggested_init()))\n",
    "\n",
    "bestfit = pyhf.infer.mle.fit(data, model)  # not really needed since it is an Asimov\n",
    "bestfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0374",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "H = -2 * jax.hessian(model.logpdf)(bestfit, data)[0]\n",
    "np.linalg.inv(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27072e",
   "metadata": {},
   "source": [
    "We are able to compute the expected errros without any minimization!\n",
    "\n",
    "Plot the likelihood as a function of the parameters with ***the gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f07520",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid = x, y = np.meshgrid(np.linspace(0.5, 1.5, 101), np.linspace(0.5, 1.5, 101))\n",
    "\n",
    "points = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "v = jax.vmap(model.logpdf, in_axes = (0,None))(points,data)\n",
    "v = np.swapaxes(v.reshape(101,101),0,-1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(x,y,v, levels = 100)\n",
    "ax.contour(x,y,v, levels = 20, colors = 'w')\n",
    "\n",
    "\n",
    "grid = x,y = np.meshgrid(np.linspace(0.5, 1.5, 11), np.linspace(0.5, 1.5, 11))\n",
    "points = np.swapaxes(grid,0,-1).reshape(-1,2)\n",
    "values, gradients = jax.vmap(\n",
    "    jax.value_and_grad(\n",
    "        lambda p,d: model.logpdf(p,d)[0]\n",
    "    ), in_axes = (0,None)\n",
    ")(points,data)\n",
    "\n",
    "ax.quiver(\n",
    "    points[:,0],\n",
    "    points[:,1],\n",
    "    gradients[:,0],\n",
    "    gradients[:,1],\n",
    "    angles = 'xy',\n",
    "    scale = 75\n",
    ")\n",
    "ax.scatter(bestfit[0],bestfit[1], c = 'r')\n",
    "\n",
    "ax.set_xlim(0.5,1.5)\n",
    "ax.set_ylim(0.5,1.5)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebe1be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heavy number crunching\n",
    "Even if the interface to most of the ML is in python, the expressions (the model, but also the minimization steps, the preprocessing, ...) are represented as a computational graph, which is optimized, compiled and distributed to the optimal hardware (CPU/GPU/TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d702e6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ymin, ymax = -1.5, 1.5\n",
    "xmin, xmax = -1.5, 1.5\n",
    "\n",
    "nx, ny = 500, 500\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(xmin, xmax, nx), np.linspace(ymin, ymax, ny))\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Grid of complex numbers\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "\n",
    "# Z-values for determining divergence; initialized at zero\n",
    "zs = tf.zeros_like(xs)\n",
    "\n",
    "# N-values store the number of iterations taken before divergence\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "def step(c, z, n):\n",
    "    z = z * z + c\n",
    "    \n",
    "    not_diverged = tf.abs(z) < 4\n",
    "    n = tf.add(n, tf.cast(not_diverged, tf.float32))\n",
    "    \n",
    "    return c, z, n\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "iterations = 1000\n",
    "\n",
    "# mandelbrot\n",
    "for _ in range(iterations): \n",
    "    xs, zs, ns = step(xs, zs, ns)\n",
    "\n",
    "def shade_fractal(fractal):\n",
    "    fractal = np.where(fractal == 0, iterations, fractal)\n",
    "    fractal = fractal / fractal.max()\n",
    "    fractal = np.log10(fractal)  \n",
    "    return fractal\n",
    "\n",
    "axs[0].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "#julia\n",
    "zs = tf.zeros_like(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "for _ in range(iterations): \n",
    "    zs, xs, ns = step(-0.7269 + 0.1889j, xs, ns)\n",
    "    \n",
    "axs[1].pcolormesh(X, Y, shade_fractal(ns), shading='gouraud')    \n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ai-and-compute-all-error-no-title.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from https://arxiv.org/abs/2005.04305\n",
    "<img src=\"imgs/ai-and-efficiency-compute.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3 175B model (175B parameters) required $3.14\\times 10^{23}$ flop for training. Even at theoretical 28 TFLOPS for V100 GPU (1 = 10k\\\\$) and lowest 3 year reserved cloud pricing we could find, this will take 355 GPU-years and cost \\\\$4.6M for a single training.\n",
    "\n",
    "<img src=\"imgs/gpt3_table.png\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
