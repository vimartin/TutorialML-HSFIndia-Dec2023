{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy regression (calibration)\n",
    "Compare different algorithm to calibrate the energy and check the performances\n",
    "\n",
    "    This is not very different to what is done in ATLAS for electrons and photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=U%2BMJxVi5El1wxtCz%2B45VqLmUuok%3D&Expires=1828739034')\n",
    "df_test = pd.read_csv('http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=YKG4lzc%2FI0%2BcJZRnQG350DnVVK4%3D&Expires=1828739085')\n",
    "\n",
    "#df_train = pd.read_csv('train_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "#df_test = pd.read_csv('test_electron_Et0-10_eta1.0-1.2_Eaccordion.csv')\n",
    "\n",
    "df_train['el_rawcl_Es1Over2'] = df_train['el_rawcl_Es1'] / df_train['el_rawcl_Es2']\n",
    "df_test['el_rawcl_Es1Over2'] = df_test['el_rawcl_Es1'] / df_test['el_rawcl_Es2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "Normalize the data such that the average of of each variable is 0 and the variance is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_X = ['el_rawcl_E', 'el_rawcl_Es1Over2', 'el_f0', 'el_cl_aeta', ]  # input variables\n",
    "column_y = 'el_erawOverEtrue'  # this is the target\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "# this will take a while since it needs to read all your data and compute the mean and the variabnce\n",
    "# normalization is non-trainable layer, it must be run before the training\n",
    "normalizer.adapt(np.array(df_train[columns_X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the normalization works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.reduce_mean(normalizer(df_train[columns_X]), axis=0))\n",
    "print(tf.math.reduce_variance(normalizer(df_train[columns_X]), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model: linear regression\n",
    "The output is just a linear combination of the inputs\n",
    "\n",
    "$$\\hat y = \\sum_i w_i x_i + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = tf.keras.Input(shape=len(columns_X))\n",
    "x = tf.keras.layers.Dense(units=1, activation=None, name='out_0')(x_inputs)\n",
    "\n",
    "model_linear_regression = tf.keras.Model(x_inputs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear_regression.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n",
    "                                loss='mean_absolute_error')\n",
    "\n",
    "history = model_linear_regression.fit(\n",
    "    normalizer(df_train[columns_X].values),\n",
    "    df_train[column_y].values,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_split=0.2) # Calculate validation results on 20% of the training data.\n",
    "\n",
    "# note that the normalization is applied here, and not inside the model.\n",
    "# This is because of a limitation of lwtnn (see at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basically the linear regression is just shifting the response to 1\n",
    "Dump the weights and the bias of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, weight in zip(columns_X, model_linear_regression.layers[1].kernel):\n",
    "    print(\"{:<20s} : {:+.3f}\".format(col, weight.numpy()[0]))\n",
    "print(\"bias                 : {:+.3f}\".format(model_linear_regression.layers[1].bias.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the median is very close to the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[column_y].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance with test sample\n",
    "Compare the performance checking the $E/E_\\text{true}$ distribution using different version of the $E$:\n",
    "\n",
    "   * the raw energy\n",
    "   * the raw energy scaled so that the median of $E/E_\\text{true}$ is 1\n",
    "   * the output of the trained algorithm\n",
    "   \n",
    "Take into account that if we scale the energy with a number $>1$ we will increase the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['E_linear_regression'] = df_test['el_rawcl_E'] / model_linear_regression.predict(normalizer(df_test[columns_X])).T[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['E_median_shifted'] = df_test['el_rawcl_E'] / df_test['el_erawOverEtrue'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xspace = np.linspace(0.4, 1.4, 100)\n",
    "ax.hist(df_test['el_rawcl_E'] / df_test['el_truth_E'], bins=xspace, label='raw', histtype='step')\n",
    "ax.hist(df_test['E_median_shifted'] / df_test['el_truth_E']  , bins=xspace, label='raw scaled', histtype='step', lw=2)\n",
    "ax.hist(df_test['E_linear_regression'] / df_test['el_truth_E'], bins=xspace, label='linear model', histtype='stepfilled')\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('E/Etrue', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the linearity as a function of input variables\n",
    "If we are using all the information the ratio of E/Etrue should be 1 as a function of the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile(df, xvar, energy_var, xedges):\n",
    "    df_agg = df_test.groupby(np.digitize(df_test[xvar], xedges)).apply(\n",
    "        # approximate the error of the median as the standard error on the mean (sem)\n",
    "        lambda df: (df[energy_var] / df['el_truth_E']).apply(('median', 'sem')))\n",
    "    df_agg = df_agg.reindex(range(1, len(xedges)))\n",
    "    xmidpoints = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "    df_agg.index = xmidpoints\n",
    "    return df_agg\n",
    "\n",
    "def plot_profile(ax, df_agg, **kwargs):\n",
    "    ax.errorbar(df_agg.index, df_agg['median'], df_agg['sem'], **kwargs)\n",
    "\n",
    "xvars = ['el_f0', 'el_cl_aeta', 'el_rawcl_E']\n",
    "for xvar in xvars:\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 100)).values\n",
    "    df_agg_E_median_shifted = get_profile(df_test, xvar, 'E_median_shifted', xedges)\n",
    "    df_agg_E_linear_regression = get_profile(df_test, xvar, 'E_linear_regression', xedges)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    plot_profile(ax, df_agg_E_median_shifted, fmt='.', label='median shifted')\n",
    "    plot_profile(ax, df_agg_E_linear_regression, fmt='.', label='linear regression')\n",
    "    ax.set_xlabel(xvar, fontsize=15)\n",
    "    ax.set_ylabel(r'$\\mathrm{median}[E / E_{true}]$', fontsize=15)\n",
    "    ax.set_ylim(0.8, 1.05)\n",
    "    ax.legend(loc=0, fontsize=12)\n",
    "    ax.axhline(1, ls='--', color='0.4', zorder=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo with a 2 layers NN (2LNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2LNN = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=len(columns_X)),\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation=None),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2LNN.compile(optimizer=tf.optimizers.Adam(learning_rate=0.005), loss='mean_absolute_error')\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model_2LNN.fit(df_train[columns_X].values, df_train[column_y].values,\n",
    "                         epochs=20, verbose=True, batch_size=1024, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['E_2LNN'] = df_test['el_rawcl_E'] / model_2LNN.predict(df_test[columns_X]).T[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xspace = np.linspace(0.4, 1.4, 100)\n",
    "ax.hist(df_test['el_rawcl_E'] / df_test['el_truth_E'], bins=xspace, label='raw', histtype='step')\n",
    "ax.hist(df_test['E_median_shifted'] / df_test['el_truth_E']  , bins=xspace, label='raw scaled', histtype='step')\n",
    "ax.hist(df_test['E_linear_regression'] / df_test['el_truth_E'], bins=xspace, label='linear model', histtype='step', lw=2)\n",
    "ax.hist(df_test['E_2LNN'] / df_test['el_truth_E'], bins=xspace, label='2LNN', histtype='stepfilled')\n",
    "\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('E/Etrue', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat with lightgbm (BDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "    \"task\": \"train\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"l1\",\n",
    "    \"metric\": [\"l2\", \"l1\", \"huber\"],\n",
    "    \"num_leaves\": 60,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"num_boost_round\": 100,\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[columns_X], df_train[column_y])\n",
    "lgb_eval = lgb.Dataset(df_test[columns_X], df_test[column_y], reference=lgb_train)\n",
    "\n",
    "model_lgbm = lgb.train(params,\n",
    "                 train_set=lgb_train,\n",
    "                 valid_sets=lgb_eval,\n",
    "                 callbacks=[lgb.early_stopping(5), lgb.log_evaluation(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['E_BDT'] = df_test['el_rawcl_E'] / model_lgbm.predict(df_test[columns_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xspace = np.linspace(0.4, 1.4, 100)\n",
    "ax.hist(df_test['E_2LNN'] / df_test['el_truth_E'], bins=xspace, label='2LNN', histtype='step', lw=2)\n",
    "ax.hist(df_test['E_BDT'] / df_test['el_truth_E'], bins=xspace, label='BDT', histtype='stepfilled')\n",
    "\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('E/Etrue', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['el_f0', 'el_cl_aeta', 'el_rawcl_E']\n",
    "for xvar in xvars:\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.hist(df_test[xvar], 50, color='0.7')\n",
    "    ax2.set_ylim(0, ax2.get_ylim()[1] * 1.5)\n",
    "    \n",
    "\n",
    "    xedges = df_test[xvar].quantile(np.linspace(0, 1, 50)).values\n",
    "    for calibration in 'E_median_shifted', 'E_linear_regression', 'E_2LNN', 'E_BDT':\n",
    "        df_agg = get_profile(df_test, xvar, calibration, xedges)    \n",
    "        plot_profile(ax, df_agg, fmt='.', label=calibration)\n",
    "    ax.set_xlabel(xvar, fontsize=15)\n",
    "    ax.set_ylabel(r'$\\mathrm{median}[E / E_{true}]$', fontsize=15)\n",
    "    ax.set_ylim(0.8, 1.05)\n",
    "    ax.legend(loc=0, fontsize=12)\n",
    "    ax.axhline(1, ls='--', color='0.4', zorder=-1)\n",
    "    \n",
    "    ax.set_zorder(ax2.get_zorder() + 1)\n",
    "    ax.set_frame_on(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model which are able to use most of the information are the 2LNN and the BDT (very visible in the last plot where the linear regression is not flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E/Etrue for different values of raw energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "el_rawcl_E_edges = df_test['el_rawcl_E'].quantile(np.linspace(0, 1, 5 + 1)).values\n",
    "\n",
    "for calibration in 'E_median_shifted', 'E_linear_regression', 'E_2LNN', 'E_BDT':\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    xspace = np.linspace(0.5, 1.5, 100)\n",
    "    for eraw_min, eraw_max in pairwise(el_rawcl_E_edges):\n",
    "        mask = (df_test['el_rawcl_E'] >= eraw_min) & (df_test['el_rawcl_E'] < eraw_max)\n",
    "        ax.hist(df_test[mask].eval(f'{calibration} / el_truth_E'),\n",
    "                bins=xspace, label='%.1f - %.1f' % (eraw_min / 1E3, eraw_max / 1E3), histtype='step')\n",
    "        ax.axvline(1, ls='--', color='0.5')\n",
    "        ax.set_title(calibration, fontsize=15)\n",
    "\n",
    "    ax.legend(loc=0, title='raw energy / GeV', fontsize=12)\n",
    "    ax.set_xlabel('E/Etrue', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model with lwtnn to be used in Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lwtnn does not support normalization layers\n",
    "\n",
    "# 1) we need to write the model structure to json\n",
    "# get the architecture as a json string\n",
    "arch = model_linear_regression.to_json(indent=2)\n",
    "print(\"== 1. architecture.json == \")\n",
    "print(arch)\n",
    "# save the architecture string to a file somehow, the below will work\n",
    "with open('architecture.json', 'w') as arch_file:\n",
    "    arch_file.write(arch)\n",
    "\n",
    "# 2) save the weights as an HDF5 file\n",
    "print(\"\\n== 2. weights ==\")\n",
    "print(model_linear_regression.weights)\n",
    "model_linear_regression.save_weights('weights.h5')\n",
    "\n",
    "# 3) we need write an additional json file with the list of input variables\n",
    "# lwtnn support simple normalization preprocessing, we need to specify it manually\n",
    "import json\n",
    "variables_json = []\n",
    "for column, scale, offset in zip(columns_X, normalizer.mean.numpy()[0], normalizer.variance.numpy()[0]):\n",
    "    variables_json.append({'name': column, 'offset': float(offset), 'scale': float(scale)})\n",
    "data_variables = {\"inputs\": [{\"name\": \"node0\", \"variables\": variables_json}]}\n",
    "data_variables[\"input_sequences\"] = []\n",
    "data_variables[\"outputs\"] = [\n",
    "    {\n",
    "      \"labels\": [\n",
    "        \"out_0\"\n",
    "      ],\n",
    "      \"name\": \"output\"\n",
    "    }\n",
    "  ]\n",
    "\n",
    "print(\"\\n== 3. input variables ==\")\n",
    "from pprint import pprint\n",
    "pprint(data_variables)\n",
    "\n",
    "\n",
    "json.dump(data_variables, open('variables.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the produced file to a single final json to be used inside Athena with lwtnn\n",
    "!~/lwtnn/converters/kerasfunc2json.py architecture.json weights.h5 variables.json > model_lwtnn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat model_lwtnn.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/lwtnn/build/bin/lwtnn-test-lightweight-graph model_lwtnn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
