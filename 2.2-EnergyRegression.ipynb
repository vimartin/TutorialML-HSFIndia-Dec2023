{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_X = ['el_rawcl_Es0', 'el_rawcl_Es1', 'el_rawcl_Es2', 'el_rawcl_Es3', 'el_rawcl_E', 'el_cl_aeta', 'el_f0']\n",
    "column_y = 'el_erawOverEtrue'\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "# this will take a while since it needs to read all your data and compute the mean and the variabnce\n",
    "# normalization is non-trainable layer, it must be run before the training\n",
    "normalizer.adapt(np.array(df_train[columns_X]))\n",
    "\n",
    "normalizer.mean, normalizer.variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.reduce_mean(normalizer(df_train[columns_X]), axis=0))\n",
    "print(tf.math.reduce_variance(normalizer(df_train[columns_X]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def my_mixture(x, *args):\n",
    "    m1, m2, m3, m4, m5, s1, s2, s3, s4, s5, k1, k2, k3, k4, k5 = args\n",
    "    ret = k1 * scipy.stats.norm.pdf(x, loc=m1 ,scale=s1)\n",
    "    ret += k2 * scipy.stats.norm.pdf(x, loc=m2 ,scale=s2)\n",
    "    ret += k3 * scipy.stats.norm.pdf(x, loc=m3 ,scale=s3)\n",
    "    ret += k4 * scipy.stats.norm.pdf(x, loc=m4 ,scale=s4)\n",
    "    ret += k5 * scipy.stats.norm.pdf(x, loc=m5 ,scale=s5)\n",
    "    return ret / 5.\n",
    "\n",
    "\n",
    "params = [1, 1, 1, 1, 1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 1]\n",
    "\n",
    "xspace = np.linspace(0.4, 1.3, 200)\n",
    "y, x = np.histogram(df_train[column_y], bins=xspace)\n",
    "xmid = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "fitted_params,_ = scipy.optimize.curve_fit(my_mixture, xmid, y, p0=params)\n",
    "my_mixture_fitted = lambda x: my_mixture(x, *fitted_params)\n",
    "integral = scipy.integrate.quad(my_mixture_fitted, 0.4, 1.3)[0]\n",
    "my_mixture_normalized = lambda x: my_mixture_fitted(x) / integral\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(xmid, my_mixture_normalized(xmid))\n",
    "ax.hist(df_train[column_y], bins=xspace, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.sum(np.log(my_mixture_normalized(df_train[column_y]))) / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_shape = [1]\n",
    "num_components = 3\n",
    "params_size = tfp.layers.MixtureNormal.params_size(num_components, event_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pdf_template = lambda t: tfd.Normal(loc=t[..., :1],\n",
    "                           scale=tf.math.softplus(t[..., 1:])\n",
    "                           #scale = tf.abs(2 + t[..., 1:])\n",
    "                          )\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=len(columns_X)),\n",
    "    normalizer,    \n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(params_size, activation=None),\n",
    "    tfp.layers.MixtureNormal(num_components, event_shape),\n",
    "    #tfp.layers.MixtureSameFamily(num_components, tfp.layers.IndependentNormal(event_shape))\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = Dense(k_mixt, activation=tf.nn.softmax)(hidden)\n",
    "#mu = Dense(k_mixt, activation=None)(hidden)\n",
    "#sigma = Dense(k_mixt, activation=tf.nn.softplus,name='sigma')(hidden)\n",
    "\n",
    "#gm = tfd.MixtureSameFamily(\n",
    "#mixture_distribution=tfd.Categorical(\n",
    "#probs=alpha),\n",
    "#components_distribution=tfd.Normal(\n",
    "#loc=mu, \n",
    "#scale=sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negloglik = lambda y, p_y: -p_y.log_prob(y)# - tfd.Normal(loc=2, scale=2).log_prob(p_y.scale)\n",
    "\n",
    "#negloglik(df_train.head(10).mass, model(df_train.head(10)[input_columns].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=negloglik)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(df_train[columns_X].values, df_train[column_y].values,\n",
    "                    epochs=20, verbose=True, batch_size=1024, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(df_test[columns_X].values)\n",
    "df_results_tf = df_test.copy()\n",
    "#df_results_tf['muCB'] = yhat.loc.numpy().flatten()\n",
    "#df_results_tf['sigmaCB'] = yhat.scale.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.parameter_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "bins = np.linspace(0.5, 1.2, 100)\n",
    "axs[0].hist(yhat.mean().numpy().flatten(), bins=bins, density=True)\n",
    "axs[0].hist(df_test['el_erawOverEtrue'], bins=bins, density=True, histtype='step')\n",
    "\n",
    "axs[1].hist(np.sqrt(yhat.variance().numpy().flatten()), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.components_distribution.mean().numpy()[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(0.5, 1.2, 100)\n",
    "ax.hist(1./yhat.mean().numpy().flatten() * df_results_tf['el_rawcl_E'] / df_results_tf['el_truth_E'], label='NN', bins=bins, density=True)\n",
    "ax.hist(df_results_tf['el_erawOverEtrue'], label='raw', bins=bins, density=True)\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = yhat.mixture_distribution.probs_parameter().numpy()\n",
    "means = yhat.components_distribution.mean().numpy()[:, :, 0]\n",
    "variances = yhat.components_distribution.variance().numpy()[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.2, 1.3, 100)\n",
    "ysum = np.zeros_like(xspace)\n",
    "for i in range(500):\n",
    "    xx = model(df_test[columns_X].iloc[i].values).tensor_distribution\n",
    "    y = xx.prob(xspace.reshape(-1, 1))\n",
    "    ysum += y\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xspace, ysum / 500)\n",
    "ax.hist(df_test[column_y], bins=xspace, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.2, 1.3, 100)\n",
    "idx = 0\n",
    "y = scipy.stats.norm(means[idx], np.sqrt(variances[1])).pdf(np.tile(xspace, (3, 1)).T)\n",
    "y = (y * alphas[idx]).sum(axis=1)\n",
    "plt.plot(xspace, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(scipy.stats.norm(means, np.sqrt(variances)).pdf(np.tile(df_test[column_y].values, (2, 1)).T).sum(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "xspace = np.linspace(0.5, 1.2, 10)\n",
    "\n",
    "components = []\n",
    "for mean, variance in zip(means.T, variances.T):\n",
    "    print(mean.shape)\n",
    "    print(variance.shape)\n",
    "    stats.norm(mean, variance).pdf(xspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = np.linspace(0.5, 1.2, 10)\n",
    "xmidpoints = 0.5 * (xspace[1:] + xspace[:-1])\n",
    "all_pdf = np.exp(yhat.log_prob(xspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = tf.convert_to_tensor(np.array([[0, 0.1]]).T, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.log_prob(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xmidpoints, all_pdf[0])\n",
    "plt.plot(xmidpoints, all_pdf[1])\n",
    "plt.plot(xmidpoints, all_pdf[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_test['el_erawOverEtrue'].hist(ax=ax, grid=False, bins=xspace)\n",
    "ax.plot(xmidpoints, all_pdf.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "# Load data -- graph of a [cardioid](https://en.wikipedia.org/wiki/Cardioid).\n",
    "n = 2000\n",
    "t = tfd.Uniform(low=-np.pi, high=np.pi).sample([n, 1])\n",
    "r = 2 * (1 - tf.cos(t))\n",
    "x = r * tf.sin(t) + tfd.Normal(loc=0., scale=0.1).sample([n, 1])\n",
    "y = r * tf.cos(t) + tfd.Normal(loc=0., scale=0.1).sample([n, 1])\n",
    "\n",
    "# Model the distribution of y given x with a Mixture Density Network.\n",
    "event_shape = [1]\n",
    "num_components = 2\n",
    "params_size = tfpl.MixtureSameFamily.params_size(\n",
    "    num_components,\n",
    "    component_params_size=tfpl.IndependentNormal.params_size(event_shape))\n",
    "model = tfk.Sequential([\n",
    "      tfkl.Dense(1024, activation='relu'),\n",
    "\n",
    "  tfkl.Dense(128, activation='relu'),\n",
    "    tfkl.Dense(64, activation='relu'),\n",
    "  tfkl.Dense(params_size, activation=None),\n",
    "  tfpl.MixtureSameFamily(num_components, tfpl.IndependentNormal(event_shape)),\n",
    "])\n",
    "\n",
    "# Fit.\n",
    "batch_size = 100\n",
    "model.compile(optimizer='adam',\n",
    "              loss=lambda y, model: -model.log_prob(y))\n",
    "model.fit(x, y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          steps_per_epoch=n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy().flatten(), model.predict(x).flatten(), '.')\n",
    "plt.plot(x.numpy().flatten(), y, '.')\n",
    "plt.plot(x.numpy().flatten(), model(x).mean().numpy().flatten(), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = model(x[0]).tensor_distribution\n",
    "plt.plot(xx.log_prob(np.linspace(-4, 1, 100).reshape(-1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
